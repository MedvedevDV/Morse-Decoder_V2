0) Сохдать отдельный трансформер на валинацию => Изменить class Dataset 
1) Обучить на 1-м батче
2) изменить LEARNING_RATE и WEIGHT_DECAY 

3) попробовать убрать выравнивание по длинне аудио - появляется много пустого места => 
модет плохо сказываться на обучение - Пошли взрывы градиента + непонятно что делать с ращной длиной
4) Дабавить нормализацию данных + Линейный слой для уменьшения признаков - 
5) добавить инициализацию весов/переписать архитектуру - 

6) Добавлена ифнормация перед запуском обучения + инфо о градиентах(Сейчас происходит затухание так как nan в preicts), 
    Попытна сгладит зибухание путем добалвения LeakyReLU для просчета отрицательных значений    
    Изменена свертка - есть предполодение что очень резкая вертка, делаем ее плавнее и оставляем больше признаков.
    # Уже стало лучше - градиенты не затухают а и ишибки стали алекватнее

7) Возможно подрезка кадра не нужна, так как принудительно ухудшает данные# не понятно есть ли вэлью, но я вно стало медленне обучаться
7.1) Порробую рапарралелить обучение (DataParallel to gpu) и использую flatten_parameters # ну хз

8) попробую порабгоать с моделью без удлинения сигнала по времени.
 До этого я удлинял короткие сигналы в 5 раз))


Подтянул знания в Rnn сетях:
2.1) Думаю что нужно реализовать модель encoder-decoder,
где в encoder будет использоваться CNN сеть для извлечения фич
далее пойдет рекурентный декодер. Кажется что испольщование attention не 
целесообразно так как нет зависимости в последовательности.
Цель по фото декодировать хаотичный набор символов. т.е. Будем плучать одну метку класса 
для КАЖДОГО скрытого состояния полученного на слое encoder
nn.functional.log_softmax(x.permute(1,0,2), dim=2) - добавил .detach().requires_grad_() - градиент не взрывается???


2.2) 
Train Loss: 0.8667
Val Loss: 4546.3094 - может быть предикт не соответствует таргету

2.3) поставил клипинг, но есть nan следоваетльно градиенты затухают резко.

2.4) Возможно косяк был в неправильном падинге последовательности. 
У меня симпол пропуска кодировался нулум так же как и нулевой элемент помледовательности - переделал 
на отдельный симпол и дабавил его в словарь



В версии 2 получена модель которая дает качество 1.01 в kagle

V3) Улучщение качества модели. Цель сделать хорошее качетво не увеличивая колличества эпох

3.1) Увеличил колличество рекурентных слоев num_layers=3. -> 
Train Loss: 0.1411 
Val Loss: 565.5488 и не падает особо - переобучение из-за высокой глубины слоев.
Для решения попробую длюавить dropout=0.2 в Gru + поставлю 
LEARNING_RATE = 2e-4 
WEIGHT_DECAY = 1e-4
- Стало лучше но все раво на трейне в первую эпоху переобучение

3.2) Уменьшу GRU hiden c 258 до 128 + добавлю dropout=0.3 перед линейныи слоем после GRU + 
доваблю больше аугментаций(сейчас они применяются ко все данным одинакого, мб стоит делать 
рандомные + более резкие)
Еще по идее странсформации в текухей реализации тоже обучабтся, мб это не надо - как я понял изучаю
вопрос -  FrequencyMasking и TimeMasking не обучаются. 
- что-то все равно лосс уже на первых эпохах маленький + разрыв огромный. 

3.3) Мб num_layers сделать 2 и увеличить dropout -> Похоже я просто выводил не усредненный val_loss))

3.4) Верну  num_layers=3 dropout=0.2 LEARNING_RATE = 2e-4 WEIGHT_DECAY = 1e-4 и отключу
клипинг

3.5) Верну hiden=258 и сделаю batch_size=64


----4) Что-то все сломалось, попробую откатиться к версии 2, 
только исправив высод лоса + увеличить кол-во эпох
    Получил молель с точностью 0.89

----5) Попробую поищраться с гиперпараметрами + все же добавлю Attention в модель
        MAX_TIME = 48
        SAMPLE_RATE = 8000
        N_MELS = 64
        N_FFT = 512
        HOP_LENGTH = 160
        TOP_DB = 80
        FREQ_MASK = 15
        TIME_MASK = 20

        # Гиперпараметы обучения
        SEED = 42
        BATCH_SIZE = 64
        EPOCHS = 60
        LEARNING_RATE = 0.0002
        WEIGHT_DECAY = 0.00001
        + добавлю аугментаций аудио через audiomentations. 
        + Сперва попробую в Attention
        частом виде, если не поможет попробую multihead attention
        - Вывод: не помогло.

----6) Прочитал что BatchNorm2d плохо работает при маленьком батче

----7) Наткнулся на статью про SE блоки как аналог Attention но для CNN
Squeeze-and-Excitation (SE) блок в CNN
class SEBlock(nn.Module):
    def __init__(self, channels, reduction=16):
        super().__init__()
        self.squeeze = nn.AdaptiveAvgPool2d(1)  # [B, C, 1, 1]
        self.excitation = nn.Sequential(
            nn.Linear(channels, channels // reduction),  # [B, C/reduction]
            nn.ReLU(),
            nn.Linear(channels // reduction, channels),  # [B, C]
            nn.Sigmoid()
        )
    
    def forward(self, x):
        B, C, _, _ = x.shape
        squeezed = self.squeeze(x).view(B, C)  # [B, C]
        weights = self.excitation(squeezed).view(B, C, 1, 1)  # [B, C, 1, 1]
        return x * weights  # Масштабируем каналы
    
    -> Слано немного лучше но этого мало

    7.1) Добавил mha с ключами(как понял, без ключеной это просто self attention а он как
    как уже выяснил не подходит) - вроде стало лучше но на 22+ эпуже видно переобчение

    7.2) Пробую жестко порезать модель + добавить еще dropout
    FIRST_FE_COUNT = 8  # было 16
    SECOND_FE_COUNT = 16  # было 32
    THIRD_FE_COUNT = 16  # было 32
    QAD_FE_COUNT = 16  # было 32
    GRU_HIDEN = 256  # было 512

    WEIGHT_DECAY = 0.001 #было 0.0001

    FREQ_MASK = 30
    TIME_MASK = 40
  

    class SEBlock(nn.Module):
    def __init__(self, channels, reduction=4):

    7.3) Поднимаем гиперпараметры
        FIRST_FE_COUNT = 64
        SECOND_FE_COUNT = 128
        THIRD_FE_COUNT = 128
        QAD_FE_COUNT = 128
        PADDING = 'same'
        MAXPOOL_KERNEL = 2
        KERTNEL_SIZE = 3
        NERON_COUNT = 128
        GRU_HIDEN = 512

        Такие изменения нагрузили видеокарту, хотя до этого она вообще не напрягалась.
        Попробовал num_workers=4, pin_memory=True в даталоадере, запуск первого блока просто помер. 
        Причины не известны

        Попробую записать спектрограммы заранее. Было лень писать отдельные трансформации для 
        трейн тест валидацияЮ так что написал все скопом, а аугментация уже налету делаю
        Это тоже не помогло 

        FIRST_FE_COUNT = 32
        SECOND_FE_COUNT = 64
        THIRD_FE_COUNT = 64
        QAD_FE_COUNT = 64 
        уже запустилось

        Получил молель 0.8 качества на kagle. но этого явно мало. Мб если бы смог запустить прошлый вариан было бы лучше.
        А так буду пилить странсформер

-----8) Сделать transformer 