{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d761d8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN размерность выхода: torch.Size([1, 32, 16, 89])\n",
      "CNN число фичей: 512\n",
      "Проекция из 512 в 1024\n",
      "\n",
      "MorseNet - инициалицация модели. Число обучаемых параметров: 13,855,753\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path as pt\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchaudio import transforms\n",
    "from torchvision.transforms import v2\n",
    "# from Moduls.MosreDataset import MosreDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.nn.parallel import DataParallel\n",
    "from collections import Counter\n",
    "\n",
    "DIVICE = torch.device(\"cuda\")\n",
    "\n",
    "MAIN = pt(os.getcwd())\n",
    "DATASET_PATCH = MAIN / 'morse_dataset'\n",
    "AUDIO_FILES = DATASET_PATCH / 'morse_dataset'\n",
    "\n",
    "# Поятоянные значения выявленные в процессе анализа\n",
    "MORSEALP = \"АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ 1234567890#\"\n",
    "MAX_TIME = 48\n",
    "SAMPLE_RATE = 8000\n",
    "N_MELS = 128\n",
    "N_FFT = 400\n",
    "HOP_LENGTH = 180\n",
    "TOP_DB = 80\n",
    "FREQ_MASK = 15\n",
    "TIME_MASK = 20\n",
    "\n",
    "# Гиперпараметы обучения\n",
    "SEED = 42\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.0002 #2e-4\n",
    "WEIGHT_DECAY = 0.00001\n",
    "# int_to_alph = dict(enumerate(MORSEALP))\n",
    "# alph_to_int = {char:enum for enum, char in int_to_alph.items()}\n",
    "\n",
    "#===== Import data =====\n",
    "train_data = pd.read_csv(pt.joinpath(DATASET_PATCH,'train.csv'))\n",
    "test_data = pd.read_csv(pt.joinpath(DATASET_PATCH,'test.csv'))\n",
    "sample_data = pd.read_csv(pt.joinpath(DATASET_PATCH,'sample_submission.csv'))\n",
    "\n",
    "all_chars = Counter(\"\".join(train_data['message']))\n",
    "BLANK_CHAR = \"_\"\n",
    "vocab_list = sorted(all_chars.keys()) + [BLANK_CHAR]\n",
    "num_classes = len(vocab_list)\n",
    "char_to_int = {char: i for i, char in enumerate(vocab_list)}\n",
    "int_to_char = {i: char for i, char in enumerate(vocab_list)}\n",
    "BLANK_IDX = char_to_int[BLANK_CHAR]\n",
    "\n",
    "class MosreDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Класс для обработки \n",
    "    \"\"\"\n",
    "    def __init__(self, df, data_patch,char_to_int, train=True, transforms=None, prev_chars = 1):\n",
    "        self.df = df\n",
    "        self.is_train = train\n",
    "\n",
    "        self.data_path = data_patch\n",
    "        self.audio_paths = self.data_path / 'morse_dataset'\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.char_to_int = char_to_int\n",
    "        self.prev_chars = prev_chars\n",
    "\n",
    "        if self.is_train:\n",
    "            self.messeges = self.df.message.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #Получение аугментрованых спектрограмм\n",
    "        try:\n",
    "            audio_file = self.audio_paths / self.df.id.values[index]\n",
    "            waveform, sample_rate = torchaudio.load(audio_file)\n",
    "            augmented_spectrogram = self.transforms(waveform)\n",
    "            spec_lens = augmented_spectrogram.shape[-1]\n",
    "            if self.is_train:\n",
    "                message = self.messeges[index]\n",
    "                #Получение списка индексов секта - как требует CTC los\n",
    "                '''\n",
    "                При обработке dataloader labels будут выравниваться по макс длине для выравнивания батча\n",
    "                Т.е. будет padding 0. что в будующем будет пустым значением для ctc loss\n",
    "                '''\n",
    "                target = torch.tensor([self.char_to_int[char] for char in message], dtype=torch.long); \n",
    "                target_len = torch.tensor(len(target), dtype=torch.long)\n",
    "                return augmented_spectrogram, spec_lens, target ,target_len, message\n",
    "            else:\n",
    "                return augmented_spectrogram,spec_lens, None, None, None\n",
    "        except Exception as ex:\n",
    "            print(str(ex))\n",
    "    \n",
    "FIRST_FE_COUNT = 16\n",
    "SECOND_FE_COUNT = 32\n",
    "THIRD_FE_COUNT = 32\n",
    "QAD_FE_COUNT = 32\n",
    "PADDING = 'same'\n",
    "MAXPOOL_KERNEL = 2\n",
    "KERTNEL_SIZE = 3\n",
    "NERON_COUNT = 128\n",
    "GRU_HIDEN = 512\n",
    "# Start with 4 transforms\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)  # [B, C, 1, 1]\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction),  # [B, C/reduction]\n",
    "            nn.GELU(),\n",
    "            nn.Linear(channels // reduction, channels),  # [B, C]\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, _, _ = x.shape\n",
    "        squeezed = self.squeeze(x).view(B, C)  # [B, C]\n",
    "        weights = self.excitation(squeezed).view(B, C, 1, 1)  # [B, C, 1, 1]\n",
    "        return x * weights  # Масштабируем каналы\n",
    "    \n",
    "\n",
    "class MorseNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.net_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, \n",
    "                      out_channels=FIRST_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(FIRST_FE_COUNT),\n",
    "            SEBlock(FIRST_FE_COUNT, 4),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((1, 2), (1, 2)), # [batch, FIRST_FE_COUNT = 16, 64, 960]\n",
    "\n",
    "            nn.Conv2d(in_channels=FIRST_FE_COUNT, \n",
    "                      out_channels=SECOND_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(SECOND_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            SEBlock(SECOND_FE_COUNT, 8),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)), # [batch, SECOND_FE_COUNT = 32, 32, 480]\n",
    "\n",
    "            nn.Conv2d(in_channels=SECOND_FE_COUNT, \n",
    "                      out_channels=THIRD_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(THIRD_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            SEBlock(THIRD_FE_COUNT, 16),\n",
    "            nn.MaxPool2d((2, 2), (2, 2)), # [batch, THIRD_FE_COUNT = 32, 16, 240]\n",
    "\n",
    "            nn.Conv2d(in_channels=THIRD_FE_COUNT, \n",
    "                      out_channels=QAD_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(QAD_FE_COUNT, 16),\n",
    "            nn.GELU(),\n",
    "            SEBlock(QAD_FE_COUNT),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)) # [batch=32, QAD_FE_COUNT = 32, 16, 89](что юы сохраниить большще признаков по горизонтали)\n",
    "        )\n",
    "        with torch.no_grad(): \n",
    "            dummy_input = torch.randn(1, 1, N_MELS, 356); \n",
    "            cnn_out = self.net_conv(dummy_input); \n",
    "            self.cnn_output_features = cnn_out.shape[1] * cnn_out.shape[2]\n",
    "\n",
    "        print(f\"CNN размерность выхода: {cnn_out.shape}\"); \n",
    "        print(f\"CNN число фичей: {self.cnn_output_features}\")\n",
    "\n",
    "        # Добавлен лоейный слой и функция активации. Для чего? расписать потом \n",
    "        self.layer1 = nn.Linear(self.cnn_output_features, N_MELS*2); \n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "        print(f\"Проекция из {self.cnn_output_features} в {GRU_HIDEN*2}\")\n",
    "        self.rnn = nn.LSTM(\n",
    "                input_size=N_MELS*2,\n",
    "                hidden_size=GRU_HIDEN,\n",
    "                num_layers=2,\n",
    "                bidirectional=True,\n",
    "                dropout=0.3,\n",
    "                batch_first=True \n",
    "            )\n",
    "\n",
    "        self.embed_dim = GRU_HIDEN * 2\n",
    "\n",
    "        self.mha = nn.MultiheadAttention(self.embed_dim, 8, dropout=0.3, batch_first=True)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(self.embed_dim)  \n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)   \n",
    "\n",
    "        self.layer2 = nn.Linear(self.embed_dim, num_classes)     \n",
    "\n",
    "    def _get_output_lengths(self, input_lengths):\n",
    "        output_lengths = torch.floor(input_lengths.float() / 4); \n",
    "        return torch.clamp(output_lengths.long(), min=1).to(DIVICE)\n",
    "    \n",
    "    def forward(self, x, input_lengths):\n",
    "        x = self.net_conv(x)\n",
    "\n",
    "        batch, channels, reduced_mels, reduced_time = x.shape\n",
    "        x = x.permute(0, 3, 1, 2)  # [batch, time, channels, mels]\n",
    "\n",
    "        # В частности, каждый вектор признаков в последовательности признаков генерируется \n",
    "        # слева направо на картах признаков. Это означает, что i-й вектор признаков представляет \n",
    "        # собой объединение столбцов всех карт. \n",
    "        # Таким образом, форма тензора может быть изменена, например, на (размер_пакета, 80, 256)\n",
    "        \n",
    "        x = x.reshape(batch, reduced_time, -1)  # to GRU [batch=32, seq_len=89, features/hiden_dim=512]\n",
    "        x = self.layer1(x)\n",
    "        x = self.gelu(x)\n",
    "        \n",
    "        output_lengths = self._get_output_lengths(input_lengths)\n",
    "        output_lengths.to(DIVICE)\n",
    "        self.rnn.flatten_parameters()\n",
    "        # x = self.layer_norm1(x)\n",
    "        x = self.rnn(x) # [batch=32, seq_len=89, features/hiden_dim=256 * 2]\n",
    "        x, _ = x # берем информацию со всез состояний\n",
    "\n",
    "        #att\n",
    "        max_len = reduced_time; \n",
    "        idx = torch.arange(max_len, device=DIVICE).unsqueeze(0); \n",
    "        key_padding_mask = (idx >= output_lengths.unsqueeze(1))\n",
    "        x, _ = self.mha(x, x, x, key_padding_mask=key_padding_mask)\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.layer2(x) # logits - [batch, sequence, num_classes] \n",
    "        x = nn.functional.log_softmax(x.permute(1,0,2), dim=2) # pertime так как CTC loss требует на взод (sequence/T,batch/N,num_classes/C)\n",
    "        '''\n",
    "        по одному прогнозу для каждого из признаков в последовательности, \n",
    "        в итоге получается 89 прогнозов символов для каждой секунды звука.\n",
    "        '''\n",
    "        return x\n",
    "    \n",
    "\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS),\n",
    "    transforms.AmplitudeToDB(top_db=TOP_DB),\n",
    "    transforms.FrequencyMasking(freq_mask_param=FREQ_MASK),\n",
    "    transforms.TimeMasking(time_mask_param=TIME_MASK),\n",
    "    # v2.RandomCrop((N_MELS, 1920)) # Обрезает последний кадр спектрограммы, в идеале надобы считать а не прописывать число\n",
    "    ) # заметка - Данные трансформации не создают довых обучаемых параметров. Но есть и те что создают. В будущем это стоит учитывать\n",
    "\n",
    "valid_audio_transforms = nn.Sequential(\n",
    "    transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS),\n",
    "    transforms.AmplitudeToDB(top_db=TOP_DB),\n",
    "    # v2.CenterCrop((N_MELS, 1920)) \n",
    "    )\n",
    "\n",
    "train_dataframe, val_dataframe = train_test_split(train_data, test_size=0.15, random_state=SEED)\n",
    "\n",
    "train_ds = MosreDataset(df=train_dataframe,\n",
    "                        data_patch=DATASET_PATCH,\n",
    "                        char_to_int=char_to_int,\n",
    "                        train=True,\n",
    "                        transforms=train_audio_transforms)\n",
    "\n",
    "val_ds = MosreDataset(df=val_dataframe,\n",
    "                        data_patch=DATASET_PATCH,\n",
    "                        char_to_int=char_to_int,\n",
    "                        train=True,\n",
    "                        transforms=valid_audio_transforms)\n",
    "\n",
    "\n",
    "def my_collate(batch):\n",
    "    spectrograms = [item[0].squeeze(0) for item in batch]\n",
    "    # Падинг спектрограмм по максимальной длине\n",
    "    spectrograms_permuted = [s.permute(1, 0) for s in spectrograms]\n",
    "    spectrograms_padded = nn.utils.rnn.pad_sequence(spectrograms_permuted, batch_first=True, padding_value=0.0)\n",
    "    spectrograms_padded = spectrograms_padded.permute(0, 2, 1).unsqueeze(1)\n",
    "    # spec_lens = torch.stack([item[0] for item in batch])\n",
    "    spec_lens = torch.tensor([item[1] for item in batch]).reshape(BATCH_SIZE)\n",
    "    if batch[0][3] is not None:\n",
    "        target = torch.nn.utils.rnn.pad_sequence(\n",
    "                                                [item[2] for item in batch], \n",
    "                                                batch_first=True, \n",
    "                                                padding_value=BLANK_IDX)# выравнивает последовательность до макс \n",
    "                                                                        # длины в батче заполняя пропуски нулем\n",
    "        label_len = torch.stack([item[3] for item in batch])\n",
    "        msg = [item[4] for item in batch]\n",
    "        \n",
    "        return [spectrograms_padded, spec_lens, target, label_len, msg]\n",
    "    else: \n",
    "        return [spectrograms_padded, spec_lens]\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=my_collate, drop_last=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=my_collate, drop_last=True)\n",
    "\n",
    "test, test_lens, test_target, _, mess = next(iter(train_dl))\n",
    "test, test_target= test.to(DIVICE), test_target.to(DIVICE)\n",
    "\n",
    "test_val, val_lens, val_target, __, val_mess = next(iter(val_dl))\n",
    "test_val, val_target = test_val.to(DIVICE), val_target.to(DIVICE)\n",
    "# test.shape \n",
    "\n",
    "#===== начало обучения =====\n",
    "model = MorseNet(num_classes=num_classes).to(DIVICE)\n",
    "# model = DataParallel(model)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=0.002)  # Было 0.002\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "loss_func = nn.CTCLoss(blank=BLANK_IDX, reduction='mean', zero_infinity=True).to(DIVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nMorseNet - инициалицация модели. Число обучаемых параметров: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bcbbd50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356,\n",
       "        356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356,\n",
       "        356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356,\n",
       "        356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356,\n",
       "        356, 356, 356, 356, 356, 356, 356, 356])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d2fbb29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([89, 64, 45])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model(test, test_lens)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa7f726",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c23dee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "def ctc_decoder(logits, int_char_map, blank_label_idx):\n",
    "    preds = []\n",
    "    logits_cpu = logits.cpu() \n",
    "    max_inds = torch.argmax(logits_cpu.detach(), dim=2).t().numpy() # арзмакс по лагитам и преобразование к словарю\n",
    "    \n",
    "    for ind in max_inds:\n",
    "        merged_inds = []\n",
    "        for idx in ind:\n",
    "            if idx != blank_label_idx: \n",
    "                merged_inds.append(idx)\n",
    "        text = \"\".join([int_char_map.get(i, '?') for i in merged_inds])\n",
    "        preds.append(text)\n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422ea6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 1/50 =====\n",
      "Mean grad norm: 0.019355\n",
      "Max grad norm: 0.998062\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 10.1726\n",
      "---- Val Loss: 4.0424\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m train_predicts = []\n\u001b[32m     10\u001b[39m train_tqdm = tqdm(train_dl, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mЭпоха \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m [Обучение]\u001b[39m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_tqdm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmel_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmel_spec_lens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_lens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmel_spec\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmel_spec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDIVICE\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 86\u001b[39m, in \u001b[36mMosreDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     84\u001b[39m audio_file = \u001b[38;5;28mself\u001b[39m.audio_paths / \u001b[38;5;28mself\u001b[39m.df.id.values[index]\n\u001b[32m     85\u001b[39m waveform, sample_rate = torchaudio.load(audio_file)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m augmented_spectrogram = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m spec_lens = augmented_spectrogram.shape[-\u001b[32m1\u001b[39m]\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_train:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\nn\\modules\\container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torchaudio\\transforms\\_transforms.py:619\u001b[39m, in \u001b[36mMelSpectrogram.forward\u001b[39m\u001b[34m(self, waveform)\u001b[39m\n\u001b[32m    611\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, waveform: Tensor) -> Tensor:\n\u001b[32m    612\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    613\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    614\u001b[39m \u001b[33;03m        waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    617\u001b[39m \u001b[33;03m        Tensor: Mel frequency spectrogram of size (..., ``n_mels``, time).\u001b[39;00m\n\u001b[32m    618\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m     specgram = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     mel_specgram = \u001b[38;5;28mself\u001b[39m.mel_scale(specgram)\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m mel_specgram\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torchaudio\\transforms\\_transforms.py:110\u001b[39m, in \u001b[36mSpectrogram.forward\u001b[39m\u001b[34m(self, waveform)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, waveform: Tensor) -> Tensor:\n\u001b[32m    101\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[33;03m        waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    108\u001b[39m \u001b[33;03m        Fourier bins, and time is the number of window hops (n_frame).\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspectrogram\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnormalized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43monesided\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torchaudio\\functional\\functional.py:123\u001b[39m, in \u001b[36mspectrogram\u001b[39m\u001b[34m(waveform, pad, window, n_fft, hop_length, win_length, power, normalized, center, pad_mode, onesided, return_complex)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# pack batch\u001b[39;00m\n\u001b[32m    122\u001b[39m shape = waveform.size()\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m waveform = \u001b[43mwaveform\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# default values are consistent with librosa.core.spectrum._spectrogram\u001b[39;00m\n\u001b[32m    126\u001b[39m spec_f = torch.stft(\n\u001b[32m    127\u001b[39m     \u001b[38;5;28minput\u001b[39m=waveform,\n\u001b[32m    128\u001b[39m     n_fft=n_fft,\n\u001b[32m   (...)\u001b[39m\u001b[32m    136\u001b[39m     return_complex=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    137\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "lst_loss_train = []\n",
    "lst_loss_val = []\n",
    "best_val_loss = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "\n",
    "    epoch_train_loss = 0.0\n",
    "    train_predicts = []\n",
    "\n",
    "    train_tqdm = tqdm(train_dl, desc=f\"Эпоха {epoch+1}/{EPOCHS} [Обучение]\", leave=False)\n",
    "    for batch_ind, batch in enumerate(train_tqdm):\n",
    "        mel_spec, mel_spec_lens, targets, targets_lens, _ = batch\n",
    "        mel_spec = mel_spec.to(DIVICE)\n",
    "        mel_spec_lens = mel_spec_lens.to(DIVICE)\n",
    "        targets = targets.to(DIVICE)\n",
    "        targets_lens = targets_lens.to(DIVICE)\n",
    "\n",
    "        #===== считатем длинну mel_spec для передачи в CTC loss =====\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predict = model(mel_spec, mel_spec_lens) # (N=batch,T,C)\n",
    "        N = predict.shape[1]\n",
    "        T = predict.shape[0]\n",
    "        predict_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "\n",
    "        try:\n",
    "            loss = loss_func(predict, targets, predict_lengths, targets_lens.reshape(BATCH_SIZE))\n",
    "        except RuntimeError:\n",
    "            print(predict.shape, targets.shape, predict_lengths, targets_lens.reshape(BATCH_SIZE))\n",
    "            continue\n",
    "\n",
    "        if torch.isnan(loss) or torch.isinf(loss): \n",
    "            print(f\"\\nWarning: In batch-{batch_ind} loss train is NaN/Inf: {loss.item()}\"); \n",
    "            optimizer.zero_grad(); \n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "    total_train = epoch_train_loss / len(train_dl)\n",
    "\n",
    "    # ======== Валидация ========\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    total_val = 0\n",
    "    val_predicts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_tqdm = tqdm(val_dl, desc=f\"Эпоха {epoch+1}/{EPOCHS} [Валидация]\", leave=False)\n",
    "        for val_mel_spec, val_spec_lens, val_labels, val_label_lensin, _ in val_tqdm:\n",
    "\n",
    "            val_mel_spec = val_mel_spec.to(DIVICE)\n",
    "            val_spec_lens = val_spec_lens.to(DIVICE)\n",
    "            val_labels = val_labels.to(DIVICE)\n",
    "            val_label_lensin = val_label_lensin.to(DIVICE)\n",
    "\n",
    "            val_predict = model(val_mel_spec, val_spec_lens)\n",
    "            val_N = val_predict.shape[1]\n",
    "            val_T = val_predict.shape[0]\n",
    "\n",
    "            predict_val_lengths = torch.full(size=(val_N,), fill_value=val_T, dtype=torch.long)\n",
    "            val_loss += loss_func(val_predict, val_labels, predict_val_lengths, val_label_lensin).item()\n",
    "\n",
    "    total_val = val_loss / len(val_dl)\n",
    "\n",
    "    lst_loss_train.append(total_train)\n",
    "    lst_loss_val.append(total_val)\n",
    "\n",
    "    scheduler.step(total_val)\n",
    "\n",
    "    print(f\"\\n===== Эпоха {epoch+1}/{EPOCHS} =====\")\\\n",
    "    #===== Инфо про градиенты=====\n",
    "    grad_norms = [param.grad.norm().item() for param in model.parameters() if param.grad is not None]\n",
    "    if grad_norms:\n",
    "        print(f\"Mean grad norm: {np.mean(grad_norms):.6f}\")\n",
    "        print(f\"Max grad norm: {np.max(grad_norms):.6f}\")\n",
    "        print(f\"Min grad norm: {np.min(grad_norms):.6f}\")\n",
    "    else:\n",
    "        print(\"No gradients computed yet.\")\n",
    "    #===== Инфо про шаг обучения и данные по потерям =====\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Current LR: {current_lr:.6f}\")\n",
    "    print(f\"---- Train Loss: {total_train:.4f}\")\n",
    "    print(f\"---- Val Loss: {total_val:.4f}\")\n",
    "    if current_lr <= 1e-6:\n",
    "        print(\"Learning rate достиг минимума 1e-6, остановка обучения\")\n",
    "        break\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca52e14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'MorseNet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1547a2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lst_loss_train)\n",
    "plt.plot(lst_loss_val)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45f65295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN размерность выхода: torch.Size([1, 32, 16, 89])\n",
      "CNN число фичей: 512\n",
      "Проекция из 512 в 512\n",
      "Mean accurasu by The Levenshtein in train is : 0.9306204623520523\n",
      "Mean accurasu by The Levenshtein in validate is : 0.9449027945915751\n"
     ]
    }
   ],
   "source": [
    "def ctc_decoder(logits, int_char_map, blank_label_idx):\n",
    "    preds = []\n",
    "    logits_cpu = logits.cpu() \n",
    "    max_inds = torch.argmax(logits_cpu.detach(), dim=2).t().numpy() # арзмакс по лагитам и преобразование к словарю\n",
    "    \n",
    "    for ind in max_inds:\n",
    "        merged_inds = []\n",
    "        for idx in ind:\n",
    "            if idx != blank_label_idx: \n",
    "                merged_inds.append(idx)\n",
    "        text = \"\".join([int_char_map.get(i, '?') for i in merged_inds])\n",
    "        preds.append(text)\n",
    "\n",
    "    return preds\n",
    "\n",
    "model_load = MorseNet(num_classes=num_classes)\n",
    "model_load.load_state_dict(torch.load('MorseNet.pth'))\n",
    "model_load.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_mess = []\n",
    "    train_predicts = []\n",
    "    for loader in train_dl:\n",
    "        seq, test_target, _, mess = loader\n",
    "        train_mess.extend(mess)\n",
    "\n",
    "        logits = model_load(seq)\n",
    "        predicted_values = ctc_decoder(logits, int_to_char, BLANK_IDX)\n",
    "        train_predicts.extend(predicted_values)\n",
    "\n",
    "    val_mess = []\n",
    "    val_predicts = []\n",
    "    for loader in val_dl:\n",
    "        seq, test_target, _, mess = loader\n",
    "        val_mess.extend(mess)\n",
    "\n",
    "        logits= model_load(seq)\n",
    "        predicted_values = ctc_decoder(logits, int_to_char, BLANK_IDX)\n",
    "        val_predicts.extend(predicted_values)\n",
    "\n",
    "import Levenshtein\n",
    "\n",
    "mean_acc_test = np.mean([Levenshtein.ratio(test_pred, train_mess[ind]) for ind, test_pred in enumerate(train_predicts)])\n",
    "mean_acc_val = np.mean([Levenshtein.ratio(val_pred, val_mess[ind]) for ind, val_pred in enumerate(val_predicts)])\n",
    "\n",
    "\n",
    "print(f\"Mean accurasu by The Levenshtein in train is : {mean_acc_test}\")\n",
    "print(f\"Mean accurasu by The Levenshtein in validate is : {mean_acc_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49ca828",
   "metadata": {},
   "source": [
    "# Сбор sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c85e44a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN размерность выхода: torch.Size([1, 32, 16, 89])\n",
      "CNN число фичей: 512\n",
      "Проекция из 512 в 512\n"
     ]
    }
   ],
   "source": [
    "def ctc_decoder(logits, int_char_map, blank_label_idx):\n",
    "    preds = []\n",
    "    logits_cpu = logits.cpu() \n",
    "    max_inds = torch.argmax(logits_cpu.detach(), dim=2).t().numpy() # арзмакс по лагитам и преобразование к словарю\n",
    "    \n",
    "    for ind in max_inds:\n",
    "        merged_inds = []\n",
    "        for idx in ind:\n",
    "            if idx != blank_label_idx: \n",
    "                merged_inds.append(idx)\n",
    "        text = \"\".join([int_char_map.get(i, '?') for i in merged_inds])\n",
    "        preds.append(text)\n",
    "\n",
    "    return preds\n",
    "\n",
    "model_load = MorseNet(num_classes=num_classes)\n",
    "model_load.load_state_dict(torch.load('MorseNet.pth'))\n",
    "model_load.eval()\n",
    "\n",
    "test_ds = MosreDataset(df=sample_data,\n",
    "                        data_patch=DATASET_PATCH,\n",
    "                        char_to_int=char_to_int,\n",
    "                        train=False,\n",
    "                        transforms=valid_audio_transforms)\n",
    "\n",
    "\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=20, shuffle=False, collate_fn=my_collate)\n",
    "model_load.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_predicts = []\n",
    "    for loader in test_dl:\n",
    "        seq = loader\n",
    "        logits = model_load(seq)\n",
    "        predicted_values = ctc_decoder(logits, int_to_char, BLANK_IDX)\n",
    "        test_predicts.extend(predicted_values)\n",
    "\n",
    "sample_data.message = test_predicts\n",
    "sample_data.to_csv('sample_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Morse_decoder_V2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
