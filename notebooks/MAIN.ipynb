{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d761d8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN размерность выхода: torch.Size([1, 32, 16, 89])\n",
      "CNN число фичей: 512\n",
      "Проекция из 512 в 512\n",
      "\n",
      "MorseNet - инициалицация модели. Число обучаемых параметров: 2,808,589\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path as pt\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchaudio import transforms\n",
    "from torchvision.transforms import v2\n",
    "# from Moduls.MosreDataset import MosreDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.nn.parallel import DataParallel\n",
    "from collections import Counter\n",
    "\n",
    "DIVICE = torch.device(\"cuda\")\n",
    "\n",
    "MAIN = pt(os.getcwd())\n",
    "DATASET_PATCH = MAIN / 'morse_dataset'\n",
    "AUDIO_FILES = DATASET_PATCH / 'morse_dataset'\n",
    "\n",
    "# Поятоянные значения выявленные в процессе анализа\n",
    "MORSEALP = \"АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ 1234567890#\"\n",
    "MAX_TIME = 48\n",
    "SAMPLE_RATE = 8000\n",
    "N_MELS = 128\n",
    "N_FFT = 400\n",
    "HOP_LENGTH = 180\n",
    "TOP_DB = 80\n",
    "FREQ_MASK = 15\n",
    "TIME_MASK = 20\n",
    "\n",
    "# Гиперпараметы обучения\n",
    "SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 70\n",
    "LEARNING_RATE = 0.002 #2e-4\n",
    "WEIGHT_DECAY = 0.00001\n",
    "# int_to_alph = dict(enumerate(MORSEALP))\n",
    "# alph_to_int = {char:enum for enum, char in int_to_alph.items()}\n",
    "\n",
    "#===== Import data =====\n",
    "train_data = pd.read_csv(pt.joinpath(DATASET_PATCH,'train.csv'))\n",
    "test_data = pd.read_csv(pt.joinpath(DATASET_PATCH,'test.csv'))\n",
    "sample_data = pd.read_csv(pt.joinpath(DATASET_PATCH,'sample_submission.csv'))\n",
    "\n",
    "all_chars = Counter(\"\".join(train_data['message']))\n",
    "BLANK_CHAR = \"_\"\n",
    "vocab_list = sorted(all_chars.keys()) + [BLANK_CHAR]\n",
    "num_classes = len(vocab_list)\n",
    "char_to_int = {char: i for i, char in enumerate(vocab_list)}\n",
    "int_to_char = {i: char for i, char in enumerate(vocab_list)}\n",
    "BLANK_IDX = char_to_int[BLANK_CHAR]\n",
    "\n",
    "class MosreDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Класс для обработки \n",
    "    \"\"\"\n",
    "    def __init__(self, df, data_patch,char_to_int, train=True, transforms=None, prev_chars = 1):\n",
    "        self.df = df\n",
    "        self.is_train = train\n",
    "\n",
    "        self.data_path = data_patch\n",
    "        self.audio_paths = self.data_path / 'morse_dataset'\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.char_to_int = char_to_int\n",
    "        self.prev_chars = prev_chars\n",
    "\n",
    "        if self.is_train:\n",
    "            self.messeges = self.df.message.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #Получение аугментрованых спектрограмм\n",
    "        try:\n",
    "            audio_file = self.audio_paths / self.df.id.values[index]\n",
    "            waveform, sample_rate = torchaudio.load(audio_file)\n",
    "            augmented_spectrogram = self.transforms(waveform)\n",
    "\n",
    "            if self.is_train:\n",
    "                message = self.messeges[index]\n",
    "                #Получение списка индексов секта - как требует CTC los\n",
    "                '''\n",
    "                При обработке dataloader labels будут выравниваться по макс длине для выравнивания батча\n",
    "                Т.е. будет padding 0. что в будующем будет пустым значением для ctc loss\n",
    "                '''\n",
    "                target = torch.tensor([self.char_to_int[char] for char in message], dtype=torch.long); \n",
    "                target_len = torch.tensor(len(target), dtype=torch.long)\n",
    "                return augmented_spectrogram, target, target_len, message\n",
    "            else:\n",
    "                return augmented_spectrogram, None, None, None\n",
    "        except Exception as ex:\n",
    "            print(str(ex))\n",
    "        \n",
    "    def change_time(self, audio_file, max_len = 384000):\n",
    "        waveform, sample_rate = torchaudio.load(audio_file)\n",
    "        cahanal, sig_len = waveform.shape\n",
    "\n",
    "        if sig_len < max_len:\n",
    "            pad_len = torch.zeros(max_len - sig_len).unsqueeze(0)\n",
    "            waveform = torch.cat([waveform, pad_len], dim=1)\n",
    "\n",
    "        return waveform\n",
    "    \n",
    "FIRST_FE_COUNT = 16\n",
    "SECOND_FE_COUNT = 32\n",
    "THIRD_FE_COUNT = 32\n",
    "QAD_FE_COUNT = 32\n",
    "PADDING = 'same'\n",
    "MAXPOOL_KERNEL = 2\n",
    "KERTNEL_SIZE = 3\n",
    "NERON_COUNT = 128\n",
    "GRU_HIDEN = 256\n",
    "# Start with 4 transforms\n",
    "class MorseNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.net_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, \n",
    "                      out_channels=FIRST_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(FIRST_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((1, 2), (1, 2)), # [batch, FIRST_FE_COUNT = 16, 64, 960]\n",
    "\n",
    "            nn.Conv2d(in_channels=FIRST_FE_COUNT, \n",
    "                      out_channels=SECOND_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(SECOND_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)), # [batch, SECOND_FE_COUNT = 32, 32, 480]\n",
    "\n",
    "            nn.Conv2d(in_channels=SECOND_FE_COUNT, \n",
    "                      out_channels=THIRD_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(THIRD_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((2, 2), (2, 2)), # [batch, THIRD_FE_COUNT = 32, 16, 240]\n",
    "\n",
    "            nn.Conv2d(in_channels=THIRD_FE_COUNT, \n",
    "                      out_channels=QAD_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(QAD_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)) # [batch=32, QAD_FE_COUNT = 32, 8, 80](что юы сохраниить большще признаков по горизонтали)\n",
    "        )\n",
    "        with torch.no_grad(): \n",
    "            dummy_input = torch.randn(1, 1, N_MELS, 356); \n",
    "            cnn_out = self.net_conv(dummy_input); \n",
    "            self.cnn_output_features = cnn_out.shape[1] * cnn_out.shape[2]\n",
    "\n",
    "        print(f\"CNN размерность выхода: {cnn_out.shape}\"); \n",
    "        print(f\"CNN число фичей: {self.cnn_output_features}\")\n",
    "\n",
    "        # Добавлен лоейный слой и функция активации. Для чего? расписать потом \n",
    "        self.layer1 = nn.Linear(self.cnn_output_features, N_MELS*2); \n",
    "        self.gelu = nn.GELU()\n",
    "        print(f\"Проекция из {self.cnn_output_features} в {GRU_HIDEN*2}\")\n",
    "        self.rnn = nn.LSTM(\n",
    "                input_size=N_MELS*2,\n",
    "                hidden_size=GRU_HIDEN,\n",
    "                num_layers=2,\n",
    "                bidirectional=True,\n",
    "                dropout=0.3,\n",
    "                batch_first=True \n",
    "            )\n",
    "\n",
    "        \n",
    "        self.embed_dim = GRU_HIDEN * 2\n",
    "        self.layer_norm = nn.LayerNorm(self.embed_dim)      \n",
    "        self.dropout = nn.Dropout(0.3)   \n",
    "        self.layer2 = nn.Linear(self.embed_dim, num_classes)       \n",
    "        # self.layer3 = nn.Linear(GRU_HIDEN, GRU_HIDEN // 2)       \n",
    "        # self.layer4 = nn.Linear(GRU_HIDEN // 2, 45)             \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net_conv(x)\n",
    "\n",
    "        batch, channels, reduced_mels, reduced_time = x.shape\n",
    "        x = x.permute(0, 3, 1, 2)  # [batch, time, channels, mels]\n",
    "\n",
    "        # В частности, каждый вектор признаков в последовательности признаков генерируется \n",
    "        # слева направо на картах признаков. Это означает, что i-й вектор признаков представляет \n",
    "        # собой объединение столбцов всех карт. \n",
    "        # Таким образом, форма тензора может быть изменена, например, на (размер_пакета, 80, 256)\n",
    "        \n",
    "        x = x.reshape(batch, reduced_time, -1)  # to GRU [batch=32, seq_len=89, features/hiden_dim=512]\n",
    "        x = self.layer1(x)\n",
    "        x = self.gelu(x)\n",
    "\n",
    "        self.rnn.flatten_parameters()\n",
    "\n",
    "        x = self.rnn(x) # [batch=32, seq_len=89, features/hiden_dim=256 * 2]\n",
    "        x, _ = x # берем информацию со всез состояний\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x) # logits - [batch, sequence, num_classes] \n",
    "        x = nn.functional.log_softmax(x.permute(1,0,2), dim=2) # pertime так как CTC loss требует на взод (sequence/T,batch/N,num_classes/C)\n",
    "        '''\n",
    "        по одному прогнозу для каждого из признаков в последовательности, \n",
    "        в итоге получается 89 прогнозов символов для каждой секунды звука.\n",
    "        '''\n",
    "        return x\n",
    "    \n",
    "\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS),\n",
    "    transforms.AmplitudeToDB(top_db=TOP_DB),\n",
    "    transforms.FrequencyMasking(freq_mask_param=FREQ_MASK),\n",
    "    transforms.TimeMasking(time_mask_param=TIME_MASK),\n",
    "    # v2.RandomCrop((N_MELS, 1920)) # Обрезает последний кадр спектрограммы, в идеале надобы считать а не прописывать число\n",
    "    ) # заметка - Данные трансформации не создают довых обучаемых параметров. Но есть и те что создают. В будущем это стоит учитывать\n",
    "\n",
    "valid_audio_transforms = nn.Sequential(\n",
    "    transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS),\n",
    "    transforms.AmplitudeToDB(top_db=TOP_DB),\n",
    "    # v2.CenterCrop((N_MELS, 1920)) \n",
    "    )\n",
    "\n",
    "train_dataframe, val_dataframe = train_test_split(train_data, test_size=0.15, random_state=SEED)\n",
    "\n",
    "train_ds = MosreDataset(df=train_dataframe,\n",
    "                        data_patch=DATASET_PATCH,\n",
    "                        char_to_int=char_to_int,\n",
    "                        train=True,\n",
    "                        transforms=train_audio_transforms)\n",
    "\n",
    "val_ds = MosreDataset(df=val_dataframe,\n",
    "                        data_patch=DATASET_PATCH,\n",
    "                        char_to_int=char_to_int,\n",
    "                        train=True,\n",
    "                        transforms=valid_audio_transforms)\n",
    "\n",
    "\n",
    "def my_collate(batch):\n",
    "    spectrograms = [item[0].squeeze(0) for item in batch]\n",
    "    # Падинг спектрограмм по максимальной длине\n",
    "    spectrograms_permuted = [s.permute(1, 0) for s in spectrograms]\n",
    "    spectrograms_padded = nn.utils.rnn.pad_sequence(spectrograms_permuted, batch_first=True, padding_value=0.0)\n",
    "    spectrograms_padded = spectrograms_padded.permute(0, 2, 1).unsqueeze(1)\n",
    "\n",
    "    if batch[0][3] is not None:\n",
    "        target = torch.nn.utils.rnn.pad_sequence(\n",
    "                                                [item[1] for item in batch], \n",
    "                                                batch_first=True, \n",
    "                                                padding_value=BLANK_IDX)# выравнивает последовательность до макс \n",
    "                                                                        # длины в батче заполняя пропуски нулем\n",
    "        label_len = torch.stack([item[2] for item in batch])\n",
    "        msg = [item[3] for item in batch]\n",
    "        \n",
    "        return [spectrograms_padded, target, label_len, msg]\n",
    "    else: \n",
    "        return spectrograms_padded\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=my_collate, drop_last=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=my_collate, drop_last=True)\n",
    "\n",
    "test, test_target, _, mess = next(iter(train_dl))\n",
    "test, test_target= test.to(DIVICE), test_target.to(DIVICE)\n",
    "\n",
    "test_val, val_target, __, val_mess = next(iter(val_dl))\n",
    "test_val, val_target = test_val.to(DIVICE), val_target.to(DIVICE)\n",
    "# test.shape \n",
    "\n",
    "#===== начало обучения =====\n",
    "model = MorseNet(num_classes=num_classes).to(DIVICE)\n",
    "# model = DataParallel(model)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=0.002)  # Было 0.002\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "loss_func = nn.CTCLoss(blank=BLANK_IDX, reduction='mean', zero_infinity=True).to(DIVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nMorseNet - инициалицация модели. Число обучаемых параметров: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa7f726",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c23dee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "def ctc_decoder(logits, int_char_map, blank_label_idx):\n",
    "    preds = []\n",
    "    logits_cpu = logits.cpu() \n",
    "    max_inds = torch.argmax(logits_cpu.detach(), dim=2).t().numpy() # арзмакс по лагитам и преобразование к словарю\n",
    "    \n",
    "    for ind in max_inds:\n",
    "        merged_inds = []\n",
    "        for idx in ind:\n",
    "            if idx != blank_label_idx: \n",
    "                merged_inds.append(idx)\n",
    "        text = \"\".join([int_char_map.get(i, '?') for i in merged_inds])\n",
    "        preds.append(text)\n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "422ea6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 1/70 =====\n",
      "Mean grad norm: 0.023675\n",
      "Max grad norm: 0.734516\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 4.1222\n",
      "---- Val Loss: 4.0273\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 2/70 =====\n",
      "Mean grad norm: 0.025047\n",
      "Max grad norm: 0.688410\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 4.0209\n",
      "---- Val Loss: 4.0110\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 3/70 =====\n",
      "Mean grad norm: 0.044003\n",
      "Max grad norm: 0.767650\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 4.0013\n",
      "---- Val Loss: 4.0116\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 4/70 =====\n",
      "Mean grad norm: 0.042930\n",
      "Max grad norm: 0.555077\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 3.9749\n",
      "---- Val Loss: 4.0334\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 5/70 =====\n",
      "Mean grad norm: 0.094253\n",
      "Max grad norm: 0.758000\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 2.1378\n",
      "---- Val Loss: 0.5695\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 6/70 =====\n",
      "Mean grad norm: 0.091575\n",
      "Max grad norm: 0.465354\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.7556\n",
      "---- Val Loss: 0.3405\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 7/70 =====\n",
      "Mean grad norm: 0.083576\n",
      "Max grad norm: 0.845196\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.5862\n",
      "---- Val Loss: 0.2929\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 8/70 =====\n",
      "Mean grad norm: 0.080779\n",
      "Max grad norm: 0.553523\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.5125\n",
      "---- Val Loss: 0.2640\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 9/70 =====\n",
      "Mean grad norm: 0.094130\n",
      "Max grad norm: 0.400904\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.4609\n",
      "---- Val Loss: 0.2280\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 10/70 =====\n",
      "Mean grad norm: 0.063396\n",
      "Max grad norm: 0.291099\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.4324\n",
      "---- Val Loss: 0.2276\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 11/70 =====\n",
      "Mean grad norm: 0.100314\n",
      "Max grad norm: 0.689743\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.4059\n",
      "---- Val Loss: 0.2587\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 12/70 =====\n",
      "Mean grad norm: 0.066257\n",
      "Max grad norm: 0.415017\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.3917\n",
      "---- Val Loss: 0.1945\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 13/70 =====\n",
      "Mean grad norm: 0.093376\n",
      "Max grad norm: 0.480987\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.3768\n",
      "---- Val Loss: 0.1985\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 14/70 =====\n",
      "Mean grad norm: 0.100119\n",
      "Max grad norm: 0.699742\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.3663\n",
      "---- Val Loss: 0.1859\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 15/70 =====\n",
      "Mean grad norm: 0.067964\n",
      "Max grad norm: 0.273582\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.3589\n",
      "---- Val Loss: 0.1802\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 16/70 =====\n",
      "Mean grad norm: 0.102547\n",
      "Max grad norm: 0.616991\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.3452\n",
      "---- Val Loss: 0.1763\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 17/70 =====\n",
      "Mean grad norm: 0.063354\n",
      "Max grad norm: 0.324068\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.3377\n",
      "---- Val Loss: 0.1841\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 18/70 =====\n",
      "Mean grad norm: 0.113041\n",
      "Max grad norm: 0.533269\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.3299\n",
      "---- Val Loss: 0.1828\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 19/70 =====\n",
      "Mean grad norm: 0.103012\n",
      "Max grad norm: 0.710960\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.3269\n",
      "---- Val Loss: 0.1651\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 20/70 =====\n",
      "Mean grad norm: 0.104565\n",
      "Max grad norm: 0.552992\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.3194\n",
      "---- Val Loss: 0.1563\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 21/70 =====\n",
      "Mean grad norm: 0.111411\n",
      "Max grad norm: 0.456038\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.3046\n",
      "---- Val Loss: 0.1527\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 22/70 =====\n",
      "Mean grad norm: 0.053960\n",
      "Max grad norm: 0.253663\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.3099\n",
      "---- Val Loss: 0.1693\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 23/70 =====\n",
      "Mean grad norm: 0.088423\n",
      "Max grad norm: 0.632960\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.3004\n",
      "---- Val Loss: 0.1731\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 24/70 =====\n",
      "Mean grad norm: 0.039369\n",
      "Max grad norm: 0.137050\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.2904\n",
      "---- Val Loss: 0.1511\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 25/70 =====\n",
      "Mean grad norm: 0.056855\n",
      "Max grad norm: 0.234517\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.2821\n",
      "---- Val Loss: 0.1469\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 26/70 =====\n",
      "Mean grad norm: 0.099585\n",
      "Max grad norm: 0.406696\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.2742\n",
      "---- Val Loss: 0.1477\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 27/70 =====\n",
      "Mean grad norm: 0.065297\n",
      "Max grad norm: 0.382135\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.2732\n",
      "---- Val Loss: 0.1526\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 28/70 =====\n",
      "Mean grad norm: 0.068043\n",
      "Max grad norm: 0.327657\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.2620\n",
      "---- Val Loss: 0.1539\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 29/70 =====\n",
      "Mean grad norm: 0.059419\n",
      "Max grad norm: 0.392717\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.001000\n",
      "---- Train Loss: 0.2607\n",
      "---- Val Loss: 0.1547\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 30/70 =====\n",
      "Mean grad norm: 0.082096\n",
      "Max grad norm: 0.508613\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.001000\n",
      "---- Train Loss: 0.2396\n",
      "---- Val Loss: 0.1351\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 31/70 =====\n",
      "Mean grad norm: 0.055707\n",
      "Max grad norm: 0.290325\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.001000\n",
      "---- Train Loss: 0.2272\n",
      "---- Val Loss: 0.1422\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 32/70 =====\n",
      "Mean grad norm: 0.073551\n",
      "Max grad norm: 0.398146\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.001000\n",
      "---- Train Loss: 0.2237\n",
      "---- Val Loss: 0.1371\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 33/70 =====\n",
      "Mean grad norm: 0.044252\n",
      "Max grad norm: 0.261727\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.001000\n",
      "---- Train Loss: 0.2162\n",
      "---- Val Loss: 0.1449\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 34/70 =====\n",
      "Mean grad norm: 0.034773\n",
      "Max grad norm: 0.155621\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000500\n",
      "---- Train Loss: 0.2159\n",
      "---- Val Loss: 0.1364\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 35/70 =====\n",
      "Mean grad norm: 0.036047\n",
      "Max grad norm: 0.161007\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000500\n",
      "---- Train Loss: 0.2045\n",
      "---- Val Loss: 0.1416\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 36/70 =====\n",
      "Mean grad norm: 0.055027\n",
      "Max grad norm: 0.323920\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000500\n",
      "---- Train Loss: 0.1997\n",
      "---- Val Loss: 0.1403\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 37/70 =====\n",
      "Mean grad norm: 0.100560\n",
      "Max grad norm: 0.628666\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000500\n",
      "---- Train Loss: 0.1963\n",
      "---- Val Loss: 0.1407\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 38/70 =====\n",
      "Mean grad norm: 0.114679\n",
      "Max grad norm: 0.459898\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000250\n",
      "---- Train Loss: 0.1965\n",
      "---- Val Loss: 0.1391\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 39/70 =====\n",
      "Mean grad norm: 0.052316\n",
      "Max grad norm: 0.280666\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000250\n",
      "---- Train Loss: 0.1891\n",
      "---- Val Loss: 0.1407\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 40/70 =====\n",
      "Mean grad norm: 0.051712\n",
      "Max grad norm: 0.273996\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000250\n",
      "---- Train Loss: 0.1856\n",
      "---- Val Loss: 0.1400\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 41/70 =====\n",
      "Mean grad norm: 0.095474\n",
      "Max grad norm: 0.610184\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000250\n",
      "---- Train Loss: 0.1860\n",
      "---- Val Loss: 0.1410\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 42/70 =====\n",
      "Mean grad norm: 0.049080\n",
      "Max grad norm: 0.240725\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000125\n",
      "---- Train Loss: 0.1842\n",
      "---- Val Loss: 0.1402\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 43/70 =====\n",
      "Mean grad norm: 0.071425\n",
      "Max grad norm: 0.341813\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000125\n",
      "---- Train Loss: 0.1796\n",
      "---- Val Loss: 0.1417\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 44/70 =====\n",
      "Mean grad norm: 0.056629\n",
      "Max grad norm: 0.385452\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000125\n",
      "---- Train Loss: 0.1775\n",
      "---- Val Loss: 0.1409\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 45/70 =====\n",
      "Mean grad norm: 0.045382\n",
      "Max grad norm: 0.280828\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000125\n",
      "---- Train Loss: 0.1774\n",
      "---- Val Loss: 0.1416\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 46/70 =====\n",
      "Mean grad norm: 0.041747\n",
      "Max grad norm: 0.169365\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000063\n",
      "---- Train Loss: 0.1788\n",
      "---- Val Loss: 0.1456\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 47/70 =====\n",
      "Mean grad norm: 0.063846\n",
      "Max grad norm: 0.308806\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000063\n",
      "---- Train Loss: 0.1764\n",
      "---- Val Loss: 0.1427\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 48/70 =====\n",
      "Mean grad norm: 0.040220\n",
      "Max grad norm: 0.176586\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000063\n",
      "---- Train Loss: 0.1756\n",
      "---- Val Loss: 0.1433\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 49/70 =====\n",
      "Mean grad norm: 0.040122\n",
      "Max grad norm: 0.199680\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000063\n",
      "---- Train Loss: 0.1734\n",
      "---- Val Loss: 0.1441\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 50/70 =====\n",
      "Mean grad norm: 0.044917\n",
      "Max grad norm: 0.237614\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000031\n",
      "---- Train Loss: 0.1743\n",
      "---- Val Loss: 0.1440\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 51/70 =====\n",
      "Mean grad norm: 0.059349\n",
      "Max grad norm: 0.297782\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000031\n",
      "---- Train Loss: 0.1735\n",
      "---- Val Loss: 0.1442\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 52/70 =====\n",
      "Mean grad norm: 0.065853\n",
      "Max grad norm: 0.323859\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000031\n",
      "---- Train Loss: 0.1759\n",
      "---- Val Loss: 0.1440\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 53/70 =====\n",
      "Mean grad norm: 0.080306\n",
      "Max grad norm: 0.445562\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000031\n",
      "---- Train Loss: 0.1704\n",
      "---- Val Loss: 0.1458\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 54/70 =====\n",
      "Mean grad norm: 0.095591\n",
      "Max grad norm: 0.724328\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000016\n",
      "---- Train Loss: 0.1755\n",
      "---- Val Loss: 0.1435\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 55/70 =====\n",
      "Mean grad norm: 0.108141\n",
      "Max grad norm: 0.461031\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000016\n",
      "---- Train Loss: 0.1732\n",
      "---- Val Loss: 0.1453\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 56/70 =====\n",
      "Mean grad norm: 0.038870\n",
      "Max grad norm: 0.184287\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000016\n",
      "---- Train Loss: 0.1699\n",
      "---- Val Loss: 0.1448\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 57/70 =====\n",
      "Mean grad norm: 0.106196\n",
      "Max grad norm: 0.474125\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000016\n",
      "---- Train Loss: 0.1723\n",
      "---- Val Loss: 0.1454\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 58/70 =====\n",
      "Mean grad norm: 0.045327\n",
      "Max grad norm: 0.178904\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000008\n",
      "---- Train Loss: 0.1721\n",
      "---- Val Loss: 0.1441\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m     optimizer.zero_grad(); \n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m     42\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "lst_loss_train = []\n",
    "lst_loss_val = []\n",
    "best_val_loss = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "\n",
    "    epoch_train_loss = 0.0\n",
    "    train_predicts = []\n",
    "\n",
    "    train_tqdm = tqdm(train_dl, desc=f\"Эпоха {epoch+1}/{EPOCHS} [Обучение]\", leave=False)\n",
    "    for batch_ind, batch in enumerate(train_tqdm):\n",
    "        mel_spec, targets, targets_lens, _ = batch\n",
    "        mel_spec, targets, targets_lens = mel_spec.to(DIVICE), targets.to(DIVICE), targets_lens.to(DIVICE)\n",
    "\n",
    "        #===== считатем длинну mel_spec для передачи в CTC loss =====\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predict = model(mel_spec) # (N=batch,T,C)\n",
    "        N = predict.shape[1]\n",
    "        T = predict.shape[0]\n",
    "        predict_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "\n",
    "    #     print(\"Predict shape:\", predict.shape) # [T, N, C]\n",
    "    #     print(\"Labels shape:\", targets.shape)   # [N, max_label_len]\n",
    "    #     print(\"Predict lengths:\", predict_lengths) # [N]\n",
    "    #     print(\"Target lengths:\", targets_lens.reshape(BATCH_SIZE))   # [N]\n",
    "    #     break\n",
    "    # break\n",
    "        try:\n",
    "            loss = loss_func(predict, targets, predict_lengths, targets_lens.reshape(BATCH_SIZE))\n",
    "        except RuntimeError:\n",
    "            print(predict.shape, targets.shape, predict_lengths, targets_lens.reshape(BATCH_SIZE))\n",
    "            continue\n",
    "        # print(loss)\n",
    "        if torch.isnan(loss) or torch.isinf(loss): \n",
    "            print(f\"\\nWarning: In batch-{batch_ind} loss train is NaN/Inf: {loss.item()}\"); \n",
    "            optimizer.zero_grad(); \n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "    train_loss = epoch_train_loss / len(train_dl)\n",
    "\n",
    "    # ======== Валидация ========\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    total_val = 0\n",
    "    val_predicts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_mel_spec, val_labels, val_label_lensin, _ in tqdm(\n",
    "                                                        val_dl, \n",
    "                                                        desc=f\"Эпоха {epoch+1}/{EPOCHS} [Валидация]\", \n",
    "                                                        leave=False):\n",
    "            val_mel_spec, val_labels, val_label_lensin = val_mel_spec.to(DIVICE), val_labels.to(DIVICE), val_label_lensin.to(DIVICE)\n",
    "            val_predict = model(val_mel_spec)\n",
    "\n",
    "            val_N = val_predict.shape[1]\n",
    "            val_T = val_predict.shape[0]\n",
    "            predict_val_lengths = torch.full(size=(val_N,), fill_value=val_T, dtype=torch.long)\n",
    "            val_loss += loss_func(val_predict, val_labels, predict_val_lengths, val_label_lensin).item()\n",
    "\n",
    "    total_val = val_loss / len(val_dl)\n",
    "\n",
    "    lst_loss_train.append(train_loss)\n",
    "    lst_loss_val.append(total_val)\n",
    "\n",
    "    scheduler.step(total_val)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f\"Новая лучшая модель сохранена с val_loss: {val_loss:.4f}\")\n",
    "    \n",
    "\n",
    "    print(f\"\\n===== Эпоха {epoch+1}/{EPOCHS} =====\")\\\n",
    "    #===== Инфо про градиенты=====\n",
    "    grad_norms = [param.grad.norm().item() for param in model.parameters() if param.grad is not None]\n",
    "    if grad_norms:\n",
    "        print(f\"Mean grad norm: {np.mean(grad_norms):.6f}\")\n",
    "        print(f\"Max grad norm: {np.max(grad_norms):.6f}\")\n",
    "        print(f\"Min grad norm: {np.min(grad_norms):.6f}\")\n",
    "    else:\n",
    "        print(\"No gradients computed yet.\")\n",
    "    #===== Инфо про шаг обучения и данные по потерям =====\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Current LR: {current_lr:.6f}\")\n",
    "    print(f\"---- Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"---- Val Loss: {total_val:.4f}\")\n",
    "    if current_lr <= 1e-6:\n",
    "        print(\"Learning rate достиг минимума 1e-6, остановка обучения\")\n",
    "        break\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca52e14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'MorseNet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1547a2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAATBRJREFUeJzt3Xl8W/Wd7//X0e5NdlY7O6EJZCMhCZvhtqElC0sp6fRyO8D8wjCFmbZhftD0wjTzmymktJgZSilTGCiXtrntnUwg3Et6CwHiBpIUklCylYQlJCxxSmxn9W5LR9L5/XEkWY6XWLIl2db7+Xich6Tjo6OvPxj05vv9nu8xLMuyEBEREckSR7YbICIiIrlNYURERESySmFEREREskphRERERLJKYURERESySmFEREREskphRERERLJKYURERESyypXtBvRGJBLh6NGjFBUVYRhGtpsjIiIivWBZFo2NjYwdOxaHo/v+j0ERRo4ePcqECROy3QwRERFJwZEjRxg/fny3Px8UYaSoqAiwfxm/399v5zVNk40bN7J48WLcbne/nTcXqHapU+36RvVLnWqXOtUuNQ0NDUyYMCH+Pd6dQRFGYkMzfr+/38NIfn4+fr9ff1xJUu1Sp9r1jeqXOtUudapd35xtioUmsIqIiEhWKYyIiIhIVimMiIiISFYpjIiIiEhWKYyIiIhIVimMiIiISFYpjIiIiEhWKYyIiIhIVimMiIiISFYpjIiIiEhWKYyIiIhIVimMiIiISFbldBh586OT/Px9B21mONtNERERyVk5G0ZagiFWrHuH9+oc3P3cO4TCkWw3SUREJCflbBjJ97j42V/OwW1YbPrgOPf+73eIRKxsN0tERCTn5GwYAbjknOH89XkRnA6D/7P7M3740vtYlgKJiIhIJuV0GAGYNdziX746E4BfvvkJj792KMstEhERyS05H0YAbrhwLPddPwOARyo/5Dc7Dme5RSIiIrlDYSTqtism8/9eNRWA7/92P7/d+1mWWyQiIpIbFEYSfGfhVG4tn4RlwXef+xOvHzjWtxNGdMmwiIjI2fQpjDz00EMYhsHdd9/d43Hr1q1j2rRp+Hw+LrjgAjZs2NCXj+03jreeZOZn/4nxzlo4ugcj1MZ918/khgvHEopYfOt/7WLnp6d6f8KWU3DgFfj9/fDLq+FHY+CZhRDRZcMiIiLdcaX6xrfffpuf//znzJ49u8fjtm3bxk033URFRQVf/vKXWbNmDUuXLmX37t3MmjUr1Y/vF8b+55ly7E/wu5ejOxw4hp/Lo6Om88XSEl49PpwHn/mIKaMLmTq6gPNHFzBldCFj/B4MACsCpw/DkR1QtQOOf9D5Q/78NgQbwVecyV9NRERk0EgpjDQ1NXHLLbfwP/7H/+CHP/xhj8c+9thjXH311dxzzz0APPDAA1RWVvL444/z1FNPpfLx/SZy8d/y6Y71nJPfguPY+9B6Ck4ewnHyEEuBpZ7ogaeiWxdZo5MRU2DiZTDhUvi/f2/vCwXT0XwREZEhIaUwsnz5cq677joWLlx41jCyfft2VqxY0WHfkiVLWL9+fbfvCQQCBAKB+OuGhgYATNPENM1Umtwlc/pfsO/PRZQtWoTb5YLmYxjH3sc4/j7Gsffh+HuEG2owwxbBMATCEAxbhC2DCAYWBqcpYq91HnUj5zN82hVcMmMq55UWYhgGrhdXYERMzEAzeEv6rd0DQeyfQ3/+88gVql3fqH6pU+1Sp9qlprf1SjqMrF27lt27d/P222/36viamhpKS0s77CstLaWmpqbb91RUVLBq1apO+zdu3Eh+fn5yDe6FysrKM/ZMAuckKLsayjr+JByBoy1Q1WxwuNHgYIPBqaAB1UD1CXj9BMVui2klFr+2nHgx2bJpI83eUoaizrWT3lLt+kb1S51qlzrVLjktLS29Oi6pMHLkyBHuuusuKisr8fl8KTWsN1auXNmhN6WhoYEJEyawePFi/H5/v32OaZpUVlayaNEi3G53SuewLItPT7aw9eAJ/nDoJG99cop6M8Jbxw1avC68Bnz+8stwlE7vt3YPBP1Ru1yl2vWN6pc61S51ql1qYiMbZ5NUGNm1axfHjh1j3rx58X3hcJitW7fy+OOPEwgEcDqdHd5TVlZGbW1th321tbWUlZ3R5ZDA6/Xi9Xo77Xe73Wn5I+jrec8b4+G8MSXc/oUptJlh3v70FJsPHCf4tl3eQDCAf4j+8abrn0kuUO36RvVLnWqXOtUuOb2tVVKX9l511VXs27ePvXv3xreLLrqIW265hb1793YKIgDl5eVs2rSpw77KykrKy8uT+ehBw+d28vmpo/jnL8/AxP6H0NraluVWiYiIDFxJ9YwUFRV1uhy3oKCAESNGxPcvW7aMcePGUVFRAcBdd93FggULeOSRR7juuutYu3YtO3fu5Omnn+6nX2HgChl2GGkL9G7MTEREJBf1+wqsVVVVVFdXx19ffvnlrFmzhqeffpo5c+bw/PPPs379+qyvMZIJIcO+NjjQ1prlloiIiAxcKS96FrN58+YeXwPceOON3HjjjX39qEEn7HBDGIJtGqYRERHpju5Nk0YRh90zEgyoZ0RERKQ7CiNpFHHGwoh6RkRERLqjMJJGVjSMhIIKIyIiIt1RGEknp71WisKIiIhI9xRG0klhRERE5KwURtLIcNnDNBEzcJYjRUREcpfCSBoZbrtnJGyqZ0RERKQ7CiNp5HDbNxOMhNQzIiIi0h2FkTRyRntGLIURERGRbimMpJErGkZQGBEREemWwkgauTz2MA3hYHYbIiIiMoApjKRRLIwYYTPLLRERERm4FEbSyO2NhREN04iIiHRHYSSNvN48AJwRE8uystwaERGRgUlhJI080TDixqTVDGe5NSIiIgOTwkgaeaLDNB5MmgKhLLdGRERkYFIYSSPDZV/a6zFCNAfUMyIiItIVhZF0ioURTJrVMyIiItIlhZF0cto3yvMSorFNYURERKQrCiPppJ4RERGRs1IYSSdnLIyEaA4qjIiIiHRFYSSdXPYwjcfQ1TQiIiLdURhJp8SeEYURERGRLimMpFOsZwSTJl3aKyIi0iWFkXRSz4iIiMhZKYykU/TSXrcRpqUtmOXGiIiIDEwKI+kUHaYBaGtrzWJDREREBi6FkXSKDtMABAJtWWyIiIjIwKUwkk7O9p6RYJvCiIiISFcURtLJ4SDicAMQDGqYRkREpCsKI2lmOezekVBQPSMiIiJdSSqMPPnkk8yePRu/34/f76e8vJyXX3652+NXr16NYRgdNp/P1+dGDyZWdBKrqTkjIiIiXXIlc/D48eN56KGHmDp1KpZl8T//5//khhtuYM+ePcycObPL9/j9fg4cOBB/bRhG31o82EQnsapnREREpGtJhZHrr7++w+sf/ehHPPnkk+zYsaPbMGIYBmVlZam3cJAzonfuNcIBQuEILqdGxkRERBIlFUYShcNh1q1bR3NzM+Xl5d0e19TUxKRJk4hEIsybN48HH3yw2+ASEwgECAQC8dcNDQ0AmKaJaZqpNrmT2Ln685xncjpjN8sLUdfcRnGeO22flUmZqN1Qpdr1jeqXOtUudapdanpbL8OyLCuZE+/bt4/y8nLa2tooLCxkzZo1XHvttV0eu337dg4ePMjs2bOpr6/nxz/+MVu3buXdd99l/Pjx3X7G/fffz6pVqzrtX7NmDfn5+ck0N+uu/OCfKG6tYlnwH/jihTMZ7j37e0RERIaClpYWbr75Zurr6/H7/d0el3QYCQaDVFVVUV9fz/PPP88zzzzDli1bmDFjxlnfa5om06dP56abbuKBBx7o9riuekYmTJjAiRMnevxlkmWaJpWVlSxatAi3Oz09Fs5fLcZxdDe3B7/Lim//PVNLC9PyOZmWidoNVapd36h+qVPtUqfapaahoYGRI0eeNYwkPUzj8XiYMmUKAPPnz+ftt9/mscce4+c///lZ3+t2u5k7dy6HDh3q8Tiv14vX27kLwe12p+WPIF3nBcBlXz3kwaQtwpD7I05r7YY41a5vVL/UqXapU+2S09ta9Xk2ZSQS6dCL0ZNwOMy+ffsYM2ZMXz928Ihe2qs794qIiHQtqZ6RlStXcs011zBx4kQaGxtZs2YNmzdv5tVXXwVg2bJljBs3joqKCgB+8IMfcNlllzFlyhTq6up4+OGHOXz4MLfffnv//yYDVfTSXo9hKoyIiIh0IakwcuzYMZYtW0Z1dTXFxcXMnj2bV199lUWLFgFQVVWFw9He2XL69GnuuOMOampqGDZsGPPnz2fbtm29ml8yZCT0jDQFwllujIiIyMCTVBj5xS9+0ePPN2/e3OH1o48+yqOPPpp0o4aU6KW9XtQzIiIi0hWtwJVu0WEaNyGaFEZEREQ6URhJtw7DNAojIiIiZ1IYSTdNYBUREemRwki6qWdERESkRwoj6RbrGdEEVhERkS4pjKSbKzGM6NJeERGRMymMpFvs0l5DwzQiIiJdURhJN5eGaURERHqiMJJuTt2bRkREpCcKI+mW0DOiYRoREZHOFEbSLX41TYjmYBjLsrLcIBERkYFFYSTdYuuMGCbhiEUgFMlyg0RERAYWhZF0S+gZATRUIyIicgaFkXSL9oz4DDuEaBKriIhIRwoj6RbtGYmFkcY2hREREZFECiPpFru0Vz0jIiIiXVIYSbeEG+UBNAcVRkRERBIpjKRbwo3yAJp0fxoREZEOFEbSLdoz4o6GEQ3TiIiIdKQwkm7RnhG3pTAiIiLSFYWRdIsuB+8kjEFE64yIiIicQWEk3aJX04BuliciItIVhZF0i/aMAHgxNYFVRETkDAoj6aaeERERkR4pjKSbYbQvfIapMCIiInIGhZFMiK01YpiawCoiInIGhZFMSFiFVWFERESkI4WRTEhYhVXDNCIiIh0pjGRCh54RXU0jIiKSSGEkE6I9I15DPSMiIiJnUhjJBGd7z0irGSYcsbLcIBERkYEjqTDy5JNPMnv2bPx+P36/n/Lycl5++eUe37Nu3TqmTZuGz+fjggsuYMOGDX1q8KAUv1me3SvSHFTviIiISExSYWT8+PE89NBD7Nq1i507d/KlL32JG264gXfffbfL47dt28ZNN93EN77xDfbs2cPSpUtZunQp+/fv75fGDxrRYZo8IxpGNFQjIiISl1QYuf7667n22muZOnUq5513Hj/60Y8oLCxkx44dXR7/2GOPcfXVV3PPPfcwffp0HnjgAebNm8fjjz/eL40fNKI9I0XuCKAwIiIiksiV6hvD4TDr1q2jubmZ8vLyLo/Zvn07K1as6LBvyZIlrF+/vsdzBwIBAoFA/HVDQwMApmlimmaqTe4kdq7+PGdXnA43DqDQZV9JU9ccwDR9af3MdMtU7YYi1a5vVL/UqXapU+1S09t6JR1G9u3bR3l5OW1tbRQWFvLCCy8wY8aMLo+tqamhtLS0w77S0lJqamp6/IyKigpWrVrVaf/GjRvJz89PtslnVVlZ2e/nTHTx8dOMBdyhFgBe+8M2PiseGpNY0127oUy16xvVL3WqXepUu+S0tLT06rikw8j555/P3r17qa+v5/nnn+fWW29ly5Yt3QaSVKxcubJDj0pDQwMTJkxg8eLF+P3+fvsc0zSprKxk0aJFuN3ufjvvmZzr10P9TkYUuKANZs2Zz6IZo9P2eZmQqdoNRapd36h+qVPtUqfapSY2snE2SYcRj8fDlClTAJg/fz5vv/02jz32GD//+c87HVtWVkZtbW2HfbW1tZSVlfX4GV6vF6/X22m/2+1Oyx9Bus7b/gF5ABS47DkjbWFryPwxp712Q5hq1zeqX+pUu9Spdsnpba36vM5IJBLpML8jUXl5OZs2beqwr7Kysts5JkNWdJ2RAqc9Z0T3pxEREWmXVM/IypUrueaaa5g4cSKNjY2sWbOGzZs38+qrrwKwbNkyxo0bR0VFBQB33XUXCxYs4JFHHuG6665j7dq17Ny5k6effrr/f5OBzBW9tNehMCIiInKmpMLIsWPHWLZsGdXV1RQXFzN79mxeffVVFi1aBEBVVRUOR3tny+WXX86aNWv4p3/6J/7xH/+RqVOnsn79embNmtW/v8VAF+0ZyXNonREREZEzJRVGfvGLX/T4882bN3fad+ONN3LjjTcm1aghJ9oz4osveqab5YmIiMTo3jSZEF2B1RftGdEwjYiISDuFkUyIrsDq1XLwIiIinSiMZEK0Z8SDekZERETOpDCSCdGeEQ/2srjqGREREWmnMJIJ0atp3GgCq4iIyJkURjIhOkzjtuyeEQ3TiIiItFMYyYToMI0rGkaagwojIiIiMQojmRDtGXFaQUBzRkRERBIpjGRCtGfEGbF7RsywRSCkeSMiIiKgMJIZ0Z4RRyQY39XUpt4RERERUBjJjOhy8EYoSJ7bCeiKGhERkRiFkUyIXtpLOECB174dkK6oERERsSmMZEK0Z4RQgEJvtGdEV9SIiIgACiOZEe8ZCapnRERE5AwKI5mQ0DMSCyO6vFdERMSmMJIJ0atpsMIUeQxAYURERCRGYSQTouuMABR7LACadDWNiIgIoDCSGbGeEaDYHQHUMyIiIhKjMJIJTnf8abFHYURERCSRwkgmGEa8d6TIFRumURgREREBhZHMiV7eW+S254qoZ0RERMSmMJIp0UmshS57mEY9IyIiIjaFkUyJDtMUOBVGREREEimMZEq0Z6TAZYcQ3ShPRETEpjCSKbGeEYfmjIiIiCRSGMmUaM9IXjSMaJhGRETEpjCSKdGekTxHbJhGYURERAQURjLHFQsj0WGaYJhIxMpmi0RERAYEhZFMia4z4jPae0RaTE1iFRERURjJlGjPiBsTp0N37hUREYlRGMmUaM+IEQ5S4HECmsQqIiICSYaRiooKLr74YoqKihg9ejRLly7lwIEDPb5n9erVGIbRYfP5fH1q9KAU7RkhFKDQ6wLUMyIiIgJJhpEtW7awfPlyduzYQWVlJaZpsnjxYpqbm3t8n9/vp7q6Or4dPny4T40elKJX0xAOUBANI+oZERERAVcyB7/yyisdXq9evZrRo0eza9cuvvCFL3T7PsMwKCsrS62FQ0V0nRFCwXgY0SqsIiIiSYaRM9XX1wMwfPjwHo9rampi0qRJRCIR5s2bx4MPPsjMmTO7PT4QCBAIBOKvGxoaADBNE9M0+9LkDmLn6s9zdsdhuHECYbOVfI/dIVXf3JaRz06HTNZuqFHt+kb1S51qlzrVLjW9rZdhWVZKi11EIhG+8pWvUFdXxxtvvNHtcdu3b+fgwYPMnj2b+vp6fvzjH7N161beffddxo8f3+V77r//flatWtVp/5o1a8jPz0+luVk347NnmXrsJQ6NupoVzX/FO6cc3Dg5zH8p01ojIiIyNLW0tHDzzTdTX1+P3+/v9riUw8i3vvUtXn75Zd54441uQ0VXTNNk+vTp3HTTTTzwwANdHtNVz8iECRM4ceJEj79MskzTpLKykkWLFuF2u/vtvF1xbK7A+eYjhOd/g//e/Fe8sLeaexZP5W8/Pzmtn5sumazdUKPa9Y3qlzrVLnWqXWoaGhoYOXLkWcNISsM0d955Jy+++CJbt25NKogAuN1u5s6dy6FDh7o9xuv14vV6u3xvOv4I0nXeDjx5ADgtk6I8e/5IW8ga9H/UGandEKXa9Y3qlzrVLnWqXXJ6W6ukrqaxLIs777yTF154gddee43Jk5P/v/pwOMy+ffsYM2ZM0u8d1LqYwKqraURERJLsGVm+fDlr1qzht7/9LUVFRdTU1ABQXFxMXp79f/7Lli1j3LhxVFRUAPCDH/yAyy67jClTplBXV8fDDz/M4cOHuf322/v5VxngEi7t1TojIiIi7ZIKI08++SQAV155ZYf9v/rVr/jrv/5rAKqqqnA42jtcTp8+zR133EFNTQ3Dhg1j/vz5bNu2jRkzZvSt5YNNYs9IdAVWXdorIiKSZBjpzVzXzZs3d3j96KOP8uijjybVqCEpsWfEZ4+haZhGRERE96bJnA7Lwcd6RhRGREREFEYyJXqjPMKawCoiIpJIYSRTEnpG4svBBxVGREREFEYyJaFnpFD3phEREYlTGMmULnpGmtrUMyIiIqIwkinxq2mCFHrsMBIMRwiGIllslIiISPYpjGRKfJ2RAAXRq2lAV9SIiIgojGRKwjojLqcDr8suva6oERGRXKcwkimxnpGwCdA+iVVX1IiISI5TGMkUZ/swDdB+ea96RkREJMcpjGRKbJjGCkMknLDwmS7vFRGR3KYwkimxYRqAUIAi9YyIiIgACiOZE+sZAQi3X1GjCawiIpLrFEYyxelufx4Kas6IiIhIlMJIphhGh8t7CxVGREREAIWRzIovCR/UBFYREZEohZFMit8sL+H+NAEziw0SERHJPoWRTEq4WV5hdAKr7twrIiK5TmEkk+I9I4nDNJozIiIiuU1hJJM69IxoAquIiAgojGRWYs+IR2FEREQEFEYyK6FnRMM0IiIiNoWRTOpynRFNYBURkdymMJJJsfvThE0KfeoZERERAYWRzHK2D9P4E8JIOGJlsVEiIiLZpTCSSbH704SD+PPa71XT0KqFz0REJHcpjGRSwgRWt9MRnzdSrzAiIiI5TGEkkxImsAIUR3tH6hRGREQkhymMZFJsAmsoCBAfqlHPiIiI5DKFkUw6o2ekJNYz0hLMVotERESyTmEkk87oGYkN02gCq4iI5LKkwkhFRQUXX3wxRUVFjB49mqVLl3LgwIGzvm/dunVMmzYNn8/HBRdcwIYNG1Ju8KB2Zs9IvoZpREREkgojW7ZsYfny5ezYsYPKykpM02Tx4sU0Nzd3+55t27Zx00038Y1vfIM9e/awdOlSli5dyv79+/vc+EEn3jNyxgTWFoURERHJXa5kDn7llVc6vF69ejWjR49m165dfOELX+jyPY899hhXX30199xzDwAPPPAAlZWVPP744zz11FMpNnuQiveMaAKriIhITFJh5Ez19fUADB8+vNtjtm/fzooVKzrsW7JkCevXr+/2PYFAgEAgEH/d0NAAgGmamGb/fXHHztWf5+yJw3DhBCJmG2HTpMhrd0ydbg5krA39JdO1G0pUu75R/VKn2qVOtUtNb+uVchiJRCLcfffdXHHFFcyaNavb42pqaigtLe2wr7S0lJqamm7fU1FRwapVqzrt37hxI/n5+ak2uVuVlZX9fs6uTDrxIRcCtUer+OOGDXx8wgCcfPLZsUE7jyZTtRuKVLu+Uf1Sp9qlTrVLTktLS6+OSzmMLF++nP379/PGG2+keopurVy5skNvSkNDAxMmTGDx4sX4/f5++xzTNKmsrGTRokW43e6zv6GPjHca4MivKB0xjGuvvZbij06y+uAuXPlFXHvt5Wn//P6U6doNJapd36h+qVPtUqfapSY2snE2KYWRO++8kxdffJGtW7cyfvz4Ho8tKyujtra2w77a2lrKysq6fY/X68Xr9Xba73a70/JHkK7zduLJA8ARMXG43YwotF/Xt4YG7R93xmo3BKl2faP6pU61S51ql5ze1iqpq2ksy+LOO+/khRde4LXXXmPy5MlnfU95eTmbNm3qsK+yspLy8vJkPnpoSLg3DbRfTaMJrCIiksuS6hlZvnw5a9as4be//S1FRUXxeR/FxcXk5dn/l79s2TLGjRtHRUUFAHfddRcLFizgkUce4brrrmPt2rXs3LmTp59+up9/lUHgjKtpiqPrjLSaYQKhMF6XM1stExERyZqkekaefPJJ6uvrufLKKxkzZkx8e/bZZ+PHVFVVUV1dHX99+eWXs2bNGp5++mnmzJnD888/z/r163uc9DpkxdYZiYaRIq8Lw7B3qXdERERyVVI9I5ZlnfWYzZs3d9p34403cuONNybzUUOTs+OiZw6Hgd/npr7VpKHVZHSRL4uNExERyQ7dmyaTzhimgfYl4bUKq4iI5CqFkUw6Yzl40CRWERERhZFMOuNGeaAwIiIiojCSSfGekfZhGt0sT0REcp3CSCapZ0RERKQThZFMii16ZkUgHALaJ7AqjIiISK5SGMmk2KW9EO8dUc+IiIjkOoWRTHIl3G8nekVNSZ4dUBRGREQkVymMZJLDBUSXXI2uNeKPT2ANdvMmERGRoU1hJJMMQzfLExEROYPCSKadsQpr+wTWULZaJCIiklUKI5l2xiqs7T0jwV7d+0dERGSoURjJtDN6RmJhxAxbtJrhbLVKREQkaxRGMi3WMxINI/keJ26nPalVq7CKiEguUhjJNGfHCayGYWgSq4iI5DSFkUxz2sEj1jMCuqJGRERym8JIpp1xaS/oZnkiIpLbFEYyrYeb5TWoZ0RERHKQwkimxS/tbR+mKcm399W1ahVWERHJPQojmdZDz4jmjIiISC5SGMm0LnpGFEZERCSXKYxkWg89I5rAKiIiuUhhJNPOWA4e1DMiIiK5TWEk085YDh4Sb5anMCIiIrlHYSTTelhnRGFERERykcJIpjk73psGFEZERCS3KYxkWlc9IwnDNJGIlY1WiYiIZI3CSKb10DNiWdAYCGWjVSIiIlmjMJJprs4TWL0uJ3luJ6Al4UVEJPcojGSas/MwDWitERERyV0KI5nm6jxMA5rEKiIiuSvpMLJ161auv/56xo4di2EYrF+/vsfjN2/ejGEYnbaamppU2zy4OTsvegbtk1h1szwREck1SYeR5uZm5syZwxNPPJHU+w4cOEB1dXV8Gz16dLIfPTR0MYEV1DMiIiK5y5XsG6655hquueaapD9o9OjRlJSUJP2+IaeLS3tBYURERHJX0mEkVRdeeCGBQIBZs2Zx//33c8UVV3R7bCAQIBBo/7JuaGgAwDRNTLP/vqxj5+rPc56NgRMXYIXaCCV8rt9rX01zqimQ0fakKhu1GypUu75R/VKn2qVOtUtNb+tlWJaV8ipbhmHwwgsvsHTp0m6POXDgAJs3b+aiiy4iEAjwzDPP8Jvf/Ia33nqLefPmdfme+++/n1WrVnXav2bNGvLz81Nt7oAwsvFdrjj0LzT4xvH69Ir4/lf/bLDhiJPy0RH+8nORLLZQRESkf7S0tHDzzTdTX1+P3+/v9ri0h5GuLFiwgIkTJ/Kb3/ymy5931TMyYcIETpw40eMvkyzTNKmsrGTRokW43e5+O29PjCM7cP36y1jDJhP69tvx/f/xVhX3v/gBi2eM5ombLsxIW/oiG7UbKlS7vlH9UqfapU61S01DQwMjR448axjJ2DBNoksuuYQ33nij2597vV68Xm+n/W63Oy1/BOk6b5e8ds+OETY7fOawQh8AjW3hQfWHntHaDTGqXd+ofqlT7VKn2iWnt7XKyjoje/fuZcyYMdn46OyLLXoW1gRWERERSKFnpKmpiUOHDsVff/LJJ+zdu5fhw4czceJEVq5cyWeffcavf/1rAH76058yefJkZs6cSVtbG8888wyvvfYaGzdu7L/fYjCJX03T8dLeknz7kl+FERERyTVJh5GdO3fyxS9+Mf56xYoVANx6662sXr2a6upqqqqq4j8PBoN897vf5bPPPiM/P5/Zs2fz+9//vsM5ckp8nRH1jIiIiEAKYeTKK6+kpzmvq1ev7vD63nvv5d577026YUNW4jojlgWGAUBJNIw0BUKY4Qhup1bqFxGR3KBvvEyL9YxgQSQU3+3Pa5/kozv3iohILlEYyTRXwlVCCUvCOx0GRV67o0pDNSIikksURjLNmRBGur1ZnsKIiIjkDoWRTHO6wIiWXTfLExERURjJiti8kW5ulqc5IyIikksURrIhvvDZmWuNRIdpWhRGREQkdyiMZIOr554RDdOIiEguURjJhm6XhLdDinpGREQklyiMZEO8Z0QTWEVERBRGskE3yxMREYlTGMmGbnpGYhNY61uDZ75DRERkyFIYyQb1jIiIiMQpjGRD4s3yEiiMiIhILlIYyYbYomfdrMCqq2lERCSXKIxkQ3c9I9E5I4FQhDYznOlWiYiIZIXCSDZ00zNS5HXhdBiAhmpERCR3KIxkg6vr5eANw8DvcwEKIyIikjsURrKhmxvlgSaxiohI7lEYyYZuekYAivO1JLyIiOQWhZFscHY9gRXUMyIiIrlHYSQbnHbg6KpnpCR+ea9WYRURkdygMJIN3VzaC+09Iw3qGRERkRyhMJIN8Ut7NUwjIiKiMJIN8Z6RLoZpoguf1SmMiIhIjlAYyYZubpQH4FfPiIiI5BiFkWxwxdYZ6X4Cq8KIiIjkCoWRbOihZyQ+Z0TrjIiISI5QGMmGHnpGYjfLU8+IiIjkCoWRbOihZ6QkL7oCa6uJZVmZbJWIiEhWKIxkQy/WGQlHLJqD4Uy2SkREJCsURrIhvs5I52Ean9uBx2n/Y9FQjYiI5IKkw8jWrVu5/vrrGTt2LIZhsH79+rO+Z/PmzcybNw+v18uUKVNYvXp1Ck0dQnq4UZ5hGPF5I1oSXkREckHSYaS5uZk5c+bwxBNP9Or4Tz75hOuuu44vfvGL7N27l7vvvpvbb7+dV199NenGDhnO7iewglZhFRGR3OJK9g3XXHMN11xzTa+Pf+qpp5g8eTKPPPIIANOnT+eNN97g0UcfZcmSJcl+/NDg6n4CKySsNaLLe0VEJAekfc7I9u3bWbhwYYd9S5YsYfv27en+6IHL2f1y8KCeERERyS1J94wkq6amhtLS0g77SktLaWhooLW1lby8vE7vCQQCBALtvQYNDQ0AmKaJafbfF3TsXP15zt5x4AascIBQF59d5HUCcKq5LQtt653s1W7wU+36RvVLnWqXOtUuNb2tV9rDSCoqKipYtWpVp/0bN24kPz+/3z+vsrKy38/ZE69Zx9UAoQAbXnoJDKPDz0/XOgAHu/cdYEPD+xltW7IyXbuhRLXrG9Uvdapd6lS75LS0tPTquLSHkbKyMmprazvsq62txe/3d9krArBy5UpWrFgRf93Q0MCECRNYvHgxfr+/39pmmiaVlZUsWrQIt9vdb+c9q9bTsB8MLK69ejE4O372R699xJaajxg5biLXXjsjc+1KQtZqNwSodn2j+qVOtUudapea2MjG2aQ9jJSXl7Nhw4YO+yorKykvL+/2PV6vF6/X22m/2+1Oyx9Bus7bLaug/bONCJzx2cML7d+9sS084P/oM167IUS16xvVL3WqXepUu+T0tlZJT2Btampi79697N27F7Av3d27dy9VVVWA3auxbNmy+PHf/OY3+fjjj7n33nv54IMP+Pd//3eee+45vvOd7yT70UOHMyFodbHWiO5PIyIiuSTpMLJz507mzp3L3LlzAVixYgVz587l+9//PgDV1dXxYAIwefJkXnrpJSorK5kzZw6PPPIIzzzzTO5e1gvgdIERLX0PS8IrjIiISC5Iepjmyiuv7PEGbl2trnrllVeyZ8+eZD9qaHN6IdTa5VojxfGb5WkFVhERGfp0b5pscXW/CmuxFj0TEZEcojCSLc7uV2GNhZHGQIhwpPteKBERkaFAYSRbXN2vwhoLI5YFjW3qHRERkaFNYSRbYjfL66JnxONykO+xV2HVJFYRERnqFEayJd4z0vPN8uo0b0RERIY4hZFsifeMdB02/Lq8V0REcoTCSLa4up/AClprREREcofCSLbEeka6G6aJrsJapzAiIiJDnMJItsR7Rrpe2CzWM9KgMCIiIkOcwki2OHuewFocn8CqVVhFRGRoUxjJFmf0Tobd9IyU5NvDOJozIiIiQ53CSLac5dJeXU0jIiK5QmEkW3pY9Ay0zoiIiOQOhZFs6WE5eNClvSIikjsURrKlhxvlAUwakQ/AgdpG9n9Wn6lWiYiIZJzCSLa4YuuMdN0zMmlEATdcOBbLggc3vI9l6e69IiIyNCmMZMtZekYA/vvi8/E4HWz76CSbDxzPUMNEREQyS2EkW87SMwIwYXg+t11xDmD3joTCkQw0TEREJLMURrKlFz0jAN/+4hRK8t0cPNbEczv/nIGGiYiIZJbCSLacZZ2RmOI8N3ddNRWAn1R+SFMglO6WiYiIZJTCSLbE1xk5+6W7t1w6iXNG5HOiKcDTWz9Oc8NEREQyS2EkW1y9G6YB8LgcfO+aaQA8vfUjaurb0tkyERGRjFIYyRbn2SewJloys4yLJg2jzYzwk8oDaWyYiIhIZimMZEsSPSMAhmHwj9dNB2Ddrj/zfnVDulomIiKSUQoj2eLs3QTWRPMmDuO62WPiC6GJiIgMBQoj2eK07z1DuHfDNDH/sGQabqfBHw6eYMuHWghNREQGP4WRbOnlpb1nmjgin1vLzwGgYsP7hCNaJl5ERAY3hZFsiV/am1zPCMCdX5pCcZ6bD2oaWft2VT83TEREJLMURrIlxZ4RgJJ8D3//pSkA/PP6/fz75kO6kZ6IiAxaCiPZ4vLZj2YLRMJJv/2vLz+Hr180gYgF//rKAf7uN7tobDv7AmoiIiIDjcJIthSPB1+JHUYO/T7pt7ucDv7lv86m4i8uwON0sPG9Wm54/E0O1jb2f1tFRETSSGEkW1xeuPAW+/nOX6Z8mpsumchz3yxnbLGPj080c8MTb/LiO0f7qZEiIiLpl1IYeeKJJzjnnHPw+Xxceuml/PGPf+z22NWrV2MYRofN5/Ol3OAh5aK/sR8/fBXqUp+IeuGEEn739/+Fyz83gpZgmDvX7OGHL75HKBzpp4aKiIikT9Jh5Nlnn2XFihXcd9997N69mzlz5rBkyRKOHTvW7Xv8fj/V1dXx7fDhw31q9JAxcgpMXgBYsGt1n041otDLr//mEr654HMAPPPGJ9zyzFvUNug+NiIiMrAlHUZ+8pOfcMcdd3DbbbcxY8YMnnrqKfLz8/nlL7sfajAMg7KysvhWWlrap0YPKRd/w37c/ete36emOy6nfUO9p/5qHgUeJ299coov/ngzP/39hzQHQv3QWBERkf7nSubgYDDIrl27WLlyZXyfw+Fg4cKFbN++vdv3NTU1MWnSJCKRCPPmzePBBx9k5syZ3R4fCAQIBNoveW1osO/DYpomptl/V4zEztWf50zauYtwFZZiNNUSenc91oyv9vmUV50/kv/9zcv4h/+znz/9uZ6f/v4g/7HjMHddNYWvzR2Ly9n3qUIDonaDlGrXN6pf6lS71Kl2qeltvQwriQUqjh49yrhx49i2bRvl5eXx/ffeey9btmzhrbfe6vSe7du3c/DgQWbPnk19fT0//vGP2bp1K++++y7jx4/v8nPuv/9+Vq1a1Wn/mjVryM/P721zB43zq/8P02rWc6JwGm9O/cd+O69lwd6TBr+rcnAyYABQlmfxlUkRZpRYGEa/fZSIiEgnLS0t3HzzzdTX1+P3+7s9Lu1h5EymaTJ9+nRuuukmHnjggS6P6apnZMKECZw4caLHXyZZpmlSWVnJokWLcLvd/XbepDUcxfX4XAwrjPm3b8Ko8/v19MFQhDVvH+GJ1z+mrtVOqZdNHsY/LDmfWeNSq+eAqd0gpNr1jeqXOtUudapdahoaGhg5cuRZw0hSwzQjR47E6XRSW1vbYX9tbS1lZWW9Oofb7Wbu3LkcOnSo22O8Xi9er7fL96bjjyBd5+21EZPg/Gvggxdx7/01XPuv/Xp6txvu+MIU/tvFk/j3zYf41ZufsuOT03z1qR3MnVjCwumlLJpRytTRhRhJdpdkvXaDmGrXN6pf6lS71Kl2yeltrZKaPODxeJg/fz6bNm2K74tEImzatKlDT0lPwuEw+/btY8yYMcl89NAXu8z3T/8Jwea0fERxnpuV10znte8u4C/mjsMwYE9VHQ+/eoDFj25lwcObWfW7d9l26ASmLgsWEZEMSapnBGDFihXceuutXHTRRVxyySX89Kc/pbm5mdtuuw2AZcuWMW7cOCoqKgD4wQ9+wGWXXcaUKVOoq6vj4Ycf5vDhw9x+++39+5sMdud+EYZNhtOfwL7nYf6tafuo8cPy+cnXL+Teq6ex6YNafv9eLW9+dJKqUy386s1P+dWbn1Lkc3Hl+aP54vmj+PzUUYwq6txTJSIi0h+SDiNf//rXOX78ON///vepqanhwgsv5JVXXolfrltVVYXD0d7hcvr0ae644w5qamoYNmwY8+fPZ9u2bcyYMaP/fouhwOGwe0cq/xl2/gLmLSPdM0zLin3ccukkbrl0Ei3BEH84eILfv1fLax8c42RzkN/96Si/+5O9muuscX4WnDeKBeeNZt7EkrS2S0REckvSYQTgzjvv5M477+zyZ5s3b+7w+tFHH+XRRx9N5WNyz4W3wGs/hOo/wWe7Yfz8jH10vsfFkpllLJlZRjhisffIaTa9f4ytB4+z/7OG+PbE6x9R5HNx+bnDKW4zKD18mvPGlDC8wJOxtoqIyNCSUhiRNCkYATOXwjvP2r0jGQwjiZwOg/mThjN/0nDuvXoaxxrb+MOHJ9jy4XH+cPA4p1tMXn3vGODkuY/fBqAk3825Iws4d1Qh544q4NyRBXxuVCGTRhTgcekWSCIi0j2FkYHmom/YYWT//4bFP4T84dluEaOLfHxt/ni+Nn884YjFvs/qee39Gl7ZdZBG8qmub6OuxWR3VR27q+o6vNfpMJg0PJ9zRxUyZXQhnxtVYD+OLsTv04x0ERFRGBl4JlwCpbOgdr99ZU358my3qAOnw+DCCSXMLCvgc60HuPbaL2BaBp+eaOHjE018fLyZj4838fGJZj4+3kxTIGQ/P9HM79/veEn4yEIPY4rzGFPsY2xJHmXFPsYU++L7Sv0+9aqIiOQAhZGBxjDsiawvrYCdv4TLvp32iax9le9xMWOsnxljOy5oY1kWtQ0BPjrexKFj9hZ7fqwxwImmICeaguz7rL7bcxd5XQwr8DCswMOIAg/D8j0ML3AzrMBDmd/HOSPtIaGSfM1ZEREZrBRGBqLZ/w0qvw8nD8EnW+DcK7PdopQYhkFZsY+yYh9XTBnZ4WcNbSZVJ1uorm+jur7VfqyLPta3UVPfRjAcoTEQojEQoupUS4+fVZLv5pwRBUweaW/njCxgXIkvGl48+H1uHI6BHepERHKVwshA5C2C2V+3J7FuegCKxvT7EvHZ5ve5mTWumFnjirv8uWVZ1LeanGoOcrolyKlmk1PNAU41m5xuCXKyKcjRulY+Pdkcn7Oyt6WOvUfqujyf02EwLN/NsPyEXpYCDyML7LAyvNBrPy+Mvs739MsNBUVE5OwURgaqS/8O9v4HfLYTnrgULrgRFvwDjJyS7ZZlhGEYlOR7ejX80hoM8+nJZj6Nzk359EQzn55sprYhwOnmII2BEOGIFR8W6i2f24HX5Yw/el0OfO72x+I8N8MThpCGJ2wjCjyMKvImvby+iEguUhgZqEadD7dvgs0V8MGLsO852P88zP5LWHAPDD832y0cMPI8TqaP8TN9TNc3YQqEwtS12L0sidvJ5mC0t8UOKbH9p1uCWBa0mRHazAj1ram1K9/j5NxR9iXOn4tfTVTIOH/Hq4iCoQhNgRCNbSaNbSEa20IEwxFGFnoo9fsYnu/REJOIDGkKIwNZ2Sz4y/+Ao3th80Pw4cvwpzX2pb8X3gRfuAeGnZPtVg54XpeTUr+TUr+vV8eHIxanW4K0BsMEQhHaTPsxEIo+mmFazTD10YBzMhpgTjYFE4aVgrQEw/HF4hI5DPC7nfzgnc00BUIEQj3fB8jlMBhd5GW030ep30up3xffyvw+yortfUW6VFpEBimFkcFg7IVw81r4bBe8XgGHKmHP/4I/rbXv9nv+dXDekgGxJslQ4HQYjCzs2714zHCEqlMtfHSsiY+ON/PR8fYriRrbQtQFDQh2HDLK9zgp9Loo8rlwOx2caApysjlAKGJxtL6No/VtPX5mgcdJabEdUEr9PkYWehhZ6GVEoTf+3H7twa35MCIygCiMDCbj5sNfPQ9H3obND8JHr8H7v7M3wwETy+H8a2HatRrGyTK30xEfnklkWRbVp5tZt2ETX1rweYYV+ijyuinwOrucMGuGI5xoClDbEKC2oY1jDW3x5zUN9lVHNQ1tNLaFaA6Go+u8nP2uz8V5bkYUehhZ4LXnuBTa81xGRMNKgdeF0zBwOgwMg4Tn9qPf52JsSR4+t7PfaiYiuUthZDCacDH8Py9A9Tt2EDmwwV4k7fCb9rbx/4NR0+1QMuu/QqluSjhQGIbBqCIv5xTBtLIi3O6eh1bcTkd0Ebi8Ho9rCYbiwaQ2GlhONsXWcml/PNUcJByxr1SqbzV7FVx6MrLQy7gSH+OG5TG2OM9+LMmjyOfC5XDgdBi4HHaASXyuICMiiRRGBrMxs+3tS/8fnP4UDrwCB16CT9+E4+/b2x8egbHzYO5fwayvQV5J3z6zsRbe/784332Bq6oP4hj1GVz2Tfuuw5I1+R5X9L5AhT0eF4lY1LWanGwKcLLZnudysjnQ8bEpSIsZIhyxj49YFmHLij5vn1PTEgxHg06AP/25+4XruuJ2GswYW8y8iSXMnzSMeROHMbak58AlIkOXwshQMewcOxRc9k1oPQ0HK+G938KHr8DR3fb26j/C9K/YweScz/c+QDTWwHv/1z7f4TcBCwdQCLBxJRx8GW74dyiZkLZfT/qHw2HELz+e2ofzWJZFXYvJZ3Wt9na6laPR50frWmkOhglHrA5bKGIRjkQIhiI0B8P86UgdfzpSx6/e/BSAMcU+5k0cxtyJJZxXWsTE4fmMLcnTLQFEcoDCyFCUN8xexXX2f4Om4/Zlwbt/Y/eU7HvO3komwpybwD8WMOw5J4bR8XnLSXj/RajaDljt5x83n/C063nvgw+ZVfsCxidb4cnL4eqH4MKbB/zy9dJ3hmHEl+nvbuG67liWxZ9Pt7K76jS7Dp9md9Vp3q9upLq+jZf2VfPSvur4sQ4DxhTnMXF4vr2NyGeM38MHpw2Gf3yKgjwPeW4nPre9HozP5STP49QQkMggozAy1BWOsm+2d9m34bPdsOc39h2B66pgy7/0/jzjL4YZS2HGV6BkIhHT5ONTG5j2lbtw/+7v4c9/hN9+214T5frHoHB02n4lGdwMw2DC8HwmDM/nhgvHAfaclz8dqWd31Wn2Hqnj8Mlmqk610GZG4r0v2z8+mXAWJ09/sLPbz8hzOzssQpe4Fee5cUbXbTFoz84GBhjgcToYU2zPgynz+7QSr0gGKIzkCsOA8fPtbcmD9sTXD18Gsw2wwIqAZUWfR187PXDuAntop7shmOGfg795Bd58DF5/0J5Me+Qt+PKjMOOGTP6GMojle1yUf24E5Z8bEd9nWRbHmwIcOdVC1akWqk62UnWqhSOnmqk+fgpvfiGBcITWYPvaL6GI3YPXaobjIaYvnA6DMr8dTMaX2BN07TVdXBR6XRR42x8LvPal2Xlup1beFUmSwkgu8uTDnK/bW39wOOHzK2DqYnjh7+wre55bBlOXQMFICDaD2QLBFjCbo48tdkAqmQTDJ9tzXuLbZHuoSf9Bz2mGYTC6yMfoIh/zJ7WvoWOaJhs2bODaa6/odDVSKByhxQxzutNKu0FOR5/Xt5pYlh1aog9YEN/XaoY5WmffwNEMW/FQ88detxt80dsI+NxO8txOvNFhpDy3fQl37LM6tsF+4nM5O9w/aXj0fkrDC+x7KxX6XPGhKa0XI0OFwoj0n7JZcMfrsOUheONROPjq2d9TVwWf/qHzfm+xPZ/FVxzd/Paj19/+2usHdx6488FTYD+689qfh9qg+QS0nIDm49B80n5sOWHvj4TtIOVwgsMV3RKel0yEstn2VjCicxtlwHE5HfidDvw+N5NGFPTpXOGIxfHGAJ/VtfDn0638+bQdSo41BGgOhGgOhmgKhOzngTDNwZDdqWjZgabVDANm//xi3XA5DDuYeJzRgOLAYRh2O7Cij3bQsttm0dLi5MmPt+FyOXA5HLidBi6HA5fTwO10kOdxUuhxkR/t6SnwuijwOCnwusj3uPC62o91Rx9dDgcel32exP+HMIgOh0X3WRaELXsicyhiEQonTm62r9zyuBx4nI6Ojy4Hbqf92jDAYRg4oo+GgXqihgCFEelfLg9c9X2Yfj18+Ko91BMLB558cBe0P4aDUHfYviz59Kdw6hP7sakGAvVwPLnLRdOqaCyUXWBvY2ZD6SwonmD/vv0lEoH6Kqh9F0593N6TZLbaPUlma3uvksNpL4I38TIYf4kdzqRfOR0GZcU+yop9zJ909uMjEYtW0w4lAdO+jUCrGY7e4yj2PEwobEW/QO33GRgdvsBbg2FOtQQ51RTkVIvdo3OqxYz39sRCD0AoYtEYCNEYCCXxmxnUtjYlcfzAFwsoToeBJyEkuZ2xIGO/dhhGPPTYW8fL1w06hhw79Bg4HAYGFqfrnfz0wzcwIxbBUAQzHMEMR59HIvhcTgp99irKRT43/thzr5sCr4tAKExTIERTm/3PrKnNDrSxUOtyGLijIcztdOB1tf8OHpcjfpPO2GTtWI9brJfMDEcwQxGCYfuqtfhjKEI4OoTZ/rdmJPwN2vvvvXpap4UaM0VhRNJj7Fx7O5tJ5Z33BVvsHpPGagg0QFs9tEUf46/rIdDY/kUdGwoyW+3nWIBhD/cUjISCUZA/IuH5SHC6IRKy58dEQglbxO5VOXkIavbBqY+g8ai9ndnbkzfcnqxbOBoKS+2tYJT92lPY3lsT68Fx54HhxmM2YFRtgxMH4Ni7UPseHHsPgkl8SXz0mv1oOKB0pr0C78TL7Ef/2N6fJ1nhEIQDds9SJGQ/WuGE+oWjc5AiHZ9bsefYvU6Fo9LXxixwOIzo3JH0/mfVsqz4PZPazIjdCxMM0xYK0xYME7HsLxYD7Ivjol+whmEQCoXYtn0H8y++BBwOQmGLUDiCGYk+hiO0BMO0BMMde32iPUEtwTBm9AvODNu9G/aXn0UoYn8RdmhrF+1vX/zOEV8Ez+U04pOKzYQv0NgXfTDc8/2b2ntc7OPTx4CWlm5/GusRO94YSOnsAYBgOLWm9YO/W/C5rH22wogMPJ58GD3N3lJhWXaYcLjB2Q9/4oFGOyzUvBPd9tmvwwFoPWVvxz/o9encwDUA+7v4odNj37F55Hn2cFSHMFPQHmqCTXDkj1C1ze5Nqtlnb3982j5PwSjwFkV7omK9Ufl2QPLkg9ObEA7O3KL1CzRCoAmCsccm+zHUt0mhccUTYdxce1G+cfPtezB5i7o/Phyy19BpOk5e8IQdSJ3D7F6i7kQi0FZnX6befNwengs02vdxKhwNBdEg6erFvYjCpv3eUBsY0eE9w9FxeM+I7otNBO/0SPR/4109t/tMlhUPfwbgMxz4vAb43GB4u55fFUkIgNHAaAYtThe28F8menG7vdHL+BM3o4twfkbQ7PSe6Ptiz61I++975t9WJNR57ljinLKIaf874PJFNy+4fFjOPEyHmzAuLCuCFYlgRSwsK0wkYkX32WvbhEIhwuEwoXCYcDhMOGy/NsNhsNp7UYxor4eR0BMSK7UV7TmxojsilkU4HOajQx8x7fypeNxuXLHhLYcDl9OBwwGmGaKtrY22QIBgsI1AIEgwGMAMBjBDJk6nG7fHa29eHx6PD68vD5/Ph8ftsX+HUJBQKEg4FCIcChIJmURCJuFQyA58YYtgBMywhZnwGIpYuBwGLoeFywC3A1wOcDrAZYDTgIjDhWU4iBB9NJxEDBcRDCyHi4m+1EJUf1AYkaHHMOwv7f7iLYKJl9pbTCQS/WKstbfm4+3Pm47Zr2P/sTVbOw61RL/MreIJGKWz7OX6R8+wh35GfM7usemNi26zHxuq4cgOqNphrwlTsy/6xXu8/2pwNoaz/Qs28Ys6tiW+tiLQcNQekqqvshfTs09iB7ExF7avc9Nyyn5sPWWHD+wwtxjg3RX229wF9j+j2ObOt//ZtJyw3xvpxRCGrzgaTErtIa9gkx082hqioayx/0JYe9ES5iolBBo4IwyEevc7JNbX6rp3wA1cB/BOf/0OmWEA/TggmrILAI5luxVpNHcmUJaVj1YYEUmFw2FPai0YkfS9f8xggJdfepFrvvyVs96bplf8Y2DmV+0N7C/OxDknwebOz8PBzoEhcXN57V4UbyF4iqKPhe1f+C5v+5dofMG8JLQ1QPVee+2bo7vtx/ojdg/TWXqZLG8RkWAbTis6OdRstremmu7f5C2O/vMaZf8erafs0Nh0zP6/8djQ38mDZ2+74Wz/v/8+sezPjvTTJNceQki/iIXMxB6PXtXA6BhKY7188d66hDllDrf9txkK2D1QiY/hgN07FptwYzg6nju2aGNPf9eJf6fWmW2PDu3Gm534N20QsSKcOnWa4cOH4TCMhPfHLoey7H8fnK5or6w7+trd/u9KJGz/fmEz+vvEngej/07Ggmm0V9eRcC7D0f55Vjc9bx16qs5YxBKivVyJw6pnDLV6szf3TGFEJNMMB5Yjjf/qeYtgzJz0nb8/+Pww+Qv2FtN0DI7usYfCnB57Pk7+cHuuT17ssYRQOGJf2rv4KtyRNnseUaz3ItBoB64z5wp1NwxjWXYvSvPxaDiptc+R2NPi9Xd8Heu5ig2DxP6DHntuRbC/BGKzA42E17FhkHAXvR/haDBJGMZJ/CKLD+0YdFgbKHF4zYoNo8SGkBKfOzFDYV555RWuXrLYviz4zOE5K9LFFWY9BM7Yl2LinCDD2cWX4uAXNk3e3LCBa6+9Fkd//E+EdKAwIiIDQ+FoOG+JvfUkNpnR5QV3oR06UmUY0cAz3B4iSobDATh6P6w2IJhEHO5o70Q/tDseNhzo60T6QivmiIiISFYpjIiIiEhWKYyIiIhIVimMiIiISFYpjIiIiEhWpRRGnnjiCc455xx8Ph+XXnopf/xjz/ezXLduHdOmTcPn83HBBRewYcOGlBorIiIiQ0/SYeTZZ59lxYoV3HfffezevZs5c+awZMkSjh3relm6bdu2cdNNN/GNb3yDPXv2sHTpUpYuXcr+/V2thS0iIiK5Jukw8pOf/IQ77riD2267jRkzZvDUU0+Rn5/PL3/5yy6Pf+yxx7j66qu55557mD59Og888ADz5s3j8ccf73PjRUREZPBLapWaYDDIrl27WLlyZXyfw+Fg4cKFbN++vcv3bN++nRUrVnTYt2TJEtavX9/t5wQCAQKB9hv2NDQ0AGCaJqbZT0snR8+X+Ci9p9qlTrXrG9Uvdapd6lS71PS2XkmFkRMnThAOhyktLe2wv7S0lA8+6Pp+EjU1NV0eX1PT/X0kKioqWLVqVaf9GzduJD8/P5km90plZWW/nzNXqHapU+36RvVLnWqXOtUuOS0tLb06bkCu37ty5coOvSkNDQ1MmDCBxYsX4/f33418TNOksrKSRYsW9c8Ny3KIapc61a5vVL/UqXapU+1SExvZOJukwsjIkSNxOp3U1tZ22F9bW0tZWde3HS4rK0vqeACv14vX2/nGVm63Oy1/BOk6by5Q7VKn2vWN6pc61S51ql1yelurpCawejwe5s+fz6ZNm+L7IpEImzZtory8vMv3lJeXdzge7G6u7o4XERGR3JL0MM2KFSu49dZbueiii7jkkkv46U9/SnNzM7fddhsAy5YtY9y4cVRUVABw1113sWDBAh555BGuu+461q5dy86dO3n66ad7/ZmWZQG97+7pLdM0aWlpoaGhQUk3Sapd6lS7vlH9UqfapU61S03sezv2Pd4tKwU/+9nPrIkTJ1oej8e65JJLrB07dsR/tmDBAuvWW2/tcPxzzz1nnXfeeZbH47FmzpxpvfTSS0l93pEjRyxAmzZt2rRp0zYItyNHjvT4PW9Y1tniSvZFIhGOHj1KUVERhmH023ljE2OPHDnSrxNjc4FqlzrVrm9Uv9SpdqlT7VJjWRaNjY2MHTsWh6P7mSED8mqaMzkcDsaPH5+28/v9fv1xpUi1S51q1zeqX+pUu9SpdskrLi4+6zG6UZ6IiIhklcKIiIiIZFVOhxGv18t9993X5Zom0jPVLnWqXd+ofqlT7VKn2qXXoJjAKiIiIkNXTveMiIiISPYpjIiIiEhWKYyIiIhIVimMiIiISFbldBh54oknOOecc/D5fFx66aX88Y9/zHaTBpytW7dy/fXXM3bsWAzDYP369R1+blkW3//+9xkzZgx5eXksXLiQgwcPZqexA0xFRQUXX3wxRUVFjB49mqVLl3LgwIEOx7S1tbF8+XJGjBhBYWEhX/va1zrd5ToXPfnkk8yePTu+wFR5eTkvv/xy/OeqW+899NBDGIbB3XffHd+n+nXt/vvvxzCMDtu0adPiP1fd0idnw8izzz7LihUruO+++9i9ezdz5sxhyZIlHDt2LNtNG1Cam5uZM2cOTzzxRJc//9d//Vf+7d/+jaeeeoq33nqLgoIClixZQltbW4ZbOvBs2bKF5cuXs2PHDiorKzFNk8WLF9Pc3Bw/5jvf+Q6/+93vWLduHVu2bOHo0aP8xV/8RRZbPTCMHz+ehx56iF27drFz506+9KUvccMNN/Duu+8Cqltvvf322/z85z9n9uzZHfarft2bOXMm1dXV8e2NN96I/0x1S6Ok7lg3hFxyySXW8uXL46/D4bA1duxYq6KiIoutGtgA64UXXoi/jkQiVllZmfXwww/H99XV1Vler9f6z//8zyy0cGA7duyYBVhbtmyxLMuuldvtttatWxc/5v3337cAa/v27dlq5oA1bNgw65lnnlHdeqmxsdGaOnWqVVlZaS1YsMC66667LMvS311P7rvvPmvOnDld/kx1S6+c7BkJBoPs2rWLhQsXxvc5HA4WLlzI9u3bs9iyweWTTz6hpqamQx2Li4u59NJLVccu1NfXAzB8+HAAdu3ahWmaHeo3bdo0Jk6cqPolCIfDrF27lubmZsrLy1W3Xlq+fDnXXXddhzqB/u7O5uDBg4wdO5Zzzz2XW265haqqKkB1S7dBcaO8/nbixAnC4TClpaUd9peWlvLBBx9kqVWDT01NDUCXdYz9TGyRSIS7776bK664glmzZgF2/TweDyUlJR2OVf1s+/bto7y8nLa2NgoLC3nhhReYMWMGe/fuVd3OYu3atezevZu3336708/0d9e9Sy+9lNWrV3P++edTXV3NqlWr+PznP8/+/ftVtzTLyTAikmnLly9n//79HcafpWfnn38+e/fupb6+nueff55bb72VLVu2ZLtZA96RI0e46667qKysxOfzZbs5g8o111wTfz579mwuvfRSJk2axHPPPUdeXl4WWzb05eQwzciRI3E6nZ1mQdfW1lJWVpalVg0+sVqpjj278847efHFF3n99dcZP358fH9ZWRnBYJC6uroOx6t+No/Hw5QpU5g/fz4VFRXMmTOHxx57THU7i127dnHs2DHmzZuHy+XC5XKxZcsW/u3f/g2Xy0Vpaanq10slJSWcd955HDp0SH93aZaTYcTj8TB//nw2bdoU3xeJRNi0aRPl5eVZbNngMnnyZMrKyjrUsaGhgbfeekt1xL7s+c477+SFF17gtddeY/LkyR1+Pn/+fNxud4f6HThwgKqqKtWvC5FIhEAgoLqdxVVXXcW+ffvYu3dvfLvooou45ZZb4s9Vv95pamrio48+YsyYMfq7S7dsz6DNlrVr11per9davXq19d5771l/+7d/a5WUlFg1NTXZbtqA0tjYaO3Zs8fas2ePBVg/+clPrD179liHDx+2LMuyHnroIaukpMT67W9/a73zzjvWDTfcYE2ePNlqbW3Ncsuz71vf+pZVXFxsbd682aquro5vLS0t8WO++c1vWhMnTrRee+01a+fOnVZ5eblVXl6exVYPDN/73vesLVu2WJ988on1zjvvWN/73vcswzCsjRs3WpaluiUr8Woay1L9uvPd737X2rx5s/XJJ59Yb775prVw4UJr5MiR1rFjxyzLUt3SKWfDiGVZ1s9+9jNr4sSJlsfjsS655BJrx44d2W7SgPP6669bQKft1ltvtSzLvrz3n//5n63S0lLL6/VaV111lXXgwIHsNnqA6KpugPWrX/0qfkxra6v17W9/2xo2bJiVn59vffWrX7Wqq6uz1+gB4m/+5m+sSZMmWR6Pxxo1apR11VVXxYOIZaluyTozjKh+Xfv6179ujRkzxvJ4PNa4ceOsr3/969ahQ4fiP1fd0sewLMvKTp+MiIiISI7OGREREZGBQ2FEREREskphRERERLJKYURERESySmFEREREskphRERERLJKYURERESySmFEREREskphRERERLJKYURERESySmFEREREskphRERERLLq/wfGhYhHtD7KhgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lst_loss_train)\n",
    "plt.plot(lst_loss_val)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f65295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN размерность выхода: torch.Size([1, 32, 16, 89])\n",
      "CNN число фичей: 512\n",
      "Проекция из 512 в 512\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m     train_mess.extend(mess)\n\u001b[32m     40\u001b[39m     logits = model_load(seq)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     predicted_values = \u001b[43mctc_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_to_char\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBLANK_IDX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m     train_predicts.extend(predicted_values)\n\u001b[32m     44\u001b[39m val_mess = []\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mctc_decoder\u001b[39m\u001b[34m(logits, int_char_map, blank_label_idx)\u001b[39m\n\u001b[32m     22\u001b[39m preds = []\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m log_probs:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     decoded_seq = \u001b[43mbeam_search_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_width\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblank_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBLANK_IDX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     preds.append(decoded_seq[\u001b[32m0\u001b[39m])\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m preds\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mbeam_search_decoder\u001b[39m\u001b[34m(log_probs, beam_width, blank_idx)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m beam \u001b[38;5;129;01min\u001b[39;00m beams:\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(C):\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m         prob = \u001b[43mlog_probs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m         new_text = beam.text.copy()\n\u001b[32m     19\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m c == blank_idx:\n\u001b[32m     20\u001b[39m             \u001b[38;5;66;03m# stay at same text (repeat blank)\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def ctc_decoder(logits, int_char_map, blank_label_idx):\n",
    "    preds = []\n",
    "    logits_cpu = logits.cpu()\n",
    "    max_inds = torch.argmax(logits_cpu.detach(), dim=2).t().numpy()\n",
    "\n",
    "    for ind in max_inds:\n",
    "        merged_inds = []\n",
    "        prev_idx = None\n",
    "        for idx in ind:\n",
    "            if idx != blank_label_idx and idx != prev_idx:\n",
    "                merged_inds.append(idx)\n",
    "            prev_idx = idx\n",
    "        text = \"\".join([int_char_map.get(i, '') for i in merged_inds])\n",
    "        preds.append(text)\n",
    "\n",
    "    return preds\n",
    "\n",
    "model_load = MorseNet(num_classes=num_classes)\n",
    "model_load.load_state_dict(torch.load('MorseNet_№1.pth'))\n",
    "model_load.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_mess = []\n",
    "    train_predicts = []\n",
    "    for loader in train_dl:\n",
    "        seq, test_target, _, mess = loader\n",
    "        train_mess.extend(mess)\n",
    "\n",
    "        logits = model_load(seq)\n",
    "        predicted_values = ctc_decoder(logits, int_to_char, BLANK_IDX)\n",
    "        train_predicts.extend(predicted_values)\n",
    "\n",
    "    val_mess = []\n",
    "    val_predicts = []\n",
    "    for loader in val_dl:\n",
    "        seq, test_target, _, mess = loader\n",
    "        val_mess.extend(mess)\n",
    "\n",
    "        logits= model_load(seq)\n",
    "        predicted_values = ctc_decoder(logits, int_to_char, BLANK_IDX)\n",
    "        val_predicts.extend(predicted_values)\n",
    "\n",
    "import Levenshtein\n",
    "\n",
    "mean_acc_test = np.mean([Levenshtein.ratio(test_pred, train_mess[ind]) for ind, test_pred in enumerate(train_predicts)])\n",
    "mean_acc_val = np.mean([Levenshtein.ratio(val_pred, val_mess[ind]) for ind, val_pred in enumerate(val_predicts)])\n",
    "\n",
    "\n",
    "print(f\"Mean accurasu by The Levenshtein in train is : {mean_acc_test}\")\n",
    "print(f\"Mean accurasu by The Levenshtein in validate is : {mean_acc_val}\")\n",
    "\n",
    "test_ds = MosreDataset(df=sample_data,\n",
    "                        data_patch=DATASET_PATCH,\n",
    "                        char_to_int=char_to_int,\n",
    "                        train=False,\n",
    "                        transforms=valid_audio_transforms)\n",
    "\n",
    "\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=20, shuffle=False, collate_fn=my_collate)\n",
    "model_load.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_predicts = []\n",
    "    for loader in test_dl:\n",
    "        seq = loader\n",
    "        logits = model_load(seq)\n",
    "        predicted_values = ctc_decoder(logits, int_to_char, BLANK_IDX)\n",
    "        test_predicts.extend(predicted_values)\n",
    "\n",
    "sample_data.message = test_predicts\n",
    "sample_data.to_csv('sample_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
