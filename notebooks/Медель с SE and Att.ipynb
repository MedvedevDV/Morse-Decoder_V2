{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9b8de53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path as pt\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchaudio import transforms\n",
    "from torchvision.transforms import v2\n",
    "# from Moduls.MosreDataset import MosreDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.nn.parallel import DataParallel\n",
    "from collections import Counter\n",
    "\n",
    "DIVICE = torch.device(\"cuda\")\n",
    "\n",
    "MAIN = pt(os.getcwd())\n",
    "DATASET_PATH = MAIN / 'morse_dataset'\n",
    "AUDIO_FILES = DATASET_PATH / 'morse_dataset'\n",
    "SAVE_DIR = DATASET_PATH / 'spectrogram'\n",
    "\n",
    "# Поятоянные значения выявленные в процессе анализа\n",
    "MORSEALP = \"АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ 1234567890#\"\n",
    "MAX_TIME = 48\n",
    "SAMPLE_RATE = 8000\n",
    "N_MELS = 128\n",
    "N_FFT = 400\n",
    "HOP_LENGTH = 180\n",
    "TOP_DB = 80\n",
    "FREQ_MASK = 30\n",
    "TIME_MASK = 40\n",
    "\n",
    "# Гиперпараметы обучения\n",
    "SEED = 42\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.0002 #2e-4\n",
    "WEIGHT_DECAY = 0.001\n",
    "# int_to_alph = dict(enumerate(MORSEALP))\n",
    "# alph_to_int = {char:enum for enum, char in int_to_alph.items()}\n",
    "def preload(df, fodl_name):\n",
    "    transform1 = transforms.MelSpectrogram(sample_rate=SAMPLE_RATE,\n",
    "                                      n_fft=N_FFT,\n",
    "                                      hop_length=HOP_LENGTH,\n",
    "                                      n_mels=N_MELS)\n",
    "    \n",
    "    transform2 = transforms.AmplitudeToDB(top_db=TOP_DB)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        file_id = row[\"id\"]\n",
    "        audio_path = AUDIO_FILES / file_id\n",
    "\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(audio_path)\n",
    "            mel_spec = transform1(waveform)\n",
    "            mel_spec = transform2(mel_spec)\n",
    "            torch.save(mel_spec, SAVE_DIR/fodl_name/f\"{file_id}.pt\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка в {file_id}: {e}\")\n",
    "\n",
    "#===== Import data =====\n",
    "train_data = pd.read_csv(pt.joinpath(DATASET_PATH,'train.csv'))\n",
    "test_data = pd.read_csv(pt.joinpath(DATASET_PATH,'test.csv'))\n",
    "sample_data = pd.read_csv(pt.joinpath(DATASET_PATH,'sample_submission.csv'))\n",
    "\n",
    "# audio_transforms = nn.Sequential(\n",
    "#     transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS),\n",
    "#     transforms.AmplitudeToDB(top_db=TOP_DB)\n",
    "#     # transforms.FrequencyMasking(freq_mask_param=FREQ_MASK), \n",
    "#     # transforms.TimeMasking(time_mask_param=TIME_MASK), \n",
    "#     )\n",
    "preload(test_data, 'test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d761d8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN размерность выхода: torch.Size([1, 64, 16, 89])\n",
      "CNN число фичей: 1024\n",
      "Проекция из 1024 в 1024\n",
      "\n",
      "MorseNet - инициалицация модели. Число обучаемых параметров: 20,358,117\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path as pt\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchaudio import transforms\n",
    "from torchvision.transforms import v2\n",
    "# from Moduls.MosreDataset import MosreDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.nn.parallel import DataParallel\n",
    "from collections import Counter\n",
    "\n",
    "DIVICE = torch.device(\"cuda\")\n",
    "\n",
    "MAIN = pt(os.getcwd())\n",
    "DATASET_PATH = MAIN / 'morse_dataset'\n",
    "AUDIO_FILES = DATASET_PATH / 'morse_dataset'\n",
    "SAVE_DIR = DATASET_PATH / 'spectrogram'\n",
    "\n",
    "# Поятоянные значения выявленные в процессе анализа\n",
    "MORSEALP = \"АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ 1234567890#\"\n",
    "MAX_TIME = 48\n",
    "SAMPLE_RATE = 8000\n",
    "N_MELS = 128\n",
    "N_FFT = 400\n",
    "HOP_LENGTH = 180\n",
    "TOP_DB = 80\n",
    "FREQ_MASK = 30\n",
    "TIME_MASK = 40\n",
    "\n",
    "# Гиперпараметы обучения\n",
    "SEED = 42\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.0002 #2e-4\n",
    "WEIGHT_DECAY = 0.001\n",
    "# int_to_alph = dict(enumerate(MORSEALP))\n",
    "# alph_to_int = {char:enum for enum, char in int_to_alph.items()}\n",
    "\n",
    "#===== Import data =====\n",
    "train_data = pd.read_csv(pt.joinpath(DATASET_PATH,'train.csv'))\n",
    "test_data = pd.read_csv(pt.joinpath(DATASET_PATH,'test.csv'))\n",
    "sample_data = pd.read_csv(pt.joinpath(DATASET_PATH,'sample_submission.csv'))\n",
    "\n",
    "all_chars = Counter(\"\".join(train_data['message']))\n",
    "BLANK_CHAR = \"_\"\n",
    "vocab_list = sorted(all_chars.keys()) + [BLANK_CHAR]\n",
    "num_classes = len(vocab_list)\n",
    "char_to_int = {char: i for i, char in enumerate(vocab_list)}\n",
    "int_to_char = {i: char for i, char in enumerate(vocab_list)}\n",
    "BLANK_IDX = char_to_int[BLANK_CHAR]\n",
    "\n",
    "class MosreDatasetToTest(Dataset):\n",
    "    \"\"\"\n",
    "    Класс для обработки \n",
    "    \"\"\"\n",
    "    def __init__(self, df, data_patch,char_to_int, train=True, transforms=None, prev_chars = 1):\n",
    "        self.df = df\n",
    "        self.is_train = train\n",
    "\n",
    "        self.data_path = data_patch\n",
    "        self.audio_paths = self.data_path / 'morse_dataset'\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.char_to_int = char_to_int\n",
    "        self.prev_chars = prev_chars\n",
    "\n",
    "        if self.is_train:\n",
    "            self.messeges = self.df.message.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #Получение аугментрованых спектрограмм\n",
    "        try:\n",
    "            file_id = self.df.id.values[index]\n",
    "            spec_path = self.data_path/ f\"{file_id}.pt\"\n",
    "\n",
    "            spectrogram = torch.load(spec_path)\n",
    "            spec_lens = spectrogram.shape[-1]\n",
    "\n",
    "            if self.transforms:\n",
    "                spectrogram = self.transforms(spectrogram)\n",
    "\n",
    "            if self.is_train:\n",
    "                message = self.df.message.values[index]\n",
    "                target = torch.tensor([self.char_to_int[char] for char in message], dtype=torch.long)\n",
    "                target_len = torch.tensor(len(target), dtype=torch.long)\n",
    "                return spectrogram, spec_lens, target, target_len, message\n",
    "            else:\n",
    "                return spectrogram, spec_lens, None, None, None\n",
    "        except Exception as ex:\n",
    "            print(str(ex))\n",
    "            return ex\n",
    "    \n",
    "FIRST_FE_COUNT = 32\n",
    "SECOND_FE_COUNT = 64\n",
    "THIRD_FE_COUNT = 64\n",
    "QAD_FE_COUNT = 64\n",
    "PADDING = 'same'\n",
    "MAXPOOL_KERNEL = 2\n",
    "KERTNEL_SIZE = 3\n",
    "NERON_COUNT = 128\n",
    "GRU_HIDEN = 512\n",
    "# Start with 4 transforms\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)  # [B, C, 1, 1]\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction),  # [B, C/reduction]\n",
    "            nn.GELU(),\n",
    "            nn.Linear(channels // reduction, channels),  # [B, C]\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, _, _ = x.shape\n",
    "        squeezed = self.squeeze(x).view(B, C)  # [B, C]\n",
    "        weights = self.excitation(squeezed).view(B, C, 1, 1)  # [B, C, 1, 1]\n",
    "        return x * weights # масштабирование\n",
    "    \n",
    "\n",
    "class MorseNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.net_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, \n",
    "                      out_channels=FIRST_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(FIRST_FE_COUNT),\n",
    "            nn.Dropout2d(0.2),  # добавлено\n",
    "            SEBlock(FIRST_FE_COUNT, 4),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((1, 2), (1, 2)), # [batch, FIRST_FE_COUNT = 16, 64, 960]\n",
    "\n",
    "            nn.Conv2d(in_channels=FIRST_FE_COUNT, \n",
    "                      out_channels=SECOND_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(SECOND_FE_COUNT),\n",
    "            nn.Dropout2d(0.2),  # добавлено\n",
    "            nn.GELU(),\n",
    "            SEBlock(SECOND_FE_COUNT, 8),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)), # [batch, SECOND_FE_COUNT = 32, 32, 480]\n",
    "\n",
    "            nn.Conv2d(in_channels=SECOND_FE_COUNT, \n",
    "                      out_channels=THIRD_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(THIRD_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout2d(0.2),  # добавлено\n",
    "            SEBlock(THIRD_FE_COUNT, 16),\n",
    "            nn.MaxPool2d((2, 2), (2, 2)), # [batch, THIRD_FE_COUNT = 32, 16, 240]\n",
    "\n",
    "            nn.Conv2d(in_channels=THIRD_FE_COUNT, \n",
    "                      out_channels=QAD_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(QAD_FE_COUNT, 16),\n",
    "            nn.Dropout2d(0.2),  # добавлено\n",
    "            nn.GELU(),\n",
    "            SEBlock(QAD_FE_COUNT),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)) # [batch=32, QAD_FE_COUNT = 32, 16, 89](что юы сохраниить большще признаков по горизонтали)\n",
    "        )\n",
    "        with torch.no_grad(): \n",
    "            dummy_input = torch.randn(1, 1, N_MELS, 356); \n",
    "            cnn_out = self.net_conv(dummy_input); \n",
    "            self.cnn_output_features = cnn_out.shape[1] * cnn_out.shape[2]\n",
    "\n",
    "        print(f\"CNN размерность выхода: {cnn_out.shape}\"); \n",
    "        print(f\"CNN число фичей: {self.cnn_output_features}\")\n",
    "\n",
    "        self.layer1 = nn.Linear(self.cnn_output_features, N_MELS*2); \n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "        print(f\"Проекция из {self.cnn_output_features} в {GRU_HIDEN*2}\")\n",
    "        self.rnn = nn.LSTM(\n",
    "                input_size=N_MELS*2,\n",
    "                hidden_size=GRU_HIDEN,\n",
    "                num_layers=3,\n",
    "                bidirectional=True,\n",
    "                dropout=0.5,\n",
    "                batch_first=True \n",
    "            )\n",
    "\n",
    "        self.embed_dim = GRU_HIDEN * 2\n",
    "\n",
    "        self.mha = nn.MultiheadAttention(self.embed_dim, 8, dropout=0.3, batch_first=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5) \n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(self.embed_dim)  \n",
    "\n",
    "        self.layer2 = nn.Linear(self.embed_dim, num_classes)     \n",
    "\n",
    "    def _get_output_lengths(self, input_lengths):\n",
    "        output_lengths = torch.floor(input_lengths.float() / 4); \n",
    "        return torch.clamp(output_lengths.long(), min=1).to(DIVICE)\n",
    "    \n",
    "    def forward(self, x, input_lengths):\n",
    "        x = self.net_conv(x)\n",
    "\n",
    "        batch, channels, reduced_mels, reduced_time = x.shape\n",
    "        x = x.permute(0, 3, 1, 2)  # [batch, time, channels, mels]\n",
    "\n",
    "        # В частности, каждый вектор признаков в последовательности признаков генерируется \n",
    "        # слева направо на картах признаков. Это означает, что i-й вектор признаков представляет \n",
    "        # собой объединение столбцов всех карт. \n",
    "        # Таким образом, форма тензора может быть изменена, например, на (размер_пакета, 80, 256)\n",
    "        \n",
    "        x = x.reshape(batch, reduced_time, -1)  # to GRU [batch=32, seq_len=89, features/hiden_dim=512]\n",
    "        x = self.layer1(x)\n",
    "        x = self.gelu(x)\n",
    "        \n",
    "        output_lengths = self._get_output_lengths(input_lengths)\n",
    "        output_lengths.to(DIVICE)\n",
    "        self.rnn.flatten_parameters()\n",
    "        # x = self.layer_norm1(x)\n",
    "        x = self.rnn(x) # [batch=32, seq_len=89, features/hiden_dim=256 * 2]\n",
    "        x, _ = x # берем информацию со всез состояний\n",
    "\n",
    "        #att\n",
    "        max_len = reduced_time; \n",
    "        idx = torch.arange(max_len, device=DIVICE).unsqueeze(0); \n",
    "        key_padding_mask = (idx >= output_lengths.unsqueeze(1))\n",
    "        attenc, _ = self.mha(x, x, x, key_padding_mask=key_padding_mask)\n",
    "        x = x + self.dropout(attenc)\n",
    "        x = self.layer_norm(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.layer2(x) # logits - [batch, sequence, num_classes] \n",
    "        x = nn.functional.log_softmax(x.permute(1,0,2), dim=2) # pertime так как CTC loss требует на взод (sequence/T,batch/N,num_classes/C)\n",
    "        '''\n",
    "        по одному прогнозу для каждого из признаков в последовательности, \n",
    "        в итоге получается 89 прогнозов символов для каждой секунды звука.\n",
    "        '''\n",
    "        return x\n",
    "    \n",
    "\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    transforms.FrequencyMasking(freq_mask_param=FREQ_MASK), \n",
    "    transforms.TimeMasking(time_mask_param=TIME_MASK), \n",
    "    )\n",
    "\n",
    "\n",
    "train_dataframe, val_dataframe = train_test_split(train_data, test_size=0.25, random_state=SEED)\n",
    "\n",
    "train_ds = MosreDatasetToTest(df=train_dataframe,\n",
    "                        data_patch=SAVE_DIR/'train',\n",
    "                        char_to_int=char_to_int,\n",
    "                        train=True,\n",
    "                        transforms=train_audio_transforms)\n",
    "\n",
    "val_ds = MosreDatasetToTest(df=val_dataframe,\n",
    "                        data_patch=SAVE_DIR/'train',\n",
    "                        char_to_int=char_to_int,\n",
    "                        train=True)\n",
    "\n",
    "\n",
    "def my_collate(batch):\n",
    "    spectrograms = [item[0].squeeze(0) for item in batch]\n",
    "    # Падинг спектрограмм по максимальной длине\n",
    "    spectrograms_permuted = [s.permute(1, 0) for s in spectrograms]\n",
    "    spectrograms_padded = nn.utils.rnn.pad_sequence(spectrograms_permuted, batch_first=True, padding_value=0.0)\n",
    "    spectrograms_padded = spectrograms_padded.permute(0, 2, 1).unsqueeze(1)\n",
    "    # spec_lens = torch.stack([item[0] for item in batch])\n",
    "    spec_lens = torch.tensor([item[1] for item in batch]).reshape(BATCH_SIZE)\n",
    "    if batch[0][3] is not None:\n",
    "        target = torch.nn.utils.rnn.pad_sequence(\n",
    "                                                [item[2] for item in batch], \n",
    "                                                batch_first=True, \n",
    "                                                padding_value=BLANK_IDX)# выравнивает последовательность до макс \n",
    "                                                                        # длины в батче заполняя пропуски нулем\n",
    "        label_len = torch.stack([item[3] for item in batch])\n",
    "        msg = [item[4] for item in batch]\n",
    "        \n",
    "        return [spectrograms_padded, spec_lens, target, label_len, msg]\n",
    "    else: \n",
    "        return [spectrograms_padded, spec_lens]\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, \n",
    "                                       batch_size=BATCH_SIZE, \n",
    "                                       shuffle=True, \n",
    "                                       collate_fn=my_collate, \n",
    "                                       drop_last=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, \n",
    "                                     batch_size=BATCH_SIZE, \n",
    "                                     shuffle=True, \n",
    "                                     collate_fn=my_collate, \n",
    "                                     drop_last=True)\n",
    "\n",
    "test, test_lens, test_target, _, mess = next(iter(train_dl))\n",
    "test, test_target= test.to(DIVICE), test_target.to(DIVICE)\n",
    "\n",
    "test_val, val_lens, val_target, __, val_mess = next(iter(val_dl))\n",
    "test_val, val_target = test_val.to(DIVICE), val_target.to(DIVICE)\n",
    "# test.shape \n",
    "\n",
    "#===== начало обучения =====\n",
    "model = MorseNet(num_classes=num_classes).to(DIVICE)\n",
    "# model = DataParallel(model)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=LEARNING_RATE)  # Было 0.002\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "loss_func = nn.CTCLoss(blank=BLANK_IDX, reduction='mean', zero_infinity=True).to(DIVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nMorseNet - инициалицация модели. Число обучаемых параметров: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bcbbd50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356,\n",
       "        356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356,\n",
       "        356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356,\n",
       "        356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356,\n",
       "        356, 356, 356, 356, 356, 356, 356, 356])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d2fbb29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([89, 64, 45])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model(test, test_lens)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa7f726",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c23dee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "def ctc_decoder(logits, int_char_map, blank_label_idx):\n",
    "    preds = []\n",
    "    logits_cpu = logits.cpu() \n",
    "    max_inds = torch.argmax(logits_cpu.detach(), dim=2).t().numpy() # арзмакс по лагитам и преобразование к словарю\n",
    "    \n",
    "    for ind in max_inds:\n",
    "        merged_inds = []\n",
    "        for idx in ind:\n",
    "            if idx != blank_label_idx: \n",
    "                merged_inds.append(idx)\n",
    "        text = \"\".join([int_char_map.get(i, '?') for i in merged_inds])\n",
    "        preds.append(text)\n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "422ea6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 1/100 =====\n",
      "Mean grad norm: 0.023107\n",
      "Max grad norm: 0.978718\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 4.2696\n",
      "---- Val Loss: 5.1980\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 2/100 =====\n",
      "Mean grad norm: 0.058765\n",
      "Max grad norm: 0.738597\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 3.6466\n",
      "---- Val Loss: 1.3094\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 3/100 =====\n",
      "Mean grad norm: 0.064775\n",
      "Max grad norm: 0.601997\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 1.3208\n",
      "---- Val Loss: 0.6457\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 4/100 =====\n",
      "Mean grad norm: 0.058831\n",
      "Max grad norm: 0.635569\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 1.0189\n",
      "---- Val Loss: 0.5271\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 5/100 =====\n",
      "Mean grad norm: 0.064345\n",
      "Max grad norm: 0.542298\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.8951\n",
      "---- Val Loss: 0.4474\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 6/100 =====\n",
      "Mean grad norm: 0.065288\n",
      "Max grad norm: 0.540656\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.8172\n",
      "---- Val Loss: 0.3734\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 7/100 =====\n",
      "Mean grad norm: 0.064262\n",
      "Max grad norm: 0.542098\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.7494\n",
      "---- Val Loss: 0.3234\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 8/100 =====\n",
      "Mean grad norm: 0.063917\n",
      "Max grad norm: 0.554167\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.7096\n",
      "---- Val Loss: 0.2891\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 9/100 =====\n",
      "Mean grad norm: 0.061819\n",
      "Max grad norm: 0.534072\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.6741\n",
      "---- Val Loss: 0.2679\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 10/100 =====\n",
      "Mean grad norm: 0.062437\n",
      "Max grad norm: 0.565519\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.6481\n",
      "---- Val Loss: 0.2487\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 11/100 =====\n",
      "Mean grad norm: 0.048289\n",
      "Max grad norm: 0.615746\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.6304\n",
      "---- Val Loss: 0.2404\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 12/100 =====\n",
      "Mean grad norm: 0.066120\n",
      "Max grad norm: 0.524480\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.6109\n",
      "---- Val Loss: 0.2296\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 13/100 =====\n",
      "Mean grad norm: 0.059534\n",
      "Max grad norm: 0.556876\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.5969\n",
      "---- Val Loss: 0.2123\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 14/100 =====\n",
      "Mean grad norm: 0.057487\n",
      "Max grad norm: 0.564379\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.5850\n",
      "---- Val Loss: 0.2128\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 15/100 =====\n",
      "Mean grad norm: 0.058582\n",
      "Max grad norm: 0.599087\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.5759\n",
      "---- Val Loss: 0.2062\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 16/100 =====\n",
      "Mean grad norm: 0.060401\n",
      "Max grad norm: 0.581464\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.5591\n",
      "---- Val Loss: 0.2046\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 17/100 =====\n",
      "Mean grad norm: 0.065576\n",
      "Max grad norm: 0.531594\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.5543\n",
      "---- Val Loss: 0.1975\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 18/100 =====\n",
      "Mean grad norm: 0.060818\n",
      "Max grad norm: 0.527926\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.5412\n",
      "---- Val Loss: 0.1967\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 19/100 =====\n",
      "Mean grad norm: 0.054511\n",
      "Max grad norm: 0.614717\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.5337\n",
      "---- Val Loss: 0.1882\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 20/100 =====\n",
      "Mean grad norm: 0.064488\n",
      "Max grad norm: 0.481001\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.5286\n",
      "---- Val Loss: 0.1808\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 21/100 =====\n",
      "Mean grad norm: 0.064836\n",
      "Max grad norm: 0.536567\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.5152\n",
      "---- Val Loss: 0.1776\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 22/100 =====\n",
      "Mean grad norm: 0.058542\n",
      "Max grad norm: 0.550862\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.5135\n",
      "---- Val Loss: 0.1841\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 23/100 =====\n",
      "Mean grad norm: 0.065694\n",
      "Max grad norm: 0.491328\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.5098\n",
      "---- Val Loss: 0.1705\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 24/100 =====\n",
      "Mean grad norm: 0.063199\n",
      "Max grad norm: 0.511739\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4978\n",
      "---- Val Loss: 0.1774\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 25/100 =====\n",
      "Mean grad norm: 0.059493\n",
      "Max grad norm: 0.533305\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4972\n",
      "---- Val Loss: 0.1706\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 26/100 =====\n",
      "Mean grad norm: 0.059638\n",
      "Max grad norm: 0.601348\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4950\n",
      "---- Val Loss: 0.1735\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 27/100 =====\n",
      "Mean grad norm: 0.062692\n",
      "Max grad norm: 0.585406\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4888\n",
      "---- Val Loss: 0.1702\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 28/100 =====\n",
      "Mean grad norm: 0.057680\n",
      "Max grad norm: 0.653954\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4935\n",
      "---- Val Loss: 0.1635\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 29/100 =====\n",
      "Mean grad norm: 0.063838\n",
      "Max grad norm: 0.524278\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4897\n",
      "---- Val Loss: 0.1596\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 30/100 =====\n",
      "Mean grad norm: 0.067631\n",
      "Max grad norm: 0.503482\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4831\n",
      "---- Val Loss: 0.1596\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 31/100 =====\n",
      "Mean grad norm: 0.065871\n",
      "Max grad norm: 0.510337\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4770\n",
      "---- Val Loss: 0.1595\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 32/100 =====\n",
      "Mean grad norm: 0.060283\n",
      "Max grad norm: 0.543920\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4674\n",
      "---- Val Loss: 0.1635\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 33/100 =====\n",
      "Mean grad norm: 0.046455\n",
      "Max grad norm: 0.693198\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4627\n",
      "---- Val Loss: 0.1607\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 34/100 =====\n",
      "Mean grad norm: 0.065879\n",
      "Max grad norm: 0.471940\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4577\n",
      "---- Val Loss: 0.1580\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 35/100 =====\n",
      "Mean grad norm: 0.052470\n",
      "Max grad norm: 0.613231\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4573\n",
      "---- Val Loss: 0.1612\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 36/100 =====\n",
      "Mean grad norm: 0.059965\n",
      "Max grad norm: 0.552160\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4579\n",
      "---- Val Loss: 0.1683\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 37/100 =====\n",
      "Mean grad norm: 0.056038\n",
      "Max grad norm: 0.623918\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4595\n",
      "---- Val Loss: 0.1547\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 38/100 =====\n",
      "Mean grad norm: 0.059115\n",
      "Max grad norm: 0.581960\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4527\n",
      "---- Val Loss: 0.1537\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 39/100 =====\n",
      "Mean grad norm: 0.061905\n",
      "Max grad norm: 0.512468\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4433\n",
      "---- Val Loss: 0.1503\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 40/100 =====\n",
      "Mean grad norm: 0.065714\n",
      "Max grad norm: 0.543839\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4370\n",
      "---- Val Loss: 0.1494\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 41/100 =====\n",
      "Mean grad norm: 0.052811\n",
      "Max grad norm: 0.641376\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4416\n",
      "---- Val Loss: 0.1547\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 42/100 =====\n",
      "Mean grad norm: 0.065685\n",
      "Max grad norm: 0.523899\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4381\n",
      "---- Val Loss: 0.1459\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 43/100 =====\n",
      "Mean grad norm: 0.052295\n",
      "Max grad norm: 0.655570\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4380\n",
      "---- Val Loss: 0.1551\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 44/100 =====\n",
      "Mean grad norm: 0.056402\n",
      "Max grad norm: 0.523289\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4325\n",
      "---- Val Loss: 0.1568\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 45/100 =====\n",
      "Mean grad norm: 0.058846\n",
      "Max grad norm: 0.617465\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4262\n",
      "---- Val Loss: 0.1508\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 46/100 =====\n",
      "Mean grad norm: 0.066664\n",
      "Max grad norm: 0.443796\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000100\n",
      "---- Train Loss: 0.4290\n",
      "---- Val Loss: 0.1509\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 47/100 =====\n",
      "Mean grad norm: 0.062575\n",
      "Max grad norm: 0.529467\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000100\n",
      "---- Train Loss: 0.4131\n",
      "---- Val Loss: 0.1435\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 48/100 =====\n",
      "Mean grad norm: 0.066613\n",
      "Max grad norm: 0.486457\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000100\n",
      "---- Train Loss: 0.4004\n",
      "---- Val Loss: 0.1445\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 49/100 =====\n",
      "Mean grad norm: 0.063527\n",
      "Max grad norm: 0.499954\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000100\n",
      "---- Train Loss: 0.4025\n",
      "---- Val Loss: 0.1447\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 50/100 =====\n",
      "Mean grad norm: 0.063072\n",
      "Max grad norm: 0.541598\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000100\n",
      "---- Train Loss: 0.3943\n",
      "---- Val Loss: 0.1460\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 51/100 =====\n",
      "Mean grad norm: 0.057755\n",
      "Max grad norm: 0.533502\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000050\n",
      "---- Train Loss: 0.3921\n",
      "---- Val Loss: 0.1438\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 52/100 =====\n",
      "Mean grad norm: 0.066923\n",
      "Max grad norm: 0.492836\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000050\n",
      "---- Train Loss: 0.3836\n",
      "---- Val Loss: 0.1436\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 53/100 =====\n",
      "Mean grad norm: 0.066922\n",
      "Max grad norm: 0.504489\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000050\n",
      "---- Train Loss: 0.3764\n",
      "---- Val Loss: 0.1419\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 54/100 =====\n",
      "Mean grad norm: 0.059516\n",
      "Max grad norm: 0.453922\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000050\n",
      "---- Train Loss: 0.3809\n",
      "---- Val Loss: 0.1435\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 55/100 =====\n",
      "Mean grad norm: 0.060290\n",
      "Max grad norm: 0.554498\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000050\n",
      "---- Train Loss: 0.3745\n",
      "---- Val Loss: 0.1457\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 56/100 =====\n",
      "Mean grad norm: 0.067219\n",
      "Max grad norm: 0.481004\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000050\n",
      "---- Train Loss: 0.3750\n",
      "---- Val Loss: 0.1439\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 57/100 =====\n",
      "Mean grad norm: 0.051172\n",
      "Max grad norm: 0.660894\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000025\n",
      "---- Train Loss: 0.3717\n",
      "---- Val Loss: 0.1474\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 58/100 =====\n",
      "Mean grad norm: 0.067454\n",
      "Max grad norm: 0.503790\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000025\n",
      "---- Train Loss: 0.3695\n",
      "---- Val Loss: 0.1428\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 59/100 =====\n",
      "Mean grad norm: 0.068687\n",
      "Max grad norm: 0.529470\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000025\n",
      "---- Train Loss: 0.3667\n",
      "---- Val Loss: 0.1442\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 60/100 =====\n",
      "Mean grad norm: 0.059727\n",
      "Max grad norm: 0.557198\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000025\n",
      "---- Train Loss: 0.3682\n",
      "---- Val Loss: 0.1430\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 61/100 =====\n",
      "Mean grad norm: 0.064084\n",
      "Max grad norm: 0.564259\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000013\n",
      "---- Train Loss: 0.3663\n",
      "---- Val Loss: 0.1436\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 62/100 =====\n",
      "Mean grad norm: 0.059040\n",
      "Max grad norm: 0.530107\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000013\n",
      "---- Train Loss: 0.3630\n",
      "---- Val Loss: 0.1434\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 63/100 =====\n",
      "Mean grad norm: 0.070267\n",
      "Max grad norm: 0.408070\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000013\n",
      "---- Train Loss: 0.3652\n",
      "---- Val Loss: 0.1439\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 64/100 =====\n",
      "Mean grad norm: 0.059384\n",
      "Max grad norm: 0.530141\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000013\n",
      "---- Train Loss: 0.3639\n",
      "---- Val Loss: 0.1431\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 65/100 =====\n",
      "Mean grad norm: 0.059042\n",
      "Max grad norm: 0.548289\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000006\n",
      "---- Train Loss: 0.3596\n",
      "---- Val Loss: 0.1430\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 66/100 =====\n",
      "Mean grad norm: 0.054387\n",
      "Max grad norm: 0.517726\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000006\n",
      "---- Train Loss: 0.3579\n",
      "---- Val Loss: 0.1433\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 67/100 =====\n",
      "Mean grad norm: 0.065376\n",
      "Max grad norm: 0.517864\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000006\n",
      "---- Train Loss: 0.3591\n",
      "---- Val Loss: 0.1431\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 68/100 =====\n",
      "Mean grad norm: 0.048130\n",
      "Max grad norm: 0.448232\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000006\n",
      "---- Train Loss: 0.3623\n",
      "---- Val Loss: 0.1430\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 69/100 =====\n",
      "Mean grad norm: 0.051778\n",
      "Max grad norm: 0.458744\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000003\n",
      "---- Train Loss: 0.3551\n",
      "---- Val Loss: 0.1439\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 70/100 =====\n",
      "Mean grad norm: 0.065970\n",
      "Max grad norm: 0.506179\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000003\n",
      "---- Train Loss: 0.3628\n",
      "---- Val Loss: 0.1436\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 71/100 =====\n",
      "Mean grad norm: 0.061197\n",
      "Max grad norm: 0.508234\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000003\n",
      "---- Train Loss: 0.3630\n",
      "---- Val Loss: 0.1431\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 72/100 =====\n",
      "Mean grad norm: 0.068414\n",
      "Max grad norm: 0.438439\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000003\n",
      "---- Train Loss: 0.3569\n",
      "---- Val Loss: 0.1435\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 73/100 =====\n",
      "Mean grad norm: 0.056093\n",
      "Max grad norm: 0.504668\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000002\n",
      "---- Train Loss: 0.3613\n",
      "---- Val Loss: 0.1434\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 74/100 =====\n",
      "Mean grad norm: 0.069608\n",
      "Max grad norm: 0.477030\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000002\n",
      "---- Train Loss: 0.3602\n",
      "---- Val Loss: 0.1430\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 75/100 =====\n",
      "Mean grad norm: 0.068482\n",
      "Max grad norm: 0.498535\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000002\n",
      "---- Train Loss: 0.3566\n",
      "---- Val Loss: 0.1430\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 76/100 =====\n",
      "Mean grad norm: 0.057556\n",
      "Max grad norm: 0.624162\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000002\n",
      "---- Train Loss: 0.3615\n",
      "---- Val Loss: 0.1439\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 77/100 =====\n",
      "Mean grad norm: 0.065804\n",
      "Max grad norm: 0.519335\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000001\n",
      "---- Train Loss: 0.3612\n",
      "---- Val Loss: 0.1432\n",
      "Learning rate достиг минимума 1e-6, остановка обучения\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "lst_loss_train = []\n",
    "lst_loss_val = []\n",
    "best_val_loss = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "\n",
    "    epoch_train_loss = 0.0\n",
    "    train_predicts = []\n",
    "\n",
    "    train_tqdm = tqdm(train_dl, desc=f\"Эпоха {epoch+1}/{EPOCHS} [Обучение]\", leave=False)\n",
    "    for batch_ind, batch in enumerate(train_tqdm):\n",
    "        mel_spec, mel_spec_lens, targets, targets_lens, _ = batch\n",
    "        mel_spec = mel_spec.to(DIVICE)\n",
    "        mel_spec_lens = mel_spec_lens.to(DIVICE)\n",
    "        targets = targets.to(DIVICE)\n",
    "        targets_lens = targets_lens.to(DIVICE)\n",
    "\n",
    "        #===== считатем длинну mel_spec для передачи в CTC loss =====\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predict = model(mel_spec, mel_spec_lens) # (N=batch,T,C)\n",
    "        N = predict.shape[1]\n",
    "        T = predict.shape[0]\n",
    "        predict_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "\n",
    "        try:\n",
    "            loss = loss_func(predict, targets, predict_lengths, targets_lens.reshape(BATCH_SIZE))\n",
    "        except RuntimeError:\n",
    "            print(predict.shape, targets.shape, predict_lengths, targets_lens.reshape(BATCH_SIZE))\n",
    "            continue\n",
    "\n",
    "        if torch.isnan(loss) or torch.isinf(loss): \n",
    "            print(f\"\\nWarning: In batch-{batch_ind} loss train is NaN/Inf: {loss.item()}\"); \n",
    "            optimizer.zero_grad(); \n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "    total_train = epoch_train_loss / len(train_dl)\n",
    "\n",
    "    # ======== Валидация ========\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    total_val = 0\n",
    "    val_predicts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_tqdm = tqdm(val_dl, desc=f\"Эпоха {epoch+1}/{EPOCHS} [Валидация]\", leave=False)\n",
    "        for val_mel_spec, val_spec_lens, val_labels, val_label_lensin, _ in val_tqdm:\n",
    "\n",
    "            val_mel_spec = val_mel_spec.to(DIVICE)\n",
    "            val_spec_lens = val_spec_lens.to(DIVICE)\n",
    "            val_labels = val_labels.to(DIVICE)\n",
    "            val_label_lensin = val_label_lensin.to(DIVICE)\n",
    "\n",
    "            val_predict = model(val_mel_spec, val_spec_lens)\n",
    "            val_N = val_predict.shape[1]\n",
    "            val_T = val_predict.shape[0]\n",
    "\n",
    "            predict_val_lengths = torch.full(size=(val_N,), fill_value=val_T, dtype=torch.long)\n",
    "            val_loss += loss_func(val_predict, val_labels, predict_val_lengths, val_label_lensin).item()\n",
    "\n",
    "    total_val = val_loss / len(val_dl)\n",
    "\n",
    "    lst_loss_train.append(total_train)\n",
    "    lst_loss_val.append(total_val)\n",
    "\n",
    "    scheduler.step(total_val)\n",
    "\n",
    "    print(f\"\\n===== Эпоха {epoch+1}/{EPOCHS} =====\")\\\n",
    "    #===== Инфо про градиенты=====\n",
    "    grad_norms = [param.grad.norm().item() for param in model.parameters() if param.grad is not None]\n",
    "    if grad_norms:\n",
    "        print(f\"Mean grad norm: {np.mean(grad_norms):.6f}\")\n",
    "        print(f\"Max grad norm: {np.max(grad_norms):.6f}\")\n",
    "        print(f\"Min grad norm: {np.min(grad_norms):.6f}\")\n",
    "    else:\n",
    "        print(\"No gradients computed yet.\")\n",
    "    #===== Инфо про шаг обучения и данные по потерям =====\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Current LR: {current_lr:.6f}\")\n",
    "    print(f\"---- Train Loss: {total_train:.4f}\")\n",
    "    print(f\"---- Val Loss: {total_val:.4f}\")\n",
    "    if current_lr <= 1e-6:\n",
    "        print(\"Learning rate достиг минимума 1e-6, остановка обучения\")\n",
    "        break\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e3f851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca52e14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'MorseNet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1547a2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lst_loss_train)\n",
    "plt.plot(lst_loss_val)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45f65295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN размерность выхода: torch.Size([1, 64, 16, 89])\n",
      "CNN число фичей: 1024\n",
      "Проекция из 1024 в 1024\n",
      "Mean accurasu by The Levenshtein in train is : 0.8932685114069244\n",
      "Mean accurasu by The Levenshtein in validate is : 0.9503369680701127\n"
     ]
    }
   ],
   "source": [
    "def ctc_decoder(logits, int_char_map, blank_label_idx):\n",
    "    preds = []\n",
    "    logits_cpu = logits.cpu() \n",
    "    max_inds = torch.argmax(logits_cpu.detach(), dim=2).t().numpy() # арзмакс по лагитам и преобразование к словарю\n",
    "    \n",
    "    for ind in max_inds:\n",
    "        merged_inds = []\n",
    "        for idx in ind:\n",
    "            if idx != blank_label_idx: \n",
    "                merged_inds.append(idx)\n",
    "        text = \"\".join([int_char_map.get(i, '?') for i in merged_inds])\n",
    "        preds.append(text)\n",
    "\n",
    "    return preds\n",
    "\n",
    "model_load = MorseNet(num_classes=num_classes).to(DIVICE)\n",
    "model_load.load_state_dict(torch.load('MorseNet.pth'))\n",
    "model_load.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_mess = []\n",
    "    train_predicts = []\n",
    "    for loader in train_dl:\n",
    "        seq,seq_lens, test_target, _, mess = loader\n",
    "        seq = seq.to(DIVICE)\n",
    "        seq_lens = seq_lens.to(DIVICE)\n",
    "        train_mess.extend(mess)\n",
    "\n",
    "        logits = model_load(seq,seq_lens)\n",
    "        predicted_values = ctc_decoder(logits, int_to_char, BLANK_IDX)\n",
    "        train_predicts.extend(predicted_values)\n",
    "\n",
    "    val_mess = []\n",
    "    val_predicts = []\n",
    "    for loader in val_dl:\n",
    "        seq,seq_lens, test_target, _, mess = loader\n",
    "        seq = seq.to(DIVICE)\n",
    "        seq_lens = seq_lens.to(DIVICE)\n",
    "        val_mess.extend(mess)\n",
    "\n",
    "        logits= model_load(seq,seq_lens)\n",
    "        predicted_values = ctc_decoder(logits, int_to_char, BLANK_IDX)\n",
    "        val_predicts.extend(predicted_values)\n",
    "\n",
    "import Levenshtein\n",
    "\n",
    "mean_acc_test = np.mean([Levenshtein.ratio(test_pred, train_mess[ind]) for ind, test_pred in enumerate(train_predicts)])\n",
    "mean_acc_val = np.mean([Levenshtein.ratio(val_pred, val_mess[ind]) for ind, val_pred in enumerate(val_predicts)])\n",
    "\n",
    "\n",
    "print(f\"Mean accurasu by The Levenshtein in train is : {mean_acc_test}\")\n",
    "print(f\"Mean accurasu by The Levenshtein in validate is : {mean_acc_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49ca828",
   "metadata": {},
   "source": [
    "# Сбор sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85e44a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN размерность выхода: torch.Size([1, 64, 16, 89])\n",
      "CNN число фичей: 1024\n",
      "Проекция из 1024 в 1024\n",
      "Mean accurasu by The Levenshtein in train is : 0.9066151537313227\n",
      "Mean accurasu by The Levenshtein in validate is : 0.9661893167521509\n"
     ]
    }
   ],
   "source": [
    "def ctc_decoder(logits, int_char_map, blank_label_idx):\n",
    "    preds = []\n",
    "    logits_cpu = logits.cpu()\n",
    "    max_inds = torch.argmax(logits_cpu.detach(), dim=2).t().numpy()\n",
    "\n",
    "    for ind in max_inds:\n",
    "        merged_inds = []\n",
    "        prev_idx = None\n",
    "        for idx in ind:\n",
    "            if idx != blank_label_idx and idx != prev_idx:\n",
    "                merged_inds.append(idx)\n",
    "            prev_idx = idx\n",
    "        text = \"\".join([int_char_map.get(i, '') for i in merged_inds])\n",
    "        preds.append(text)\n",
    "\n",
    "    return preds\n",
    "\n",
    "model_load = MorseNet(num_classes=num_classes).to(DIVICE)\n",
    "model_load.load_state_dict(torch.load('MorseNet.pth'))\n",
    "model_load.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_mess = []\n",
    "    train_predicts = []\n",
    "    for loader in train_dl:\n",
    "        seq,seq_lens, test_target, _, mess = loader\n",
    "        seq = seq.to(DIVICE)\n",
    "        seq_lens = seq_lens.to(DIVICE)\n",
    "        train_mess.extend(mess)\n",
    "\n",
    "        logits = model_load(seq,seq_lens)\n",
    "        predicted_values = ctc_decoder(logits, int_to_char, BLANK_IDX)\n",
    "        train_predicts.extend(predicted_values)\n",
    "\n",
    "    val_mess = []\n",
    "    val_predicts = []\n",
    "    for loader in val_dl:\n",
    "        seq,seq_lens, test_target, _, mess = loader\n",
    "        seq = seq.to(DIVICE)\n",
    "        seq_lens = seq_lens.to(DIVICE)\n",
    "        val_mess.extend(mess)\n",
    "\n",
    "        logits= model_load(seq,seq_lens)\n",
    "        predicted_values = ctc_decoder(logits, int_to_char, BLANK_IDX)\n",
    "        val_predicts.extend(predicted_values)\n",
    "\n",
    "import Levenshtein\n",
    "\n",
    "mean_acc_test = np.mean([Levenshtein.ratio(test_pred, train_mess[ind]) for ind, test_pred in enumerate(train_predicts)])\n",
    "mean_acc_val = np.mean([Levenshtein.ratio(val_pred, val_mess[ind]) for ind, val_pred in enumerate(val_predicts)])\n",
    "\n",
    "\n",
    "print(f\"Mean accurasu by The Levenshtein in train is : {mean_acc_test}\")\n",
    "print(f\"Mean accurasu by The Levenshtein in validate is : {mean_acc_val}\")\n",
    "\n",
    "\n",
    "test_ds = MosreDatasetToTest(df=sample_data,\n",
    "                        data_patch=SAVE_DIR/'test',\n",
    "                        char_to_int=char_to_int,\n",
    "                        train=False)\n",
    "\n",
    "def my_collate(batch):\n",
    "    spectrograms = [item[0].squeeze(0) for item in batch]\n",
    "    # Падинг спектрограмм по максимальной длине\n",
    "    spectrograms_permuted = [s.permute(1, 0) for s in spectrograms]\n",
    "    spectrograms_padded = nn.utils.rnn.pad_sequence(spectrograms_permuted, batch_first=True, padding_value=0.0)\n",
    "    spectrograms_padded = spectrograms_padded.permute(0, 2, 1).unsqueeze(1)\n",
    "    # spec_lens = torch.stack([item[0] for item in batch])\n",
    "    spec_lens = torch.tensor([item[1] for item in batch]).reshape(20)\n",
    "    if batch[0][3] is not None:\n",
    "        target = torch.nn.utils.rnn.pad_sequence(\n",
    "                                                [item[2] for item in batch], \n",
    "                                                batch_first=True, \n",
    "                                                padding_value=BLANK_IDX)# выравнивает последовательность до макс \n",
    "                                                                        # длины в батче заполняя пропуски нулем\n",
    "        label_len = torch.stack([item[3] for item in batch])\n",
    "        msg = [item[4] for item in batch]\n",
    "        \n",
    "        return [spectrograms_padded, spec_lens, target, label_len, msg]\n",
    "    else: \n",
    "        return [spectrograms_padded, spec_lens]\n",
    "    \n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=20, shuffle=False, collate_fn=my_collate)\n",
    "model_load.eval()\n",
    "next(iter(test_dl))\n",
    "with torch.no_grad():\n",
    "    test_predicts = []\n",
    "    for loader in test_dl:\n",
    "        seq, seq_lens = loader\n",
    "        seq = seq.to(DIVICE)\n",
    "        seq_lens = seq_lens.to(DIVICE)\n",
    "\n",
    "        logits = model_load(seq,seq_lens)\n",
    "        predicted_values = ctc_decoder(logits, int_to_char, BLANK_IDX)\n",
    "        test_predicts.extend(predicted_values)\n",
    "\n",
    "sample_data.message = test_predicts\n",
    "sample_data.to_csv('sample_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Morse_decoder_V2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
