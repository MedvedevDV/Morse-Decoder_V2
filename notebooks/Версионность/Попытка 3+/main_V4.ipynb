{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d761d8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN размерность выхода: torch.Size([1, 32, 16, 89])\n",
      "CNN число фичей: 512\n",
      "Проекция из 512 в 512\n",
      "\n",
      "MorseNet - инициалицация модели. Число обучаемых параметров: 2,808,589\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path as pt\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchaudio import transforms\n",
    "from torchvision.transforms import v2\n",
    "# from Moduls.MosreDataset import MosreDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.nn.parallel import DataParallel\n",
    "from collections import Counter\n",
    "\n",
    "DIVICE = torch.device(\"cuda\")\n",
    "\n",
    "MAIN = pt(os.getcwd())\n",
    "DATASET_PATCH = MAIN / 'morse_dataset'\n",
    "AUDIO_FILES = DATASET_PATCH / 'morse_dataset'\n",
    "\n",
    "# Поятоянные значения выявленные в процессе анализа\n",
    "MORSEALP = \"АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ 1234567890#\"\n",
    "MAX_TIME = 48\n",
    "SAMPLE_RATE = 8000\n",
    "N_MELS = 128\n",
    "N_FFT = 400\n",
    "HOP_LENGTH = 180\n",
    "TOP_DB = 80\n",
    "FREQ_MASK = 15\n",
    "TIME_MASK = 20\n",
    "\n",
    "# Гиперпараметы обучения\n",
    "SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 70\n",
    "LEARNING_RATE = 0.002 #2e-4\n",
    "WEIGHT_DECAY = 0.00001\n",
    "# int_to_alph = dict(enumerate(MORSEALP))\n",
    "# alph_to_int = {char:enum for enum, char in int_to_alph.items()}\n",
    "\n",
    "#===== Import data =====\n",
    "train_data = pd.read_csv(pt.joinpath(DATASET_PATCH,'train.csv'))\n",
    "test_data = pd.read_csv(pt.joinpath(DATASET_PATCH,'test.csv'))\n",
    "sample_data = pd.read_csv(pt.joinpath(DATASET_PATCH,'sample_submission.csv'))\n",
    "\n",
    "all_chars = Counter(\"\".join(train_data['message']))\n",
    "BLANK_CHAR = \"_\"\n",
    "vocab_list = sorted(all_chars.keys()) + [BLANK_CHAR]\n",
    "num_classes = len(vocab_list)\n",
    "char_to_int = {char: i for i, char in enumerate(vocab_list)}\n",
    "int_to_char = {i: char for i, char in enumerate(vocab_list)}\n",
    "BLANK_IDX = char_to_int[BLANK_CHAR]\n",
    "\n",
    "class MosreDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Класс для обработки \n",
    "    \"\"\"\n",
    "    def __init__(self, df, data_patch,char_to_int, train=True, transforms=None, prev_chars = 1):\n",
    "        self.df = df\n",
    "        self.is_train = train\n",
    "\n",
    "        self.data_path = data_patch\n",
    "        self.audio_paths = self.data_path / 'morse_dataset'\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.char_to_int = char_to_int\n",
    "        self.prev_chars = prev_chars\n",
    "\n",
    "        if self.is_train:\n",
    "            self.messeges = self.df.message.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #Получение аугментрованых спектрограмм\n",
    "        try:\n",
    "            audio_file = self.audio_paths / self.df.id.values[index]\n",
    "            waveform, sample_rate = torchaudio.load(audio_file)\n",
    "            augmented_spectrogram = self.transforms(waveform)\n",
    "\n",
    "            if self.is_train:\n",
    "                message = self.messeges[index]\n",
    "                #Получение списка индексов секта - как требует CTC los\n",
    "                '''\n",
    "                При обработке dataloader labels будут выравниваться по макс длине для выравнивания батча\n",
    "                Т.е. будет padding 0. что в будующем будет пустым значением для ctc loss\n",
    "                '''\n",
    "                target = torch.tensor([self.char_to_int[char] for char in message], dtype=torch.long); \n",
    "                target_len = torch.tensor(len(target), dtype=torch.long)\n",
    "                return augmented_spectrogram, target, target_len, message\n",
    "            else:\n",
    "                return augmented_spectrogram, None, None, None\n",
    "        except Exception as ex:\n",
    "            print(str(ex))\n",
    "        \n",
    "    def change_time(self, audio_file, max_len = 384000):\n",
    "        waveform, sample_rate = torchaudio.load(audio_file)\n",
    "        cahanal, sig_len = waveform.shape\n",
    "\n",
    "        if sig_len < max_len:\n",
    "            pad_len = torch.zeros(max_len - sig_len).unsqueeze(0)\n",
    "            waveform = torch.cat([waveform, pad_len], dim=1)\n",
    "\n",
    "        return waveform\n",
    "    \n",
    "FIRST_FE_COUNT = 16\n",
    "SECOND_FE_COUNT = 32\n",
    "THIRD_FE_COUNT = 32\n",
    "QAD_FE_COUNT = 32\n",
    "PADDING = 'same'\n",
    "MAXPOOL_KERNEL = 2\n",
    "KERTNEL_SIZE = 3\n",
    "NERON_COUNT = 128\n",
    "GRU_HIDEN = 256\n",
    "# Start with 4 transforms\n",
    "class MorseNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.net_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, \n",
    "                      out_channels=FIRST_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(FIRST_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((1, 2), (1, 2)), # [batch, FIRST_FE_COUNT = 16, 64, 960]\n",
    "\n",
    "            nn.Conv2d(in_channels=FIRST_FE_COUNT, \n",
    "                      out_channels=SECOND_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(SECOND_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)), # [batch, SECOND_FE_COUNT = 32, 32, 480]\n",
    "\n",
    "            nn.Conv2d(in_channels=SECOND_FE_COUNT, \n",
    "                      out_channels=THIRD_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(THIRD_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((2, 2), (2, 2)), # [batch, THIRD_FE_COUNT = 32, 16, 240]\n",
    "\n",
    "            nn.Conv2d(in_channels=THIRD_FE_COUNT, \n",
    "                      out_channels=QAD_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(QAD_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)) # [batch=32, QAD_FE_COUNT = 32, 8, 80](что юы сохраниить большще признаков по горизонтали)\n",
    "        )\n",
    "        with torch.no_grad(): \n",
    "            dummy_input = torch.randn(1, 1, N_MELS, 356); \n",
    "            cnn_out = self.net_conv(dummy_input); \n",
    "            self.cnn_output_features = cnn_out.shape[1] * cnn_out.shape[2]\n",
    "\n",
    "        print(f\"CNN размерность выхода: {cnn_out.shape}\"); \n",
    "        print(f\"CNN число фичей: {self.cnn_output_features}\")\n",
    "\n",
    "        # Добавлен лоейный слой и функция активации. Для чего? расписать потом \n",
    "        self.layer1 = nn.Linear(self.cnn_output_features, N_MELS*2); \n",
    "        self.gelu = nn.GELU()\n",
    "        \n",
    "        self.rnn = nn.LSTM(\n",
    "                input_size=N_MELS*2,\n",
    "                hidden_size=GRU_HIDEN,\n",
    "                num_layers=2,\n",
    "                bidirectional=True,\n",
    "                dropout=0.3,\n",
    "                batch_first=True \n",
    "            )\n",
    "\n",
    "        \n",
    "        self.embed_dim = GRU_HIDEN * 2\n",
    "        self.layer_norm = nn.LayerNorm(self.embed_dim)      \n",
    "        self.dropout = nn.Dropout(0.3)   \n",
    "        self.layer2 = nn.Linear(self.embed_dim, num_classes)       \n",
    "        # self.layer3 = nn.Linear(GRU_HIDEN, GRU_HIDEN // 2)       \n",
    "        # self.layer4 = nn.Linear(GRU_HIDEN // 2, 45)             \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net_conv(x)\n",
    "\n",
    "        batch, channels, reduced_mels, reduced_time = x.shape\n",
    "        x = x.permute(0, 3, 1, 2)  # [batch, time, channels, mels]\n",
    "\n",
    "        # В частности, каждый вектор признаков в последовательности признаков генерируется \n",
    "        # слева направо на картах признаков. Это означает, что i-й вектор признаков представляет \n",
    "        # собой объединение столбцов всех карт. \n",
    "        # Таким образом, форма тензора может быть изменена, например, на (размер_пакета, 80, 256)\n",
    "        \n",
    "        x = x.reshape(batch, reduced_time, -1)  # to GRU [batch=32, seq_len=89, features/hiden_dim=512]\n",
    "        x = self.layer1(x)\n",
    "        x = self.gelu(x)\n",
    "\n",
    "        self.rnn.flatten_parameters()\n",
    "\n",
    "        x = self.rnn(x) # [batch=32, seq_len=89, features/hiden_dim=256 * 2]\n",
    "        x, _ = x # берем информацию со всез состояний\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x) # logits - [batch, sequence, num_classes] \n",
    "        x = nn.functional.log_softmax(x.permute(1,0,2), dim=2) # pertime так как CTC loss требует на взод (sequence/T,batch/N,num_classes/C)\n",
    "        '''\n",
    "        по одному прогнозу для каждого из признаков в последовательности, \n",
    "        в итоге получается 89 прогнозов символов для каждой секунды звука.\n",
    "        '''\n",
    "        return x\n",
    "    \n",
    "\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS),\n",
    "    transforms.AmplitudeToDB(top_db=TOP_DB),\n",
    "    transforms.FrequencyMasking(freq_mask_param=FREQ_MASK),\n",
    "    transforms.TimeMasking(time_mask_param=TIME_MASK),\n",
    "    # v2.RandomCrop((N_MELS, 1920)) # Обрезает последний кадр спектрограммы, в идеале надобы считать а не прописывать число\n",
    "    ) # заметка - Данные трансформации не создают довых обучаемых параметров. Но есть и те что создают. В будущем это стоит учитывать\n",
    "\n",
    "valid_audio_transforms = nn.Sequential(\n",
    "    transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS),\n",
    "    transforms.AmplitudeToDB(top_db=TOP_DB),\n",
    "    # v2.CenterCrop((N_MELS, 1920)) \n",
    "    )\n",
    "\n",
    "train_dataframe, val_dataframe = train_test_split(train_data, test_size=0.15, random_state=SEED)\n",
    "\n",
    "train_ds = MosreDataset(df=train_dataframe,\n",
    "                        data_patch=DATASET_PATCH,\n",
    "                        char_to_int=char_to_int,\n",
    "                        train=True,\n",
    "                        transforms=train_audio_transforms)\n",
    "\n",
    "val_ds = MosreDataset(df=val_dataframe,\n",
    "                        data_patch=DATASET_PATCH,\n",
    "                        char_to_int=char_to_int,\n",
    "                        train=True,\n",
    "                        transforms=valid_audio_transforms)\n",
    "\n",
    "\n",
    "def my_collate(batch):\n",
    "    spectrograms = [item[0].squeeze(0) for item in batch]\n",
    "    # Падинг спектрограмм по максимальной длине\n",
    "    spectrograms_permuted = [s.permute(1, 0) for s in spectrograms]\n",
    "    spectrograms_padded = nn.utils.rnn.pad_sequence(spectrograms_permuted, batch_first=True, padding_value=0.0)\n",
    "    spectrograms_padded = spectrograms_padded.permute(0, 2, 1).unsqueeze(1)\n",
    "\n",
    "    if batch[0][3] is not None:\n",
    "        target = torch.nn.utils.rnn.pad_sequence(\n",
    "                                                [item[1] for item in batch], \n",
    "                                                batch_first=True, \n",
    "                                                padding_value=BLANK_IDX)# выравнивает последовательность до макс \n",
    "                                                                        # длины в батче заполняя пропуски нулем\n",
    "        label_len = torch.stack([item[2] for item in batch])\n",
    "        msg = [item[3] for item in batch]\n",
    "        \n",
    "        return [spectrograms_padded, target, label_len, msg]\n",
    "    else: \n",
    "        return spectrograms_padded\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=my_collate, drop_last=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=my_collate, drop_last=True)\n",
    "\n",
    "test, test_target, _, mess = next(iter(train_dl))\n",
    "test, test_target= test.to(DIVICE), test_target.to(DIVICE)\n",
    "\n",
    "test_val, val_target, __, val_mess = next(iter(val_dl))\n",
    "test_val, val_target = test_val.to(DIVICE), val_target.to(DIVICE)\n",
    "# test.shape \n",
    "\n",
    "#===== начало обучения =====\n",
    "model = MorseNet(num_classes=num_classes).to(DIVICE)\n",
    "# model = DataParallel(model)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=0.002)  # Было 0.002\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "loss_func = nn.CTCLoss(blank=BLANK_IDX, reduction='mean', zero_infinity=True).to(DIVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nMorseNet - инициалицация модели. Число обучаемых параметров: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08e52f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLANK_CHAR = \"_\"\n",
    "# vocab_list = sorted(all_chars.keys()) + [BLANK_CHAR]\n",
    "# num_classes = len(vocab_list)\n",
    "# char_to_int = {char: i for i, char in enumerate(vocab_list)}\n",
    "# int_to_char = {i: char for i, char in enumerate(vocab_list)}\n",
    "# BLANK_IDX = char_to_int[BLANK_CHAR]\n",
    "# vocab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be51c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a= model(test)\n",
    "# a.shape, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "297d2357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8, 12,  7, 11,  9, 10,  8, 11,  8,  9,  8,  8, 11, 10,  9, 11, 11,  9,\n",
       "         7,  8,  8,  9, 11,  8,  9,  9,  9,  8, 10,  8,  6, 11])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_.reshape(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa77922d",
   "metadata": {},
   "source": [
    "Подсказка по ctc loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "222c0bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\homer\\AppData\\Local\\Temp\\ipykernel_20744\\2733710114.py:15: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  loss.grad\n"
     ]
    }
   ],
   "source": [
    "# Target are to be un-padded\n",
    "T = 50      # Input sequence length\n",
    "C = 20      # Number of classes (including blank)\n",
    "N = 16      # Batch size\n",
    "# Initialize random batch of input vectors, for *size = (T,N,C)\n",
    "input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()\n",
    "input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "# Initialize random batch of targets (0 = blank, 1:C = classes)\n",
    "target_lengths = torch.randint(low=1, high=T, size=(N,), dtype=torch.long)\n",
    "target = torch.randint(low=1, high=C, size=(sum(target_lengths),), dtype=torch.long)\n",
    "ctc_loss = nn.CTCLoss()\n",
    "loss = ctc_loss(input, target, input_lengths, target_lengths)\n",
    "loss.backward()\n",
    "# input.detach().numpy().shape\n",
    "loss.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89dc4bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 16, 20]), torch.Size([334]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape, target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaf999d",
   "metadata": {},
   "source": [
    "# Декодировщик предсказаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad235125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = model(test)\n",
    "# a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa7f726",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c23dee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "def ctc_decoder(logits, int_char_map, blank_label_idx):\n",
    "    preds = []\n",
    "    logits_cpu = logits.cpu() \n",
    "    max_inds = torch.argmax(logits_cpu.detach(), dim=2).t().numpy() # арзмакс по лагитам и преобразование к словарю\n",
    "    \n",
    "    for ind in max_inds:\n",
    "        merged_inds = []\n",
    "        for idx in ind:\n",
    "            if idx != blank_label_idx: \n",
    "                merged_inds.append(idx)\n",
    "        text = \"\".join([int_char_map.get(i, '?') for i in merged_inds])\n",
    "        preds.append(text)\n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "422ea6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 1/70 =====\n",
      "Mean grad norm: 0.031749\n",
      "Max grad norm: 0.964444\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 4.1524\n",
      "---- Val Loss: 566.3249\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 2/70 =====\n",
      "Mean grad norm: 0.057799\n",
      "Max grad norm: 0.946184\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 3.9985\n",
      "---- Val Loss: 542.1041\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 3/70 =====\n",
      "Mean grad norm: 0.079392\n",
      "Max grad norm: 0.849592\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 1.2418\n",
      "---- Val Loss: 50.9208\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 4/70 =====\n",
      "Mean grad norm: 0.078201\n",
      "Max grad norm: 0.489384\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.5566\n",
      "---- Val Loss: 40.2232\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 5/70 =====\n",
      "Mean grad norm: 0.083760\n",
      "Max grad norm: 0.823228\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.4677\n",
      "---- Val Loss: 34.1836\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 6/70 =====\n",
      "Mean grad norm: 0.081633\n",
      "Max grad norm: 0.747327\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.4274\n",
      "---- Val Loss: 30.5460\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 7/70 =====\n",
      "Mean grad norm: 0.086838\n",
      "Max grad norm: 0.755736\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.3980\n",
      "---- Val Loss: 27.8897\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 8/70 =====\n",
      "Mean grad norm: 0.063483\n",
      "Max grad norm: 0.923405\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.3734\n",
      "---- Val Loss: 31.7352\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 9/70 =====\n",
      "Mean grad norm: 0.058400\n",
      "Max grad norm: 0.394095\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.3577\n",
      "---- Val Loss: 25.5145\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 10/70 =====\n",
      "Mean grad norm: 0.078144\n",
      "Max grad norm: 0.429119\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.3423\n",
      "---- Val Loss: 28.1654\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 11/70 =====\n",
      "Mean grad norm: 0.107300\n",
      "Max grad norm: 0.633152\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.3309\n",
      "---- Val Loss: 25.2238\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 12/70 =====\n",
      "Mean grad norm: 0.089959\n",
      "Max grad norm: 0.763623\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.3213\n",
      "---- Val Loss: 27.4794\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 13/70 =====\n",
      "Mean grad norm: 0.084108\n",
      "Max grad norm: 0.773851\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.3165\n",
      "---- Val Loss: 23.7769\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 14/70 =====\n",
      "Mean grad norm: 0.075656\n",
      "Max grad norm: 0.584528\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.3109\n",
      "---- Val Loss: 22.3405\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 15/70 =====\n",
      "Mean grad norm: 0.083645\n",
      "Max grad norm: 0.775671\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.3042\n",
      "---- Val Loss: 22.0505\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 16/70 =====\n",
      "Mean grad norm: 0.098555\n",
      "Max grad norm: 0.565705\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.3010\n",
      "---- Val Loss: 21.9248\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 17/70 =====\n",
      "Mean grad norm: 0.094645\n",
      "Max grad norm: 0.729457\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.3027\n",
      "---- Val Loss: 22.1326\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 18/70 =====\n",
      "Mean grad norm: 0.086346\n",
      "Max grad norm: 0.841801\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.2934\n",
      "---- Val Loss: 23.8944\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 19/70 =====\n",
      "Mean grad norm: 0.096738\n",
      "Max grad norm: 0.735379\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 0.2830\n",
      "---- Val Loss: 23.0104\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 20/70 =====\n",
      "Mean grad norm: 0.063523\n",
      "Max grad norm: 0.520102\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.001000\n",
      "---- Train Loss: 0.2784\n",
      "---- Val Loss: 22.2050\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 21/70 =====\n",
      "Mean grad norm: 0.048975\n",
      "Max grad norm: 0.274780\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.001000\n",
      "---- Train Loss: 0.2460\n",
      "---- Val Loss: 19.9927\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 22/70 =====\n",
      "Mean grad norm: 0.070923\n",
      "Max grad norm: 0.446764\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.001000\n",
      "---- Train Loss: 0.2344\n",
      "---- Val Loss: 20.0059\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 23/70 =====\n",
      "Mean grad norm: 0.079054\n",
      "Max grad norm: 0.403918\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.001000\n",
      "---- Train Loss: 0.2326\n",
      "---- Val Loss: 19.9165\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 24/70 =====\n",
      "Mean grad norm: 0.087483\n",
      "Max grad norm: 0.512904\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.001000\n",
      "---- Train Loss: 0.2251\n",
      "---- Val Loss: 19.6405\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 25/70 =====\n",
      "Mean grad norm: 0.062460\n",
      "Max grad norm: 0.302706\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.001000\n",
      "---- Train Loss: 0.2206\n",
      "---- Val Loss: 19.9371\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 26/70 =====\n",
      "Mean grad norm: 0.073021\n",
      "Max grad norm: 0.419702\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.001000\n",
      "---- Train Loss: 0.2180\n",
      "---- Val Loss: 19.6642\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 27/70 =====\n",
      "Mean grad norm: 0.065123\n",
      "Max grad norm: 0.341130\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.001000\n",
      "---- Train Loss: 0.2124\n",
      "---- Val Loss: 20.5823\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 28/70 =====\n",
      "Mean grad norm: 0.076933\n",
      "Max grad norm: 0.368269\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000500\n",
      "---- Train Loss: 0.2107\n",
      "---- Val Loss: 20.4653\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 29/70 =====\n",
      "Mean grad norm: 0.050007\n",
      "Max grad norm: 0.261946\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000500\n",
      "---- Train Loss: 0.2007\n",
      "---- Val Loss: 19.3901\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 30/70 =====\n",
      "Mean grad norm: 0.054613\n",
      "Max grad norm: 0.435005\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000500\n",
      "---- Train Loss: 0.1943\n",
      "---- Val Loss: 19.5496\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 31/70 =====\n",
      "Mean grad norm: 0.069028\n",
      "Max grad norm: 0.395802\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000500\n",
      "---- Train Loss: 0.1905\n",
      "---- Val Loss: 19.6895\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 32/70 =====\n",
      "Mean grad norm: 0.072171\n",
      "Max grad norm: 0.424428\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000500\n",
      "---- Train Loss: 0.1899\n",
      "---- Val Loss: 19.5680\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 33/70 =====\n",
      "Mean grad norm: 0.076661\n",
      "Max grad norm: 0.374793\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000250\n",
      "---- Train Loss: 0.1817\n",
      "---- Val Loss: 20.3783\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 34/70 =====\n",
      "Mean grad norm: 0.085111\n",
      "Max grad norm: 0.472382\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000250\n",
      "---- Train Loss: 0.1791\n",
      "---- Val Loss: 19.5458\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 35/70 =====\n",
      "Mean grad norm: 0.052739\n",
      "Max grad norm: 0.288887\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000250\n",
      "---- Train Loss: 0.1770\n",
      "---- Val Loss: 19.8199\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 36/70 =====\n",
      "Mean grad norm: 0.092685\n",
      "Max grad norm: 0.571822\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000250\n",
      "---- Train Loss: 0.1760\n",
      "---- Val Loss: 20.0084\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 37/70 =====\n",
      "Mean grad norm: 0.070862\n",
      "Max grad norm: 0.350717\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000125\n",
      "---- Train Loss: 0.1748\n",
      "---- Val Loss: 20.4016\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 38/70 =====\n",
      "Mean grad norm: 0.061697\n",
      "Max grad norm: 0.390961\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000125\n",
      "---- Train Loss: 0.1727\n",
      "---- Val Loss: 20.1658\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 39/70 =====\n",
      "Mean grad norm: 0.090807\n",
      "Max grad norm: 0.564678\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000125\n",
      "---- Train Loss: 0.1681\n",
      "---- Val Loss: 20.1513\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 40/70 =====\n",
      "Mean grad norm: 0.107882\n",
      "Max grad norm: 0.507925\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000125\n",
      "---- Train Loss: 0.1689\n",
      "---- Val Loss: 20.2224\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 41/70 =====\n",
      "Mean grad norm: 0.090126\n",
      "Max grad norm: 0.711416\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000063\n",
      "---- Train Loss: 0.1698\n",
      "---- Val Loss: 20.3577\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 42/70 =====\n",
      "Mean grad norm: 0.069642\n",
      "Max grad norm: 0.281545\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000063\n",
      "---- Train Loss: 0.1639\n",
      "---- Val Loss: 20.4286\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 43/70 =====\n",
      "Mean grad norm: 0.089410\n",
      "Max grad norm: 0.710925\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000063\n",
      "---- Train Loss: 0.1653\n",
      "---- Val Loss: 20.4022\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 44/70 =====\n",
      "Mean grad norm: 0.074833\n",
      "Max grad norm: 0.591933\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000063\n",
      "---- Train Loss: 0.1642\n",
      "---- Val Loss: 20.6021\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 45/70 =====\n",
      "Mean grad norm: 0.078482\n",
      "Max grad norm: 0.381521\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000031\n",
      "---- Train Loss: 0.1645\n",
      "---- Val Loss: 20.5480\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 46/70 =====\n",
      "Mean grad norm: 0.045610\n",
      "Max grad norm: 0.345805\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000031\n",
      "---- Train Loss: 0.1636\n",
      "---- Val Loss: 20.5094\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 47/70 =====\n",
      "Mean grad norm: 0.051762\n",
      "Max grad norm: 0.439864\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000031\n",
      "---- Train Loss: 0.1652\n",
      "---- Val Loss: 20.6366\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 48/70 =====\n",
      "Mean grad norm: 0.106793\n",
      "Max grad norm: 0.512069\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000031\n",
      "---- Train Loss: 0.1636\n",
      "---- Val Loss: 20.6487\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 49/70 =====\n",
      "Mean grad norm: 0.039991\n",
      "Max grad norm: 0.146475\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000016\n",
      "---- Train Loss: 0.1613\n",
      "---- Val Loss: 20.6424\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 50/70 =====\n",
      "Mean grad norm: 0.047348\n",
      "Max grad norm: 0.200041\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000016\n",
      "---- Train Loss: 0.1617\n",
      "---- Val Loss: 20.7670\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 51/70 =====\n",
      "Mean grad norm: 0.081613\n",
      "Max grad norm: 0.722729\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000016\n",
      "---- Train Loss: 0.1611\n",
      "---- Val Loss: 20.6988\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 52/70 =====\n",
      "Mean grad norm: 0.071443\n",
      "Max grad norm: 0.344762\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000016\n",
      "---- Train Loss: 0.1632\n",
      "---- Val Loss: 20.6626\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 53/70 =====\n",
      "Mean grad norm: 0.040241\n",
      "Max grad norm: 0.180923\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000008\n",
      "---- Train Loss: 0.1598\n",
      "---- Val Loss: 20.5986\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 54/70 =====\n",
      "Mean grad norm: 0.094522\n",
      "Max grad norm: 0.709864\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000008\n",
      "---- Train Loss: 0.1616\n",
      "---- Val Loss: 20.6317\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 55/70 =====\n",
      "Mean grad norm: 0.070291\n",
      "Max grad norm: 0.279256\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000008\n",
      "---- Train Loss: 0.1611\n",
      "---- Val Loss: 20.6290\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m     optimizer.zero_grad(); \n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m     42\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "lst_loss_train = []\n",
    "lst_loss_val = []\n",
    "best_val_loss = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "\n",
    "    epoch_train_loss = 0.0\n",
    "    train_predicts = []\n",
    "\n",
    "    train_tqdm = tqdm(train_dl, desc=f\"Эпоха {epoch+1}/{EPOCHS} [Обучение]\", leave=False)\n",
    "    for batch_ind, batch in enumerate(train_tqdm):\n",
    "        mel_spec, targets, targets_lens, _ = batch\n",
    "        mel_spec, targets, targets_lens = mel_spec.to(DIVICE), targets.to(DIVICE), targets_lens.to(DIVICE)\n",
    "\n",
    "        #===== считатем длинну mel_spec для передачи в CTC loss =====\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predict = model(mel_spec) # (N=batch,T,C)\n",
    "        N = predict.shape[1]\n",
    "        T = predict.shape[0]\n",
    "        predict_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "\n",
    "    #     print(\"Predict shape:\", predict.shape) # [T, N, C]\n",
    "    #     print(\"Labels shape:\", targets.shape)   # [N, max_label_len]\n",
    "    #     print(\"Predict lengths:\", predict_lengths) # [N]\n",
    "    #     print(\"Target lengths:\", targets_lens.reshape(BATCH_SIZE))   # [N]\n",
    "    #     break\n",
    "    # break\n",
    "        try:\n",
    "            loss = loss_func(predict, targets, predict_lengths, targets_lens.reshape(BATCH_SIZE))\n",
    "        except RuntimeError:\n",
    "            print(predict.shape, targets.shape, predict_lengths, targets_lens.reshape(BATCH_SIZE))\n",
    "            continue\n",
    "        # print(loss)\n",
    "        if torch.isnan(loss) or torch.isinf(loss): \n",
    "            print(f\"\\nWarning: In batch-{batch_ind} loss train is NaN/Inf: {loss.item()}\"); \n",
    "            optimizer.zero_grad(); \n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "    train_loss = epoch_train_loss / len(train_dl)\n",
    "\n",
    "    # ======== Валидация ========\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    total_val = 0\n",
    "    val_predicts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_mel_spec, val_labels, val_label_lensin, _ in tqdm(\n",
    "                                                        val_dl, \n",
    "                                                        desc=f\"Эпоха {epoch+1}/{EPOCHS} [Валидация]\", \n",
    "                                                        leave=False):\n",
    "            val_mel_spec, val_labels, val_label_lensin = val_mel_spec.to(DIVICE), val_labels.to(DIVICE), val_label_lensin.to(DIVICE)\n",
    "            val_predict = model(val_mel_spec)\n",
    "\n",
    "            val_N = val_predict.shape[1]\n",
    "            val_T = val_predict.shape[0]\n",
    "            predict_val_lengths = torch.full(size=(val_N,), fill_value=val_T, dtype=torch.long)\n",
    "            val_loss += loss_func(val_predict, val_labels, predict_val_lengths, val_label_lensin).item()\n",
    "\n",
    "    total_val = val_loss / len(val_dl)\n",
    "\n",
    "    lst_loss_train.append(train_loss)\n",
    "    lst_loss_val.append(total_val)\n",
    "\n",
    "    scheduler.step(total_val)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f\"Новая лучшая модель сохранена с val_loss: {val_loss:.4f}\")\n",
    "    \n",
    "\n",
    "    print(f\"\\n===== Эпоха {epoch+1}/{EPOCHS} =====\")\\\n",
    "    #===== Инфо про градиенты=====\n",
    "    grad_norms = [param.grad.norm().item() for param in model.parameters() if param.grad is not None]\n",
    "    if grad_norms:\n",
    "        print(f\"Mean grad norm: {np.mean(grad_norms):.6f}\")\n",
    "        print(f\"Max grad norm: {np.max(grad_norms):.6f}\")\n",
    "        print(f\"Min grad norm: {np.min(grad_norms):.6f}\")\n",
    "    else:\n",
    "        print(\"No gradients computed yet.\")\n",
    "    #===== Инфо про шаг обучения и данные по потерям =====\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Current LR: {current_lr:.6f}\")\n",
    "    print(f\"---- Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"---- Val Loss: {val_loss:.4f}\")\n",
    "    if current_lr <= 1e-6:\n",
    "        print(\"Learning rate достиг минимума 1e-6, остановка обучения\")\n",
    "        break\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca52e14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'MorseNet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1547a2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lst_loss_train)\n",
    "plt.plot(lst_loss_val)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45f65295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN размерность выхода: torch.Size([1, 32, 16, 89])\n",
      "CNN число фичей: 512\n",
      "Проекция из 512 в 512\n",
      "Mean accurasu by The Levenshtein in train is : 0.9320385051013883\n",
      "Mean accurasu by The Levenshtein in validate is : 0.9479961676379779\n"
     ]
    }
   ],
   "source": [
    "def ctc_decoder(logits, int_char_map, blank_label_idx):\n",
    "    preds = []\n",
    "    logits_cpu = logits.cpu() \n",
    "    max_inds = torch.argmax(logits_cpu.detach(), dim=2).t().numpy() # арзмакс по лагитам и преобразование к словарю\n",
    "    \n",
    "    for ind in max_inds:\n",
    "        merged_inds = []\n",
    "        for idx in ind:\n",
    "            if idx != blank_label_idx: \n",
    "                merged_inds.append(idx)\n",
    "        text = \"\".join([int_char_map.get(i, '?') for i in merged_inds])\n",
    "        preds.append(text)\n",
    "\n",
    "    return preds\n",
    "\n",
    "model_load = MorseNet(num_classes=num_classes)\n",
    "model_load.load_state_dict(torch.load('MorseNet.pth'))\n",
    "model_load.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_mess = []\n",
    "    train_predicts = []\n",
    "    for loader in train_dl:\n",
    "        seq, test_target, _, mess = loader\n",
    "        train_mess.extend(mess)\n",
    "\n",
    "        logits = model_load(seq)\n",
    "        predicted_values = ctc_decoder(logits, int_to_char, BLANK_IDX)\n",
    "        train_predicts.extend(predicted_values)\n",
    "\n",
    "    val_mess = []\n",
    "    val_predicts = []\n",
    "    for loader in val_dl:\n",
    "        seq, test_target, _, mess = loader\n",
    "        val_mess.extend(mess)\n",
    "\n",
    "        logits= model_load(seq)\n",
    "        predicted_values = ctc_decoder(logits, int_to_char, BLANK_IDX)\n",
    "        val_predicts.extend(predicted_values)\n",
    "\n",
    "import Levenshtein\n",
    "\n",
    "mean_acc_test = np.mean([Levenshtein.ratio(test_pred, train_mess[ind]) for ind, test_pred in enumerate(train_predicts)])\n",
    "mean_acc_val = np.mean([Levenshtein.ratio(val_pred, val_mess[ind]) for ind, val_pred in enumerate(val_predicts)])\n",
    "\n",
    "\n",
    "print(f\"Mean accurasu by The Levenshtein in train is : {mean_acc_test}\")\n",
    "print(f\"Mean accurasu by The Levenshtein in validate is : {mean_acc_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49ca828",
   "metadata": {},
   "source": [
    "# Сбор sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85e44a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN размерность выхода: torch.Size([1, 32, 16, 89])\n",
      "CNN число фичей: 512\n",
      "Проекция из 512 в 512\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'MorseNet.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m preds\n\u001b[32m     16\u001b[39m model_load = MorseNet(num_classes=num_classes)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m model_load.load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mMorseNet.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     18\u001b[39m model_load.eval()\n\u001b[32m     20\u001b[39m test_ds = MosreDataset(df=sample_data,\n\u001b[32m     21\u001b[39m                         data_patch=DATASET_PATCH,\n\u001b[32m     22\u001b[39m                         char_to_int=char_to_int,\n\u001b[32m     23\u001b[39m                         train=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     24\u001b[39m                         transforms=valid_audio_transforms)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\serialization.py:1479\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1477\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1479\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1480\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1481\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1482\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1483\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1484\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'MorseNet.pth'"
     ]
    }
   ],
   "source": [
    "def ctc_decoder(logits, int_char_map, blank_label_idx):\n",
    "    preds = []\n",
    "    logits_cpu = logits.cpu() \n",
    "    max_inds = torch.argmax(logits_cpu.detach(), dim=2).t().numpy() # арзмакс по лагитам и преобразование к словарю\n",
    "    \n",
    "    for ind in max_inds:\n",
    "        merged_inds = []\n",
    "        for idx in ind:\n",
    "            if idx != blank_label_idx: \n",
    "                merged_inds.append(idx)\n",
    "        text = \"\".join([int_char_map.get(i, '?') for i in merged_inds])\n",
    "        preds.append(text)\n",
    "\n",
    "    return preds\n",
    "\n",
    "model_load = MorseNet(num_classes=num_classes)\n",
    "model_load.load_state_dict(torch.load('MorseNet_0.89.pth'))\n",
    "model_load.eval()\n",
    "\n",
    "test_ds = MosreDataset(df=sample_data,\n",
    "                        data_patch=DATASET_PATCH,\n",
    "                        char_to_int=char_to_int,\n",
    "                        train=False,\n",
    "                        transforms=valid_audio_transforms)\n",
    "\n",
    "\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=20, shuffle=False, collate_fn=my_collate)\n",
    "model_load.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_predicts = []\n",
    "    for loader in test_dl:\n",
    "        seq = loader\n",
    "        logits = model_load(seq)\n",
    "        predicted_values = ctc_decoder(logits, int_to_char, BLANK_IDX)\n",
    "        test_predicts.extend(predicted_values)\n",
    "\n",
    "sample_data.message = test_predicts\n",
    "sample_data.to_csv('sample_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Morse_decoder_V2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
