{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d761d8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN размерность выхода: torch.Size([1, 32, 8, 89])\n",
      "CNN число фичей: 256\n",
      "\n",
      "MorseNet - инициалицация модели. Число обучаемых параметров: 15,724,685\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path as pt\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchaudio import transforms\n",
    "from torchvision.transforms import v2\n",
    "# from Moduls.MosreDataset import MosreDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.nn.parallel import DataParallel\n",
    "from collections import Counter\n",
    "\n",
    "DIVICE = torch.device(\"cuda\")\n",
    "\n",
    "MAIN = pt(os.getcwd())\n",
    "DATASET_PATCH = MAIN / 'morse_dataset'\n",
    "AUDIO_FILES = DATASET_PATCH / 'morse_dataset'\n",
    "\n",
    "# Поятоянные значения выявленные в процессе анализа\n",
    "MORSEALP = \"АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ 1234567890#\"\n",
    "MAX_TIME = 48\n",
    "SAMPLE_RATE = 8000\n",
    "N_MELS = 64\n",
    "N_FFT = 512\n",
    "HOP_LENGTH = 160\n",
    "TOP_DB = 80\n",
    "FREQ_MASK = 15\n",
    "TIME_MASK = 20\n",
    "\n",
    "# Гиперпараметы обучения\n",
    "SEED = 42\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 60\n",
    "LEARNING_RATE = 0.0002 #2e-4\n",
    "WEIGHT_DECAY = 0.00001\n",
    "# int_to_alph = dict(enumerate(MORSEALP))\n",
    "# alph_to_int = {char:enum for enum, char in int_to_alph.items()}\n",
    "\n",
    "#===== Import data =====\n",
    "train_data = pd.read_csv(pt.joinpath(DATASET_PATCH,'train.csv'))\n",
    "test_data = pd.read_csv(pt.joinpath(DATASET_PATCH,'test.csv'))\n",
    "sample_data = pd.read_csv(pt.joinpath(DATASET_PATCH,'sample_submission.csv'))\n",
    "\n",
    "all_chars = Counter(\"\".join(train_data['message']))\n",
    "BLANK_CHAR = \"_\"\n",
    "vocab_list = sorted(all_chars.keys()) + [BLANK_CHAR]\n",
    "num_classes = len(vocab_list)\n",
    "char_to_int = {char: i for i, char in enumerate(vocab_list)}\n",
    "int_to_char = {i: char for i, char in enumerate(vocab_list)}\n",
    "BLANK_IDX = char_to_int[BLANK_CHAR]\n",
    "\n",
    "\n",
    "import audiomentations as am\n",
    "\n",
    "augment = am.Compose([\n",
    "    am.AddGaussianNoise(p=0.3),\n",
    "    am.TimeStretch(p=0.3),\n",
    "    am.PitchShift(p=0.3),\n",
    "    am.LowPassFilter(p=0.3),\n",
    "    am.ClippingDistortion(p=0.2)\n",
    "])\n",
    "class MosreDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Класс для обработки \n",
    "    \"\"\"\n",
    "    def __init__(self, df, data_patch,char_to_int, train=True, transforms=None, prev_chars = 1):\n",
    "        self.df = df\n",
    "        self.is_train = train\n",
    "\n",
    "        self.data_path = data_patch\n",
    "        self.audio_paths = self.data_path / 'morse_dataset'\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.char_to_int = char_to_int\n",
    "        self.prev_chars = prev_chars\n",
    "\n",
    "        if self.is_train:\n",
    "            self.messeges = self.df.message.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #Получение аугментрованых спектрограмм\n",
    "        try:\n",
    "            audio_file = self.audio_paths / self.df.id.values[index]\n",
    "            waveform, sample_rate = torchaudio.load(audio_file)\n",
    "            waveform = waveform\n",
    "            # аунментация аудио\n",
    "            # waveform_np = waveform.numpy().squeeze(0)\n",
    "            # augmented = augment(samples=waveform_np, sample_rate=8000)\n",
    "            # waveform = torch.from_numpy(augmented).unsqueeze(0)\n",
    "\n",
    "            augmented_spectrogram = self.transforms(waveform)\n",
    "\n",
    "            if self.is_train:\n",
    "                message = self.messeges[index]\n",
    "                #Получение списка индексов секта - как требует CTC los\n",
    "                '''\n",
    "                При обработке dataloader labels будут выравниваться по макс длине для выравнивания батча\n",
    "                Т.е. будет padding 0. что в будующем будет пустым значением для ctc loss\n",
    "                '''\n",
    "                target = torch.tensor([self.char_to_int[char] for char in message], dtype=torch.long); \n",
    "                target_len = torch.tensor(len(target), dtype=torch.long)\n",
    "                return augmented_spectrogram, target, target_len, message\n",
    "            else:\n",
    "                return augmented_spectrogram, None, None, None\n",
    "        except Exception as ex:\n",
    "            print(str(ex))\n",
    "    \n",
    "FIRST_FE_COUNT = 16\n",
    "SECOND_FE_COUNT = 32\n",
    "THIRD_FE_COUNT = 32\n",
    "QAD_FE_COUNT = 32\n",
    "PADDING = 'same'\n",
    "MAXPOOL_KERNEL = 2\n",
    "KERTNEL_SIZE = 3\n",
    "NERON_COUNT = 128\n",
    "GRU_HIDEN = 512\n",
    "DROPOUT = 0.4\n",
    "# Start with 4 transforms\n",
    "class MorseNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.net_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, \n",
    "                      out_channels=FIRST_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(FIRST_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((1, 2), (1, 2)), # [batch, FIRST_FE_COUNT = 16, 64, 960]\n",
    "\n",
    "            nn.Conv2d(in_channels=FIRST_FE_COUNT, \n",
    "                      out_channels=SECOND_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(SECOND_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)), # [batch, SECOND_FE_COUNT = 32, 32, 480]\n",
    "\n",
    "            nn.Conv2d(in_channels=SECOND_FE_COUNT, \n",
    "                      out_channels=THIRD_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(THIRD_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((2, 2), (2, 2)), # [batch, THIRD_FE_COUNT = 32, 16, 240]\n",
    "\n",
    "            nn.Conv2d(in_channels=THIRD_FE_COUNT, \n",
    "                      out_channels=QAD_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(QAD_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)) # [batch=32, QAD_FE_COUNT = 32, 8, 80](что юы сохраниить большще признаков по горизонтали)\n",
    "        )\n",
    "        with torch.no_grad(): \n",
    "            dummy_input = torch.randn(1, 1, N_MELS, 356); \n",
    "            cnn_out = self.net_conv(dummy_input); \n",
    "            self.cnn_output_features = cnn_out.shape[1] * cnn_out.shape[2]\n",
    "\n",
    "        print(f\"CNN размерность выхода: {cnn_out.shape}\"); \n",
    "        print(f\"CNN число фичей: {self.cnn_output_features}\")\n",
    "        \n",
    "        # Добавлен лоейный слой и функция активации. что бы созратить разменрость\n",
    "        self.layer1 = nn.Linear(self.cnn_output_features, N_MELS*2); \n",
    "        self.gelu = nn.GELU()\n",
    "        self.rnn = nn.GRU(\n",
    "                input_size=N_MELS*2,\n",
    "                hidden_size=GRU_HIDEN,\n",
    "                num_layers=3,\n",
    "                bidirectional=True,\n",
    "                dropout=DROPOUT,\n",
    "                batch_first=True \n",
    "            )\n",
    "        \n",
    "        self.embed_dim = GRU_HIDEN * 2\n",
    "        # self.attention = nn.Sequential(\n",
    "        #     nn.Linear(self.embed_dim, 128),\n",
    "        #     nn.Tanh(),\n",
    "        #     nn.Linear(128, 1),\n",
    "        #     nn.Softmax(dim=1) # по временной оси\n",
    "        # )\n",
    "        self.attention = nn.MultiheadAttention(self.embed_dim, \n",
    "                                               num_heads=8, \n",
    "                                               dropout=DROPOUT, \n",
    "                                               batch_first=True)\n",
    "        self.layer_norm = nn.LayerNorm(self.embed_dim)      \n",
    "        self.dropout = nn.Dropout(DROPOUT)   \n",
    "        self.layer2 = nn.Linear(self.embed_dim, num_classes)       \n",
    "     \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net_conv(x)\n",
    "\n",
    "        batch, channels, reduced_mels, reduced_time = x.shape\n",
    "        x = x.permute(0, 3, 1, 2)  # [batch, time, channels, mels]\n",
    "\n",
    "        # # В частности, каждый вектор признаков в последовательности признаков генерируется \n",
    "        # # слева направо на картах признаков. Это означает, что i-й вектор признаков представляет \n",
    "        # # собой объединение столбцов всех карт. \n",
    "        # # Таким образом, форма тензора может быть изменена, например, на (размер_пакета, 80, 256)\n",
    "        \n",
    "        x = x.reshape(batch, reduced_time, -1)  # to GRU [batch=64, seq_len=100, features/hiden_dim=256]\n",
    "        x = self.layer1(x)\n",
    "        x = self.gelu(x)\n",
    "\n",
    "        self.rnn.flatten_parameters()\n",
    "\n",
    "        x = self.rnn(x) # [batch=32, seq_len=89, features/hiden_dim=256 * 4] 4-тк 2 слоя RNN\n",
    "        x, _ = x # берем информацию со всез состояний\n",
    "        \n",
    "        # #attention\n",
    "        attn, _ = self.attention(x,x,x)\n",
    "        x = x + attn\n",
    "        # attn_weights = self.attention(x)\n",
    "        # x = x * attn_weights\n",
    "        x = self.layer_norm(x)\n",
    "        attn = self.dropout(x)\n",
    "        x = self.layer2(x) # logits - [batch, sequence, num_classes] \n",
    "        x = nn.functional.log_softmax(x.permute(1,0,2), dim=2) # pertime так как CTC loss требует на взод (sequence/T,batch/N,num_classes/C)\n",
    "        '''\n",
    "        по одному прогнозу для каждого из признаков в последовательности, \n",
    "        в итоге получается 89 прогнозов символов для каждой секунды звука.\n",
    "        '''\n",
    "        return x\n",
    "    \n",
    "\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS),\n",
    "    transforms.AmplitudeToDB(top_db=TOP_DB),\n",
    "    transforms.FrequencyMasking(freq_mask_param=FREQ_MASK),\n",
    "    transforms.TimeMasking(time_mask_param=TIME_MASK),\n",
    "    # v2.RandomCrop((N_MELS, 1920)) # Обрезает последний кадр спектрограммы, в идеале надобы считать а не прописывать число\n",
    "    ) # заметка - Данные трансформации не создают довых обучаемых параметров. Но есть и те что создают. В будущем это стоит учитывать\n",
    "\n",
    "valid_audio_transforms = nn.Sequential(\n",
    "    transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS),\n",
    "    transforms.AmplitudeToDB(top_db=TOP_DB),\n",
    "    # v2.CenterCrop((N_MELS, 1920)) \n",
    "    )\n",
    "\n",
    "train_dataframe, val_dataframe = train_test_split(train_data, test_size=0.15, random_state=SEED)\n",
    "\n",
    "train_ds = MosreDataset(df=train_dataframe,\n",
    "                        data_patch=DATASET_PATCH,\n",
    "                        char_to_int=char_to_int,\n",
    "                        train=True,\n",
    "                        transforms=train_audio_transforms)\n",
    "\n",
    "val_ds = MosreDataset(df=val_dataframe,\n",
    "                        data_patch=DATASET_PATCH,\n",
    "                        char_to_int=char_to_int,\n",
    "                        train=True,\n",
    "                        transforms=valid_audio_transforms)\n",
    "\n",
    "\n",
    "def my_collate(batch):\n",
    "    spectrograms = [item[0].squeeze(0) for item in batch]\n",
    "    # Падинг спектрограмм по максимальной длине\n",
    "    spectrograms_permuted = [s.permute(1, 0) for s in spectrograms]\n",
    "    spectrograms_padded = nn.utils.rnn.pad_sequence(spectrograms_permuted, batch_first=True, padding_value=0.0)\n",
    "    spectrograms_padded = spectrograms_padded.permute(0, 2, 1).unsqueeze(1)\n",
    "\n",
    "    if batch[0][3] is not None:\n",
    "        target = torch.nn.utils.rnn.pad_sequence(\n",
    "                                                [item[1] for item in batch], \n",
    "                                                batch_first=True, \n",
    "                                                padding_value=BLANK_IDX)# выравнивает последовательность до макс \n",
    "                                                                        # длины в батче заполняя пропуски нулем\n",
    "        label_len = torch.stack([item[2] for item in batch])\n",
    "        msg = [item[3] for item in batch]\n",
    "        \n",
    "        return [spectrograms_padded, target, label_len, msg]\n",
    "    else:\n",
    "        return spectrograms_padded\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, \n",
    "                                       batch_size=BATCH_SIZE, \n",
    "                                       shuffle=True, \n",
    "                                       collate_fn=my_collate, \n",
    "                                       drop_last=True)\n",
    "\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, \n",
    "                                     batch_size=BATCH_SIZE, \n",
    "                                     shuffle=True, \n",
    "                                     collate_fn=my_collate, \n",
    "                                     drop_last=True)\n",
    "\n",
    "test, test_target, _, mess = next(iter(train_dl))\n",
    "test, test_target= test.to(DIVICE), test_target.to(DIVICE)\n",
    "\n",
    "test_val, val_target, __, val_mess = next(iter(val_dl))\n",
    "test_val, val_target = test_val.to(DIVICE), val_target.to(DIVICE)\n",
    "# test.shape \n",
    "\n",
    "#===== начало обучения =====\n",
    "model = MorseNet(num_classes=num_classes).to(DIVICE)\n",
    "# model = DataParallel(model)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=0.002)  # Было 0.002\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "loss_func = nn.CTCLoss(blank=BLANK_IDX, reduction='mean', zero_infinity=True).to(DIVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nMorseNet - инициалицация модели. Число обучаемых параметров: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa7f726",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "422ea6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 1/60 =====\n",
      "Mean grad norm: 0.021516\n",
      "Max grad norm: 0.923035\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 14.2128\n",
      "---- Val Loss: 5.3321\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 2/60 =====\n",
      "Mean grad norm: 0.023975\n",
      "Max grad norm: 1.020603\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 4.0248\n",
      "---- Val Loss: 5.3278\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 3/60 =====\n",
      "Mean grad norm: 0.027749\n",
      "Max grad norm: 1.196522\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 4.0254\n",
      "---- Val Loss: 5.1970\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 4/60 =====\n",
      "Mean grad norm: 0.025047\n",
      "Max grad norm: 1.085787\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 4.0257\n",
      "---- Val Loss: 5.2284\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 5/60 =====\n",
      "Mean grad norm: 0.023524\n",
      "Max grad norm: 1.017452\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 4.0267\n",
      "---- Val Loss: 5.1977\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 6/60 =====\n",
      "Mean grad norm: 0.021222\n",
      "Max grad norm: 0.918199\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 4.0263\n",
      "---- Val Loss: 5.2324\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 7/60 =====\n",
      "Mean grad norm: 0.019060\n",
      "Max grad norm: 0.824963\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 4.0259\n",
      "---- Val Loss: 5.0878\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 8/60 =====\n",
      "Mean grad norm: 0.017960\n",
      "Max grad norm: 0.769938\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 4.0268\n",
      "---- Val Loss: 4.9709\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 9/60 =====\n",
      "Mean grad norm: 0.021642\n",
      "Max grad norm: 0.898893\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 4.0268\n",
      "---- Val Loss: 4.9780\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 10/60 =====\n",
      "Mean grad norm: 0.021770\n",
      "Max grad norm: 0.867842\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 4.0251\n",
      "---- Val Loss: 5.1414\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 11/60 =====\n",
      "Mean grad norm: 0.016407\n",
      "Max grad norm: 0.659908\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.002000\n",
      "---- Train Loss: 4.0256\n",
      "---- Val Loss: 5.0469\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 12/60 =====\n",
      "Mean grad norm: 0.017730\n",
      "Max grad norm: 0.735499\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.001000\n",
      "---- Train Loss: 4.0249\n",
      "---- Val Loss: 5.0097\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 13/60 =====\n",
      "Mean grad norm: 0.015903\n",
      "Max grad norm: 0.645269\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.001000\n",
      "---- Train Loss: 4.0213\n",
      "---- Val Loss: 4.9730\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 14/60 =====\n",
      "Mean grad norm: 0.017795\n",
      "Max grad norm: 0.693989\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.001000\n",
      "---- Train Loss: 4.0216\n",
      "---- Val Loss: 5.0309\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 15/60 =====\n",
      "Mean grad norm: 0.015720\n",
      "Max grad norm: 0.670751\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.001000\n",
      "---- Train Loss: 4.0218\n",
      "---- Val Loss: 4.9666\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 16/60 =====\n",
      "Mean grad norm: 0.013870\n",
      "Max grad norm: 0.601535\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.001000\n",
      "---- Train Loss: 4.0216\n",
      "---- Val Loss: 4.9680\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 17/60 =====\n",
      "Mean grad norm: 0.025419\n",
      "Max grad norm: 0.917782\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.001000\n",
      "---- Train Loss: 4.0219\n",
      "---- Val Loss: 5.0735\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 18/60 =====\n",
      "Mean grad norm: 0.014543\n",
      "Max grad norm: 0.558971\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.001000\n",
      "---- Train Loss: 4.0220\n",
      "---- Val Loss: 4.8937\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 19/60 =====\n",
      "Mean grad norm: 0.022881\n",
      "Max grad norm: 0.819410\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.001000\n",
      "---- Train Loss: 4.0220\n",
      "---- Val Loss: 5.0011\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 20/60 =====\n",
      "Mean grad norm: 0.021370\n",
      "Max grad norm: 0.763163\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.001000\n",
      "---- Train Loss: 4.0216\n",
      "---- Val Loss: 4.9439\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 21/60 =====\n",
      "Mean grad norm: 0.039222\n",
      "Max grad norm: 1.320042\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.001000\n",
      "---- Train Loss: 4.0214\n",
      "---- Val Loss: 4.9899\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 22/60 =====\n",
      "Mean grad norm: 0.016503\n",
      "Max grad norm: 0.603526\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000500\n",
      "---- Train Loss: 4.0212\n",
      "---- Val Loss: 4.9810\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 23/60 =====\n",
      "Mean grad norm: 0.016763\n",
      "Max grad norm: 0.602980\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000500\n",
      "---- Train Loss: 4.0195\n",
      "---- Val Loss: 4.9870\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 24/60 =====\n",
      "Mean grad norm: 0.014014\n",
      "Max grad norm: 0.548236\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000500\n",
      "---- Train Loss: 4.0193\n",
      "---- Val Loss: 4.9663\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 25/60 =====\n",
      "Mean grad norm: 0.011633\n",
      "Max grad norm: 0.486587\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000500\n",
      "---- Train Loss: 4.0194\n",
      "---- Val Loss: 4.9733\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 26/60 =====\n",
      "Mean grad norm: 0.013773\n",
      "Max grad norm: 0.555091\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000250\n",
      "---- Train Loss: 4.0194\n",
      "---- Val Loss: 5.0013\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 27/60 =====\n",
      "Mean grad norm: 0.014945\n",
      "Max grad norm: 0.566712\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000250\n",
      "---- Train Loss: 4.0185\n",
      "---- Val Loss: 4.9540\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 28/60 =====\n",
      "Mean grad norm: 0.013580\n",
      "Max grad norm: 0.573443\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000250\n",
      "---- Train Loss: 4.0185\n",
      "---- Val Loss: 4.9799\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 29/60 =====\n",
      "Mean grad norm: 0.021717\n",
      "Max grad norm: 0.757303\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000250\n",
      "---- Train Loss: 4.0185\n",
      "---- Val Loss: 4.9707\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 30/60 =====\n",
      "Mean grad norm: 0.016908\n",
      "Max grad norm: 0.592697\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000125\n",
      "---- Train Loss: 4.0184\n",
      "---- Val Loss: 4.9291\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 31/60 =====\n",
      "Mean grad norm: 0.020773\n",
      "Max grad norm: 0.719074\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000125\n",
      "---- Train Loss: 4.0179\n",
      "---- Val Loss: 5.0016\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 32/60 =====\n",
      "Mean grad norm: 0.018396\n",
      "Max grad norm: 0.648235\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000125\n",
      "---- Train Loss: 4.0181\n",
      "---- Val Loss: 4.9740\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 33/60 =====\n",
      "Mean grad norm: 0.009093\n",
      "Max grad norm: 0.381957\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000125\n",
      "---- Train Loss: 4.0178\n",
      "---- Val Loss: 4.9678\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 34/60 =====\n",
      "Mean grad norm: 0.010822\n",
      "Max grad norm: 0.475893\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000063\n",
      "---- Train Loss: 4.0179\n",
      "---- Val Loss: 5.0049\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 35/60 =====\n",
      "Mean grad norm: 0.010403\n",
      "Max grad norm: 0.459420\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000063\n",
      "---- Train Loss: 4.0176\n",
      "---- Val Loss: 5.0017\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 36/60 =====\n",
      "Mean grad norm: 0.013269\n",
      "Max grad norm: 0.505896\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000063\n",
      "---- Train Loss: 4.0176\n",
      "---- Val Loss: 4.9736\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 37/60 =====\n",
      "Mean grad norm: 0.013327\n",
      "Max grad norm: 0.542821\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000063\n",
      "---- Train Loss: 4.0176\n",
      "---- Val Loss: 5.0090\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 38/60 =====\n",
      "Mean grad norm: 0.016507\n",
      "Max grad norm: 0.596852\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000031\n",
      "---- Train Loss: 4.0176\n",
      "---- Val Loss: 5.0112\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 39/60 =====\n",
      "Mean grad norm: 0.023068\n",
      "Max grad norm: 0.773851\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000031\n",
      "---- Train Loss: 4.0183\n",
      "---- Val Loss: 4.9885\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 40/60 =====\n",
      "Mean grad norm: 0.014355\n",
      "Max grad norm: 0.516824\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000031\n",
      "---- Train Loss: 4.0172\n",
      "---- Val Loss: 5.0024\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 41/60 =====\n",
      "Mean grad norm: 0.011352\n",
      "Max grad norm: 0.461449\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000031\n",
      "---- Train Loss: 4.0171\n",
      "---- Val Loss: 5.0038\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 42/60 =====\n",
      "Mean grad norm: 0.018696\n",
      "Max grad norm: 0.615194\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000016\n",
      "---- Train Loss: 4.0168\n",
      "---- Val Loss: 4.9955\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 43/60 =====\n",
      "Mean grad norm: 0.015717\n",
      "Max grad norm: 0.607519\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000016\n",
      "---- Train Loss: 4.0165\n",
      "---- Val Loss: 5.0075\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 44/60 =====\n",
      "Mean grad norm: 0.013909\n",
      "Max grad norm: 0.554141\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000016\n",
      "---- Train Loss: 4.0163\n",
      "---- Val Loss: 5.0304\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 45/60 =====\n",
      "Mean grad norm: 0.020881\n",
      "Max grad norm: 0.674180\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000016\n",
      "---- Train Loss: 4.0163\n",
      "---- Val Loss: 5.0137\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 46/60 =====\n",
      "Mean grad norm: 0.015123\n",
      "Max grad norm: 0.551024\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000008\n",
      "---- Train Loss: 4.0161\n",
      "---- Val Loss: 5.0156\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 47/60 =====\n",
      "Mean grad norm: 0.013187\n",
      "Max grad norm: 0.517740\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000008\n",
      "---- Train Loss: 4.0159\n",
      "---- Val Loss: 5.0245\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 48/60 =====\n",
      "Mean grad norm: 0.016902\n",
      "Max grad norm: 0.563799\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000008\n",
      "---- Train Loss: 4.0158\n",
      "---- Val Loss: 5.0027\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 49/60 =====\n",
      "Mean grad norm: 0.011389\n",
      "Max grad norm: 0.400723\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000008\n",
      "---- Train Loss: 4.0156\n",
      "---- Val Loss: 5.0207\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 50/60 =====\n",
      "Mean grad norm: 0.013830\n",
      "Max grad norm: 0.484259\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000004\n",
      "---- Train Loss: 4.0154\n",
      "---- Val Loss: 4.9994\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 51/60 =====\n",
      "Mean grad norm: 0.016093\n",
      "Max grad norm: 0.480407\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000004\n",
      "---- Train Loss: 4.0152\n",
      "---- Val Loss: 5.0089\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 52/60 =====\n",
      "Mean grad norm: 0.011835\n",
      "Max grad norm: 0.447697\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000004\n",
      "---- Train Loss: 4.0147\n",
      "---- Val Loss: 5.0233\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 53/60 =====\n",
      "Mean grad norm: 0.023373\n",
      "Max grad norm: 0.713762\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000004\n",
      "---- Train Loss: 4.0145\n",
      "---- Val Loss: 5.0052\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 54/60 =====\n",
      "Mean grad norm: 0.016256\n",
      "Max grad norm: 0.552415\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000002\n",
      "---- Train Loss: 4.0143\n",
      "---- Val Loss: 5.0225\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 55/60 =====\n",
      "Mean grad norm: 0.013108\n",
      "Max grad norm: 0.470548\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000002\n",
      "---- Train Loss: 4.0141\n",
      "---- Val Loss: 5.0171\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 56/60 =====\n",
      "Mean grad norm: 0.016860\n",
      "Max grad norm: 0.547635\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000002\n",
      "---- Train Loss: 4.0139\n",
      "---- Val Loss: 5.0138\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 57/60 =====\n",
      "Mean grad norm: 0.021793\n",
      "Max grad norm: 0.612158\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000002\n",
      "---- Train Loss: 4.0136\n",
      "---- Val Loss: 5.0165\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 58/60 =====\n",
      "Mean grad norm: 0.022176\n",
      "Max grad norm: 0.615557\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000001\n",
      "---- Train Loss: 4.0135\n",
      "---- Val Loss: 5.0224\n",
      "Learning rate достиг минимума 1e-6, остановка обучения\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "lst_loss_train = []\n",
    "lst_loss_val = []\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "\n",
    "    epoch_train_loss = 0.0\n",
    "    train_predicts = []\n",
    "\n",
    "    train_tqdm = tqdm(train_dl, desc=f\"Эпоха {epoch+1}/{EPOCHS} [Обучение]\", leave=False)\n",
    "    for batch_ind, batch in enumerate(train_tqdm):\n",
    "        mel_spec, targets, targets_lens, _ = batch\n",
    "        mel_spec, targets, targets_lens = mel_spec.to(DIVICE), targets.to(DIVICE), targets_lens.to(DIVICE)\n",
    "\n",
    "        #===== считатем длинну mel_spec для передачи в CTC loss =====\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predict = model(mel_spec) # (N=batch,T,C)\n",
    "        N = predict.shape[1]\n",
    "        T = predict.shape[0]\n",
    "        predict_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "\n",
    "    #     print(\"Predict shape:\", predict.shape) # [T, N, C]\n",
    "    #     print(\"Labels shape:\", targets.shape)   # [N, max_label_len]\n",
    "    #     print(\"Predict lengths:\", predict_lengths) # [N]\n",
    "    #     print(\"Target lengths:\", targets_lens.reshape(BATCH_SIZE))   # [N]\n",
    "    #     break\n",
    "    # break\n",
    "        try:\n",
    "            loss = loss_func(predict, targets, predict_lengths, targets_lens.reshape(BATCH_SIZE))\n",
    "        except RuntimeError:\n",
    "            print(predict.shape, targets.shape, predict_lengths, targets_lens.reshape(BATCH_SIZE))\n",
    "            continue\n",
    "        # print(loss)\n",
    "        if torch.isnan(loss) or torch.isinf(loss): \n",
    "            print(f\"\\nWarning: In batch-{batch_ind} loss train is NaN/Inf: {loss.item()}\"); \n",
    "            optimizer.zero_grad(); \n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "        # nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "    train_loss = epoch_train_loss / len(train_dl)\n",
    "\n",
    "    # ======== Валидация ========\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    total_val = 0\n",
    "    val_predicts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_mel_spec, val_labels, val_label_lensin, _ in tqdm(\n",
    "                                                        val_dl, \n",
    "                                                        desc=f\"Эпоха {epoch+1}/{EPOCHS} [Валидация]\", \n",
    "                                                        leave=False):\n",
    "            val_mel_spec, val_labels, val_label_lensin = val_mel_spec.to(DIVICE), val_labels.to(DIVICE), val_label_lensin.to(DIVICE)\n",
    "            val_predict = model(val_mel_spec)\n",
    "\n",
    "            val_N = val_predict.shape[1]\n",
    "            val_T = val_predict.shape[0]\n",
    "            predict_val_lengths = torch.full(size=(val_N,), fill_value=val_T, dtype=torch.long)\n",
    "            val_loss += loss_func(val_predict, val_labels, predict_val_lengths, val_label_lensin).item()\n",
    "\n",
    "    total_val = val_loss / len(val_dl)\n",
    "\n",
    "    lst_loss_train.append(train_loss)\n",
    "    lst_loss_val.append(total_val)\n",
    "\n",
    "    scheduler.step(total_val)\n",
    "\n",
    "    print(f\"\\n===== Эпоха {epoch+1}/{EPOCHS} =====\")\\\n",
    "    #===== Инфо про градиенты=====\n",
    "    grad_norms = [param.grad.norm().item() for param in model.parameters() if param.grad is not None]\n",
    "    if grad_norms:\n",
    "        print(f\"Mean grad norm: {np.mean(grad_norms):.6f}\")\n",
    "        print(f\"Max grad norm: {np.max(grad_norms):.6f}\")\n",
    "        print(f\"Min grad norm: {np.min(grad_norms):.6f}\")\n",
    "    else:\n",
    "        print(\"No gradients computed yet.\")\n",
    "    #===== Инфо про шаг обучения и данные по потерям =====\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Current LR: {current_lr:.6f}\")\n",
    "    print(f\"---- Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"---- Val Loss: {total_val:.4f}\")\n",
    "    if current_lr <= 1e-6:\n",
    "        print(\"Learning rate достиг минимума 1e-6, остановка обучения\")\n",
    "        break\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca52e14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'MorseNet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1547a2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lst_loss_train)\n",
    "plt.plot(lst_loss_val)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45f65295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN размерность выхода: torch.Size([1, 32, 8, 89])\n",
      "CNN число фичей: 256\n",
      "Mean accurasu by The Levenshtein in train is : 0.0\n",
      "Mean accurasu by The Levenshtein in validate is : 0.0\n"
     ]
    }
   ],
   "source": [
    "def ctc_decoder(logits, int_char_map, blank_label_idx):\n",
    "    preds = []\n",
    "    logits_cpu = logits.cpu() \n",
    "    max_inds = torch.argmax(logits_cpu.detach(), dim=2).t().numpy() # арзмакс по лагитам и преобразование к словарю\n",
    "    \n",
    "    for ind in max_inds:\n",
    "        merged_inds = []\n",
    "        for idx in ind:\n",
    "            if idx != blank_label_idx: \n",
    "                merged_inds.append(idx)\n",
    "        text = \"\".join([int_char_map.get(i, '?') for i in merged_inds])\n",
    "        preds.append(text)\n",
    "\n",
    "    return preds\n",
    "\n",
    "model_load = MorseNet(num_classes=num_classes)\n",
    "model_load.load_state_dict(torch.load('MorseNet.pth'))\n",
    "model_load.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_mess = []\n",
    "    train_predicts = []\n",
    "    for loader in train_dl:\n",
    "        seq, test_target, _, mess = loader\n",
    "        train_mess.extend(mess)\n",
    "\n",
    "        logits = model_load(seq)\n",
    "        predicted_values = ctc_decoder(logits, int_to_char, BLANK_IDX)\n",
    "        train_predicts.extend(predicted_values)\n",
    "\n",
    "    val_mess = []\n",
    "    val_predicts = []\n",
    "    for loader in val_dl:\n",
    "        seq, test_target, _, mess = loader\n",
    "        val_mess.extend(mess)\n",
    "\n",
    "        logits= model_load(seq)\n",
    "        predicted_values = ctc_decoder(logits, int_to_char, BLANK_IDX)\n",
    "        val_predicts.extend(predicted_values)\n",
    "\n",
    "import Levenshtein\n",
    "\n",
    "mean_acc_test = np.mean([Levenshtein.ratio(test_pred, train_mess[ind]) for ind, test_pred in enumerate(train_predicts)])\n",
    "mean_acc_val = np.mean([Levenshtein.ratio(val_pred, val_mess[ind]) for ind, val_pred in enumerate(val_predicts)])\n",
    "\n",
    "\n",
    "print(f\"Mean accurasu by The Levenshtein in train is : {mean_acc_test}\")\n",
    "print(f\"Mean accurasu by The Levenshtein in validate is : {mean_acc_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49ca828",
   "metadata": {},
   "source": [
    "# Сбор sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c85e44a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN размерность выхода: torch.Size([1, 32, 8, 89])\n",
      "CNN число фичей: 256\n"
     ]
    }
   ],
   "source": [
    "def ctc_decoder(logits, int_char_map, blank_label_idx):\n",
    "    preds = []\n",
    "    logits_cpu = logits.cpu() \n",
    "    max_inds = torch.argmax(logits_cpu.detach(), dim=2).t().numpy() # арзмакс по лагитам и преобразование к словарю\n",
    "    \n",
    "    for ind in max_inds:\n",
    "        merged_inds = []\n",
    "        for idx in ind:\n",
    "            if idx != blank_label_idx: \n",
    "                merged_inds.append(idx)\n",
    "        text = \"\".join([int_char_map.get(i, '?') for i in merged_inds])\n",
    "        preds.append(text)\n",
    "\n",
    "    return preds\n",
    "\n",
    "model_load = MorseNet(num_classes=num_classes)\n",
    "model_load.load_state_dict(torch.load('MorseNet.pth'))\n",
    "model_load.eval()\n",
    "\n",
    "test_ds = MosreDataset(df=sample_data,\n",
    "                        data_patch=DATASET_PATCH,\n",
    "                        char_to_int=char_to_int,\n",
    "                        train=False,\n",
    "                        transforms=valid_audio_transforms)\n",
    "\n",
    "\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=20, shuffle=False, collate_fn=my_collate)\n",
    "model_load.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_predicts = []\n",
    "    for loader in test_dl:\n",
    "        seq = loader\n",
    "        logits = model_load(seq)\n",
    "        predicted_values = ctc_decoder(logits, int_to_char, BLANK_IDX)\n",
    "        test_predicts.extend(predicted_values)\n",
    "\n",
    "sample_data.message = test_predicts\n",
    "sample_data.to_csv('sample_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bea2e22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predicts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Morse_decoder_V2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
