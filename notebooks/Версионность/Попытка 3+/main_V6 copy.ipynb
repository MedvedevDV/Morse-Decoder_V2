{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d761d8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n",
      "Compose.__call__() missing 1 required positional argument: 'sample_rate'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 303\u001b[39m\n\u001b[32m    300\u001b[39m train_dl = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn=my_collate, drop_last=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    301\u001b[39m val_dl = torch.utils.data.DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn=my_collate, drop_last=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m test, test_target, _, mess = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    304\u001b[39m test, test_target= test.to(DIVICE), test_target.to(DIVICE)\n\u001b[32m    306\u001b[39m test_val, val_target, __, val_mess = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(val_dl))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 281\u001b[39m, in \u001b[36mmy_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmy_collate\u001b[39m(batch):\n\u001b[32m    276\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[33;03m    При обработке dataloader labels будут \u001b[39;00m\n\u001b[32m    278\u001b[39m \u001b[33;03m    выравниваться по макс длине для выравнивания батча\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[33;03m    Т.е. будет padding . что в будующем будет пустым значением для ctc loss\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[33;03m    '''\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     spectrograms = [\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m.squeeze(\u001b[32m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[32m    282\u001b[39m     \u001b[38;5;66;03m# Падинг спектрограмм по максимальной длине\u001b[39;00m\n\u001b[32m    283\u001b[39m     spectrograms_permuted = [s.permute(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m spectrograms]\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path as pt\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchaudio import transforms\n",
    "from torchvision.transforms import v2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.nn.parallel import DataParallel\n",
    "from collections import Counter\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, Shift, Gain, PolarityInversion\n",
    "\n",
    "DIVICE = torch.device(\"cuda\")\n",
    "\n",
    "MAIN = pt(os.getcwd())\n",
    "DATASET_PATCH = MAIN / 'morse_dataset'\n",
    "AUDIO_FILES = DATASET_PATCH / 'morse_dataset'\n",
    "\n",
    "# Поятоянные значения выявленные в процессе анализа\n",
    "MORSEALP = \"АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ 1234567890#\"\n",
    "MAX_TIME = 48\n",
    "SAMPLE_RATE = 8000\n",
    "N_MELS = 64\n",
    "N_FFT = 512\n",
    "HOP_LENGTH = 160\n",
    "TOP_DB = 80\n",
    "FREQ_MASK = 20\n",
    "TIME_MASK = 30\n",
    "\n",
    "# Гиперпараметы обучения\n",
    "SEED = 42\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.0002 #2e-4\n",
    "WEIGHT_DECAY = 0.00001\n",
    "\n",
    "#===== Import data =====\n",
    "train_data = pd.read_csv(pt.joinpath(DATASET_PATCH,'train.csv'))\n",
    "test_data = pd.read_csv(pt.joinpath(DATASET_PATCH,'test.csv'))\n",
    "sample_data = pd.read_csv(pt.joinpath(DATASET_PATCH,'sample_submission.csv'))\n",
    "\n",
    "all_chars = Counter(\"\".join(train_data['message']))\n",
    "BLANK_CHAR = \"_\"\n",
    "vocab_list = sorted(all_chars.keys()) + [BLANK_CHAR]\n",
    "num_classes = len(vocab_list)\n",
    "char_to_int = {char: i for i, char in enumerate(vocab_list)}\n",
    "int_to_char = {i: char for i, char in enumerate(vocab_list)}\n",
    "BLANK_IDX = char_to_int[BLANK_CHAR]\n",
    "\n",
    "class MosreDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Класс для обработки \n",
    "    \"\"\"\n",
    "    def __init__(self, df, data_patch,char_to_int, train=True, transforms=None, prev_chars = 1):\n",
    "        self.df = df\n",
    "        self.is_train = train\n",
    "\n",
    "        self.data_path = data_patch\n",
    "        self.audio_paths = self.data_path / 'morse_dataset'\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.char_to_int = char_to_int\n",
    "        self.prev_chars = prev_chars\n",
    "\n",
    "        if self.is_train:\n",
    "            self.messeges = self.df.message.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #Получение аугментрованых спектрограмм\n",
    "        try:\n",
    "            audio_file = self.audio_paths / self.df.id.values[index]\n",
    "            waveform, sample_rate = torchaudio.load(audio_file)\n",
    "            waveform = TRAIN_AUGMENT(waveform)\n",
    "            augmented_spectrogram = self.transforms(waveform)\n",
    "\n",
    "            if self.is_train:\n",
    "                message = self.messeges[index]\n",
    "                #Получение списка индексов секта - как требует CTC los\n",
    "                target = torch.tensor([self.char_to_int[char] for char in message], dtype=torch.long); \n",
    "                target_len = torch.tensor(len(target), dtype=torch.long)\n",
    "                return augmented_spectrogram, target, target_len, message\n",
    "            else:\n",
    "                return augmented_spectrogram, None, None, None\n",
    "        except Exception as ex:\n",
    "            print(str(ex))\n",
    "        \n",
    "    def change_time(self, audio_file, max_len = 384000):\n",
    "        waveform, sample_rate = torchaudio.load(audio_file)\n",
    "        cahanal, sig_len = waveform.shape\n",
    "\n",
    "        if sig_len < max_len:\n",
    "            pad_len = torch.zeros(max_len - sig_len).unsqueeze(0)\n",
    "            waveform = torch.cat([waveform, pad_len], dim=1)\n",
    "\n",
    "        return waveform\n",
    "    \n",
    "FIRST_FE_COUNT = 16\n",
    "SECOND_FE_COUNT = 32\n",
    "THIRD_FE_COUNT = 32\n",
    "QAD_FE_COUNT = 32\n",
    "PADDING = 1\n",
    "MAXPOOL_KERNEL = 2\n",
    "KERTNEL_SIZE = 3\n",
    "NERON_COUNT = 128\n",
    "GRU_HIDEN = 256\n",
    "\n",
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, enc_dim, dec_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.query = nn.Parameter(torch.randn(dec_dim))  # learnable query vector\n",
    "        self.attn = nn.Linear(enc_dim + dec_dim, dec_dim)\n",
    "        self.v = nn.Parameter(torch.rand(dec_dim))\n",
    "        self.proj = nn.Linear(enc_dim, num_classes)\n",
    "\n",
    "    def forward(self, enc_outputs):\n",
    "        # enc_outputs: [B, T, enc_dim]\n",
    "        B, T, D = enc_outputs.size()\n",
    "        queries = self.query.unsqueeze(0).expand(B, -1)  # [B, D_dec]\n",
    "        scores = torch.tanh(self.attn(torch.cat([\n",
    "            enc_outputs,\n",
    "            queries.unsqueeze(1).expand(-1, T, -1)\n",
    "        ], dim=-1)))  # [B, T, dec_dim]\n",
    "        scores = torch.matmul(scores, self.v)  # [B, T]\n",
    "        attn_weights = F.softmax(scores, dim=1).unsqueeze(-1)  # [B, T,1]\n",
    "        context = (enc_outputs * attn_weights).sum(dim=1)  # [B, enc_dim]\n",
    "        logits = self.proj(context)  # [B, num_classes]\n",
    "        # replicate logits over time for CTC\n",
    "        return logits.unsqueeze(1).expand(-1, T, -1).permute(1,0,2)  # [T, B, C]\n",
    "    \n",
    "class MorseNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        chan1 = FIRST_FE_COUNT\n",
    "        chan2 = chan1 * 2\n",
    "        self.net_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, \n",
    "                      out_channels=chan1, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(chan1),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((1, 2), (1, 2)), # [batch, FIRST_FE_COUNT = 16, 64, 960]\n",
    "\n",
    "            nn.Conv2d(in_channels=chan1, \n",
    "                      out_channels=chan1, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(chan1),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)), # [batch, SECOND_FE_COUNT = 32, 32, 480]\n",
    "\n",
    "            nn.Conv2d(in_channels=chan1, \n",
    "                      out_channels=chan2, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(chan2),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((2, 2), (2, 2)), # [batch, THIRD_FE_COUNT = 32, 16, 240]\n",
    "\n",
    "            nn.Conv2d(in_channels=chan2, \n",
    "                      out_channels=chan2, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(chan2),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)) # [batch=32, QAD_FE_COUNT = 32, 8, 80](что юы сохраниить большще признаков по горизонтали)\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.randn(1,1,N_MELS, 356)\n",
    "            out = self.net_conv(dummy)\n",
    "            self.cnn_output_features = out.numel()//out.shape[0]//out.shape[-2]  # channels * mels\n",
    "\n",
    "        self.layer1 = nn.Linear(self.cnn_output_features, N_MELS*2); \n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "        self.embed_dim = GRU_HIDEN * 2\n",
    "        self.rnn = nn.LSTM(\n",
    "                input_size=N_MELS*2,\n",
    "                hidden_size=GRU_HIDEN,\n",
    "                num_layers=2,\n",
    "                bidirectional=True,\n",
    "                dropout=0.3,\n",
    "                batch_first=True \n",
    "            )\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=self.embed_dim,\n",
    "            num_heads=8,\n",
    "            dropout=0.3,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(self.embed_dim)    \n",
    "        self.attn_dec = AttentionDecoder(enc_dim=self.embed_dim, dec_dim=self.embed_dim, num_classes=num_classes)  \n",
    "        self.layer2 = nn.Linear(self.embed_dim, num_classes)       \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net_conv(x)\n",
    "\n",
    "        batch, channels, reduced_mels, reduced_time = x.shape\n",
    "        x = x.permute(0, 3, 1, 2)  # [batch, time, channels, mels]\n",
    "\n",
    "        # В частности, каждый вектор признаков в последовательности признаков генерируется \n",
    "        # слева направо на картах признаков. Это означает, что i-й вектор признаков представляет \n",
    "        # собой объединение столбцов всех карт. \n",
    "        # Таким образом, форма тензора может быть изменена, например, на (размер_пакета, 80, 256)\n",
    "        \n",
    "        x = x.reshape(batch, reduced_time, -1)  # to GRU [batch=32, seq_len=89, features/hiden_dim=512]\n",
    "        x = self.layer1(x)\n",
    "        x = self.gelu(x)\n",
    "\n",
    "        self.rnn.flatten_parameters()\n",
    "\n",
    "        x = self.rnn(x) # [batch=32, seq_len=89, features/hiden_dim=256 * 2]\n",
    "        x, _ = x # берем информацию со всез состояний\n",
    "        # Типо Self-attention\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.layer_norm(x + attn_output)  \n",
    "        # x = self.dropout(x)\n",
    "        x = self.layer2(x) # logits - [batch, sequence, num_classes] \n",
    "        x = nn.functional.log_softmax(x.permute(1,0,2), dim=2) # pertime так как CTC loss требует на взод (sequence/T,batch/N,num_classes/C)\n",
    "        '''\n",
    "        по одному прогнозу для каждого из признаков в последовательности, \n",
    "        в итоге получается 89 прогнозов символов для каждой секунды звука.\n",
    "        '''\n",
    "        return x\n",
    "    \n",
    "TRAIN_AUGMENT = Compose([\n",
    "    AddGaussianNoise(min_amplitude=1e-4, max_amplitude=0.02, p=0.5),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.25, p=0.4),\n",
    "    Shift(min_shift=-0.5, max_shift=0.5, p=0.3),\n",
    "    Gain(-12, 12, p=0.5),  # <-- исправлено\n",
    "    PolarityInversion(p=0.2)\n",
    "])\n",
    "\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS),\n",
    "    transforms.AmplitudeToDB(top_db=TOP_DB),\n",
    "    transforms.FrequencyMasking(freq_mask_param=FREQ_MASK),\n",
    "    transforms.TimeMasking(time_mask_param=TIME_MASK),\n",
    "    # v2.RandomCrop((N_MELS, 1920)) # Обрезает последний кадр спектрограммы, в идеале надобы считать а не прописывать число\n",
    "    ) # заметка - Данные трансформации не создают довых обучаемых параметров. Но есть и те что создают. В будущем это стоит учитывать\n",
    "\n",
    "valid_audio_transforms = nn.Sequential(\n",
    "    transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS),\n",
    "    transforms.AmplitudeToDB(top_db=TOP_DB),\n",
    "    # v2.CenterCrop((N_MELS, 1920)) \n",
    "    )\n",
    "\n",
    "train_dataframe, val_dataframe = train_test_split(train_data, test_size=0.15, random_state=SEED)\n",
    "\n",
    "train_ds = MosreDataset(df=train_dataframe,\n",
    "                        data_patch=DATASET_PATCH,\n",
    "                        char_to_int=char_to_int,\n",
    "                        train=True,\n",
    "                        transforms=train_audio_transforms)\n",
    "\n",
    "val_ds = MosreDataset(df=val_dataframe,\n",
    "                        data_patch=DATASET_PATCH,\n",
    "                        char_to_int=char_to_int,\n",
    "                        train=True,\n",
    "                        transforms=valid_audio_transforms)\n",
    "\n",
    "\n",
    "def my_collate(batch):\n",
    "    '''\n",
    "    При обработке dataloader labels будут \n",
    "    выравниваться по макс длине для выравнивания батча\n",
    "    Т.е. будет padding . что в будующем будет пустым значением для ctc loss\n",
    "    '''\n",
    "    spectrograms = [item[0].squeeze(0) for item in batch]\n",
    "    # Падинг спектрограмм по максимальной длине\n",
    "    spectrograms_permuted = [s.permute(1, 0) for s in spectrograms]\n",
    "    spectrograms_padded = nn.utils.rnn.pad_sequence(spectrograms_permuted, batch_first=True, padding_value=0.0)\n",
    "    spectrograms_padded = spectrograms_padded.permute(0, 2, 1).unsqueeze(1)\n",
    "\n",
    "    if batch[0][3] is not None:\n",
    "        target = torch.nn.utils.rnn.pad_sequence(\n",
    "                                                [item[1] for item in batch], \n",
    "                                                batch_first=True, \n",
    "                                                padding_value=BLANK_IDX)# выравнивает последовательность до макс \n",
    "                                                                        # длины в батче заполняя пропуски нулем\n",
    "        label_len = torch.stack([item[2] for item in batch])\n",
    "        msg = [item[3] for item in batch]\n",
    "        \n",
    "        return [spectrograms_padded, target, label_len, msg]\n",
    "    else: \n",
    "        return spectrograms_padded\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=my_collate, drop_last=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=my_collate, drop_last=True)\n",
    "\n",
    "test, test_target, _, mess = next(iter(train_dl))\n",
    "test, test_target= test.to(DIVICE), test_target.to(DIVICE)\n",
    "\n",
    "test_val, val_target, __, val_mess = next(iter(val_dl))\n",
    "test_val, val_target = test_val.to(DIVICE), val_target.to(DIVICE)\n",
    "# test.shape \n",
    "\n",
    "#===== начало обучения =====\n",
    "model = MorseNet(num_classes=num_classes).to(DIVICE)\n",
    "# model = DataParallel(model)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=LEARNING_RATE)  # Было 0.002\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "loss_func = nn.CTCLoss(blank=BLANK_IDX, reduction='mean', zero_infinity=True).to(DIVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nMorseNet - инициалицация модели. Число обучаемых параметров: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa7f726",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c23dee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "def ctc_decoder(logits, int_char_map, blank_label_idx):\n",
    "    preds = []\n",
    "    logits_cpu = logits.cpu() \n",
    "    max_inds = torch.argmax(logits_cpu.detach(), dim=2).t().numpy() # арзмакс по лагитам и преобразование к словарю\n",
    "    \n",
    "    for ind in max_inds:\n",
    "        merged_inds = []\n",
    "        for idx in ind:\n",
    "            if idx != blank_label_idx: \n",
    "                merged_inds.append(idx)\n",
    "        text = \"\".join([int_char_map.get(i, '?') for i in merged_inds])\n",
    "        preds.append(text)\n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422ea6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 1/50 =====\n",
      "Mean grad norm: 0.096066\n",
      "Max grad norm: 1.586106\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 4.1781\n",
      "---- Val Loss: 4.1766\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 2/50 =====\n",
      "Mean grad norm: 0.071237\n",
      "Max grad norm: 1.064341\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 4.0223\n",
      "---- Val Loss: 4.1563\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 3/50 =====\n",
      "Mean grad norm: 0.142785\n",
      "Max grad norm: 1.672672\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 4.0185\n",
      "---- Val Loss: 4.0694\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 4/50 =====\n",
      "Mean grad norm: 0.303648\n",
      "Max grad norm: 1.899392\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 3.1852\n",
      "---- Val Loss: 1.1023\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 5/50 =====\n",
      "Mean grad norm: 0.371700\n",
      "Max grad norm: 2.357514\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 1.0981\n",
      "---- Val Loss: 0.4965\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 6/50 =====\n",
      "Mean grad norm: 0.316188\n",
      "Max grad norm: 2.500651\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.7904\n",
      "---- Val Loss: 0.4667\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 7/50 =====\n",
      "Mean grad norm: 0.305953\n",
      "Max grad norm: 2.259201\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.6729\n",
      "---- Val Loss: 0.3408\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 8/50 =====\n",
      "Mean grad norm: 0.339325\n",
      "Max grad norm: 3.109261\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.6079\n",
      "---- Val Loss: 0.2944\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 9/50 =====\n",
      "Mean grad norm: 0.274479\n",
      "Max grad norm: 2.184908\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.5646\n",
      "---- Val Loss: 0.2567\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 10/50 =====\n",
      "Mean grad norm: 0.502051\n",
      "Max grad norm: 4.358695\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.5371\n",
      "---- Val Loss: 0.2498\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 11/50 =====\n",
      "Mean grad norm: 0.389562\n",
      "Max grad norm: 3.417289\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.5094\n",
      "---- Val Loss: 0.2903\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 12/50 =====\n",
      "Mean grad norm: 0.332673\n",
      "Max grad norm: 2.586166\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4947\n",
      "---- Val Loss: 0.2232\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 13/50 =====\n",
      "Mean grad norm: 0.256282\n",
      "Max grad norm: 2.063805\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4768\n",
      "---- Val Loss: 0.2273\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 14/50 =====\n",
      "Mean grad norm: 0.337266\n",
      "Max grad norm: 2.749291\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4658\n",
      "---- Val Loss: 0.2212\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 15/50 =====\n",
      "Mean grad norm: 0.241152\n",
      "Max grad norm: 2.052337\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4542\n",
      "---- Val Loss: 0.2143\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 16/50 =====\n",
      "Mean grad norm: 0.441269\n",
      "Max grad norm: 3.463967\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4415\n",
      "---- Val Loss: 0.2030\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 17/50 =====\n",
      "Mean grad norm: 0.177875\n",
      "Max grad norm: 1.459275\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4342\n",
      "---- Val Loss: 0.2009\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 18/50 =====\n",
      "Mean grad norm: 0.343056\n",
      "Max grad norm: 3.269634\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4318\n",
      "---- Val Loss: 0.2125\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 19/50 =====\n",
      "Mean grad norm: 0.321616\n",
      "Max grad norm: 2.408809\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4250\n",
      "---- Val Loss: 0.2083\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 20/50 =====\n",
      "Mean grad norm: 0.419765\n",
      "Max grad norm: 3.468226\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4168\n",
      "---- Val Loss: 0.1835\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 21/50 =====\n",
      "Mean grad norm: 0.388706\n",
      "Max grad norm: 3.243846\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4179\n",
      "---- Val Loss: 0.2376\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 22/50 =====\n",
      "Mean grad norm: 0.717784\n",
      "Max grad norm: 5.267270\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4109\n",
      "---- Val Loss: 0.2097\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 23/50 =====\n",
      "Mean grad norm: 0.261019\n",
      "Max grad norm: 1.882429\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4038\n",
      "---- Val Loss: 0.1820\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 24/50 =====\n",
      "Mean grad norm: 0.179867\n",
      "Max grad norm: 1.305564\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.4031\n",
      "---- Val Loss: 0.1781\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 25/50 =====\n",
      "Mean grad norm: 0.248724\n",
      "Max grad norm: 1.615864\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.3953\n",
      "---- Val Loss: 0.1858\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 26/50 =====\n",
      "Mean grad norm: 0.175271\n",
      "Max grad norm: 1.250558\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.3926\n",
      "---- Val Loss: 0.1904\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 27/50 =====\n",
      "Mean grad norm: 0.242282\n",
      "Max grad norm: 1.855692\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.3914\n",
      "---- Val Loss: 0.1959\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 28/50 =====\n",
      "Mean grad norm: 0.145248\n",
      "Max grad norm: 0.857805\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.3807\n",
      "---- Val Loss: 0.1732\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 29/50 =====\n",
      "Mean grad norm: 0.328976\n",
      "Max grad norm: 3.332460\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.3820\n",
      "---- Val Loss: 0.1722\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 30/50 =====\n",
      "Mean grad norm: 0.266405\n",
      "Max grad norm: 2.250565\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.3833\n",
      "---- Val Loss: 0.1750\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 31/50 =====\n",
      "Mean grad norm: 0.301770\n",
      "Max grad norm: 2.087591\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.3779\n",
      "---- Val Loss: 0.2087\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 32/50 =====\n",
      "Mean grad norm: 0.389465\n",
      "Max grad norm: 2.822993\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.3738\n",
      "---- Val Loss: 0.1716\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 33/50 =====\n",
      "Mean grad norm: 0.183932\n",
      "Max grad norm: 1.152354\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.3686\n",
      "---- Val Loss: 0.1630\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 34/50 =====\n",
      "Mean grad norm: 0.207592\n",
      "Max grad norm: 1.497677\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.3641\n",
      "---- Val Loss: 0.1802\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 35/50 =====\n",
      "Mean grad norm: 0.317820\n",
      "Max grad norm: 2.629927\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.3584\n",
      "---- Val Loss: 0.1792\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 36/50 =====\n",
      "Mean grad norm: 0.267309\n",
      "Max grad norm: 2.092673\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000200\n",
      "---- Train Loss: 0.3718\n",
      "---- Val Loss: 0.1727\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 37/50 =====\n",
      "Mean grad norm: 0.258507\n",
      "Max grad norm: 2.207173\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000100\n",
      "---- Train Loss: 0.3607\n",
      "---- Val Loss: 0.1879\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 38/50 =====\n",
      "Mean grad norm: 0.164192\n",
      "Max grad norm: 0.997189\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000100\n",
      "---- Train Loss: 0.3446\n",
      "---- Val Loss: 0.1626\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 39/50 =====\n",
      "Mean grad norm: 0.128998\n",
      "Max grad norm: 0.948456\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000100\n",
      "---- Train Loss: 0.3310\n",
      "---- Val Loss: 0.1550\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 40/50 =====\n",
      "Mean grad norm: 0.127667\n",
      "Max grad norm: 1.061912\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000100\n",
      "---- Train Loss: 0.3261\n",
      "---- Val Loss: 0.1627\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 41/50 =====\n",
      "Mean grad norm: 0.136888\n",
      "Max grad norm: 1.244545\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000100\n",
      "---- Train Loss: 0.3248\n",
      "---- Val Loss: 0.1587\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 42/50 =====\n",
      "Mean grad norm: 0.154719\n",
      "Max grad norm: 1.031763\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000100\n",
      "---- Train Loss: 0.3256\n",
      "---- Val Loss: 0.1579\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 43/50 =====\n",
      "Mean grad norm: 0.321592\n",
      "Max grad norm: 2.442849\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000050\n",
      "---- Train Loss: 0.3243\n",
      "---- Val Loss: 0.1579\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 44/50 =====\n",
      "Mean grad norm: 0.615070\n",
      "Max grad norm: 5.421957\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000050\n",
      "---- Train Loss: 0.3129\n",
      "---- Val Loss: 0.1534\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 45/50 =====\n",
      "Mean grad norm: 0.763766\n",
      "Max grad norm: 7.983343\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000050\n",
      "---- Train Loss: 0.3086\n",
      "---- Val Loss: 0.1522\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 46/50 =====\n",
      "Mean grad norm: 0.136167\n",
      "Max grad norm: 1.064135\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000050\n",
      "---- Train Loss: 0.3068\n",
      "---- Val Loss: 0.1536\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 47/50 =====\n",
      "Mean grad norm: 0.303000\n",
      "Max grad norm: 2.364297\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000050\n",
      "---- Train Loss: 0.3073\n",
      "---- Val Loss: 0.1510\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 48/50 =====\n",
      "Mean grad norm: 0.368863\n",
      "Max grad norm: 2.470103\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000050\n",
      "---- Train Loss: 0.3016\n",
      "---- Val Loss: 0.1514\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 49/50 =====\n",
      "Mean grad norm: 0.155270\n",
      "Max grad norm: 1.328383\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000050\n",
      "---- Train Loss: 0.2972\n",
      "---- Val Loss: 0.1526\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 50/50 =====\n",
      "Mean grad norm: 0.369572\n",
      "Max grad norm: 3.702634\n",
      "Min grad norm: 0.000000\n",
      "Current LR: 0.000050\n",
      "---- Train Loss: 0.3023\n",
      "---- Val Loss: 0.1509\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "lst_loss_train = []\n",
    "lst_loss_val = []\n",
    "best_val_loss = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "\n",
    "    epoch_train_loss = 0.0\n",
    "    train_predicts = []\n",
    "\n",
    "    train_tqdm = tqdm(train_dl, desc=f\"Эпоха {epoch+1}/{EPOCHS} [Обучение]\", leave=False)\n",
    "    for batch_ind, batch in enumerate(train_tqdm):\n",
    "        mel_spec, targets, targets_lens, _ = batch\n",
    "        mel_spec, targets, targets_lens = mel_spec.to(DIVICE), targets.to(DIVICE), targets_lens.to(DIVICE)\n",
    "\n",
    "        #===== считатем длинну mel_spec для передачи в CTC loss =====\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predict = model(mel_spec) # (N=batch,T,C)\n",
    "        N = predict.shape[1]\n",
    "        T = predict.shape[0]\n",
    "        predict_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "\n",
    "        try:\n",
    "            loss = loss_func(predict, targets, predict_lengths, targets_lens.reshape(BATCH_SIZE))\n",
    "        except RuntimeError:\n",
    "            print(predict.shape, targets.shape, predict_lengths, targets_lens.reshape(BATCH_SIZE))\n",
    "            continue\n",
    "        # print(loss)\n",
    "        if torch.isnan(loss) or torch.isinf(loss): \n",
    "            print(f\"\\nWarning: In batch-{batch_ind} loss train is NaN/Inf: {loss.item()}\"); \n",
    "            optimizer.zero_grad(); \n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "        # nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "    train_loss = epoch_train_loss / len(train_dl)\n",
    "\n",
    "    # ======== Валидация ========\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    total_val = 0\n",
    "    val_predicts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_mel_spec, val_labels, val_label_lensin, _ in tqdm(\n",
    "                                                        val_dl, \n",
    "                                                        desc=f\"Эпоха {epoch+1}/{EPOCHS} [Валидация]\", \n",
    "                                                        leave=False):\n",
    "            val_mel_spec, val_labels, val_label_lensin = val_mel_spec.to(DIVICE), val_labels.to(DIVICE), val_label_lensin.to(DIVICE)\n",
    "            val_predict = model(val_mel_spec)\n",
    "\n",
    "            val_N = val_predict.shape[1]\n",
    "            val_T = val_predict.shape[0]\n",
    "            predict_val_lengths = torch.full(size=(val_N,), fill_value=val_T, dtype=torch.long)\n",
    "            val_loss += loss_func(val_predict, val_labels, predict_val_lengths, val_label_lensin).item()\n",
    "\n",
    "    total_val = val_loss / len(val_dl)\n",
    "\n",
    "    lst_loss_train.append(train_loss)\n",
    "    lst_loss_val.append(total_val)\n",
    "\n",
    "    scheduler.step(total_val)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f\"Новая лучшая модель сохранена с val_loss: {val_loss:.4f}\")\n",
    "    \n",
    "\n",
    "    print(f\"\\n===== Эпоха {epoch+1}/{EPOCHS} =====\")\\\n",
    "    #===== Инфо про градиенты=====\n",
    "    grad_norms = [param.grad.norm().item() for param in model.parameters() if param.grad is not None]\n",
    "    if grad_norms:\n",
    "        print(f\"Mean grad norm: {np.mean(grad_norms):.6f}\")\n",
    "        print(f\"Max grad norm: {np.max(grad_norms):.6f}\")\n",
    "        print(f\"Min grad norm: {np.min(grad_norms):.6f}\")\n",
    "    else:\n",
    "        print(\"No gradients computed yet.\")\n",
    "    #===== Инфо про шаг обучения и данные по потерям =====\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Current LR: {current_lr:.6f}\")\n",
    "    print(f\"---- Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"---- Val Loss: {total_val:.4f}\")\n",
    "    if current_lr <= 1e-6:\n",
    "        print(\"Learning rate достиг минимума 1e-6, остановка обучения\")\n",
    "        break\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca52e14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'MorseNet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1547a2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lst_loss_train)\n",
    "plt.plot(lst_loss_val)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45f65295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN размерность выхода: torch.Size([1, 32, 8, 89])\n",
      "CNN число фичей: 256\n",
      "Mean accurasu by The Levenshtein in train is : 0.8834991353129507\n",
      "Mean accurasu by The Levenshtein in validate is : 0.9265079209781295\n"
     ]
    }
   ],
   "source": [
    "def ctc_decoder(logits, int_char_map, blank_label_idx):\n",
    "    preds = []\n",
    "    logits_cpu = logits.cpu() \n",
    "    max_inds = torch.argmax(logits_cpu.detach(), dim=2).t().numpy() # арзмакс по лагитам и преобразование к словарю\n",
    "    \n",
    "    for ind in max_inds:\n",
    "        merged_inds = []\n",
    "        for idx in ind:\n",
    "            if idx != blank_label_idx: \n",
    "                merged_inds.append(idx)\n",
    "        text = \"\".join([int_char_map.get(i, '?') for i in merged_inds])\n",
    "        preds.append(text)\n",
    "\n",
    "    return preds\n",
    "\n",
    "model_load = MorseNet(num_classes=num_classes)\n",
    "model_load.load_state_dict(torch.load('MorseNet.pth'))\n",
    "model_load.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_mess = []\n",
    "    train_predicts = []\n",
    "    for loader in train_dl:\n",
    "        seq, test_target, _, mess = loader\n",
    "        train_mess.extend(mess)\n",
    "\n",
    "        logits = model_load(seq)\n",
    "        predicted_values = ctc_decoder(logits, int_to_char, BLANK_IDX)\n",
    "        train_predicts.extend(predicted_values)\n",
    "\n",
    "    val_mess = []\n",
    "    val_predicts = []\n",
    "    for loader in val_dl:\n",
    "        seq, test_target, _, mess = loader\n",
    "        val_mess.extend(mess)\n",
    "\n",
    "        logits= model_load(seq)\n",
    "        predicted_values = ctc_decoder(logits, int_to_char, BLANK_IDX)\n",
    "        val_predicts.extend(predicted_values)\n",
    "\n",
    "import Levenshtein\n",
    "\n",
    "mean_acc_test = np.mean([Levenshtein.ratio(test_pred, train_mess[ind]) for ind, test_pred in enumerate(train_predicts)])\n",
    "mean_acc_val = np.mean([Levenshtein.ratio(val_pred, val_mess[ind]) for ind, val_pred in enumerate(val_predicts)])\n",
    "\n",
    "\n",
    "print(f\"Mean accurasu by The Levenshtein in train is : {mean_acc_test}\")\n",
    "print(f\"Mean accurasu by The Levenshtein in validate is : {mean_acc_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49ca828",
   "metadata": {},
   "source": [
    "# Сбор sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c85e44a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN размерность выхода: torch.Size([1, 32, 8, 89])\n",
      "CNN число фичей: 256\n"
     ]
    }
   ],
   "source": [
    "def ctc_decoder(logits, int_char_map, blank_label_idx):\n",
    "    preds = []\n",
    "    logits_cpu = logits.cpu() \n",
    "    max_inds = torch.argmax(logits_cpu.detach(), dim=2).t().numpy() # арзмакс по лагитам и преобразование к словарю\n",
    "    \n",
    "    for ind in max_inds:\n",
    "        merged_inds = []\n",
    "        for idx in ind:\n",
    "            if idx != blank_label_idx: \n",
    "                merged_inds.append(idx)\n",
    "        text = \"\".join([int_char_map.get(i, '?') for i in merged_inds])\n",
    "        preds.append(text)\n",
    "\n",
    "    return preds\n",
    "\n",
    "model_load = MorseNet(num_classes=num_classes)\n",
    "model_load.load_state_dict(torch.load('MorseNet.pth'))\n",
    "model_load.eval()\n",
    "\n",
    "test_ds = MosreDataset(df=sample_data,\n",
    "                        data_patch=DATASET_PATCH,\n",
    "                        char_to_int=char_to_int,\n",
    "                        train=False,\n",
    "                        transforms=valid_audio_transforms)\n",
    "\n",
    "\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=20, shuffle=False, collate_fn=my_collate)\n",
    "model_load.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_predicts = []\n",
    "    for loader in test_dl:\n",
    "        seq = loader\n",
    "        logits = model_load(seq)\n",
    "        predicted_values = ctc_decoder(logits, int_to_char, BLANK_IDX)\n",
    "        test_predicts.extend(predicted_values)\n",
    "\n",
    "sample_data.message = test_predicts\n",
    "sample_data.to_csv('sample_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Morse_decoder_V2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
