{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d761d8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN размерность выхода: torch.Size([1, 32, 16, 120])\n",
      "CNN число фичей: 512\n",
      "Проекция из 512 в 512\n",
      "\n",
      "MorseNet - инициалицация модели. Число обучаемых параметров: 4,384,525\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path as pt\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchaudio import transforms\n",
    "from torchvision.transforms import v2\n",
    "# from Moduls.MosreDataset import MosreDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.nn.parallel import DataParallel\n",
    "\n",
    "DIVICE = torch.device(\"cuda\")\n",
    "\n",
    "MAIN = pt(os.getcwd())\n",
    "DATASET_PATCH = MAIN / 'morse_dataset'\n",
    "AUDIO_FILES = DATASET_PATCH / 'morse_dataset'\n",
    "\n",
    "# Поятоянные значения выявленные в процессе анализа\n",
    "MORSEALP = \"АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ 1234567890#\"\n",
    "MAX_TIME = 48\n",
    "SAMPLE_RATE = 8000\n",
    "N_MELS = 128\n",
    "N_FFT = 400\n",
    "HOP_LENGTH = 180\n",
    "TOP_DB = 80\n",
    "FREQ_MASK = 30\n",
    "TIME_MASK = 40\n",
    "\n",
    "# Гиперпараметы обучения\n",
    "SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.002 #2e-4\n",
    "WEIGHT_DECAY = 0.00001\n",
    "# int_to_alph = dict(enumerate(MORSEALP))\n",
    "# alph_to_int = {char:enum for enum, char in int_to_alph.items()}\n",
    "\n",
    "#===== Import data =====\n",
    "train_data = pd.read_csv(pt.joinpath(DATASET_PATCH,'train.csv'))\n",
    "test_data = pd.read_csv(pt.joinpath(DATASET_PATCH,'test.csv'))\n",
    "sample_data = pd.read_csv(pt.joinpath(DATASET_PATCH,'sample_submission.csv'))\n",
    "\n",
    "class MosreDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Класс для обработки \n",
    "    \"\"\"\n",
    "    def __init__(self, df, data_patch, train=True, transforms=None, prev_chars = 1):\n",
    "        self.df = df\n",
    "        self.is_train = train\n",
    "\n",
    "        self.data_path = data_patch\n",
    "        self.audio_paths = self.data_path / 'morse_dataset'\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.morse_alp = \"АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ 1234567890#\"\n",
    "        self.int_to_alph = dict(enumerate(MORSEALP, start=1)) # 0 - Выводим под пустое\n",
    "        self.alph_to_int = {char:enum+1 for enum, char in self.int_to_alph.items()}\n",
    "        self.prev_chars = prev_chars\n",
    "\n",
    "        if self.is_train:\n",
    "            self.messeges = self.df.message.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #Получение аугментрованых спектрограмм\n",
    "        try:\n",
    "            audio_file = self.audio_paths / self.df.id.values[index]\n",
    "            waveform, sample_rate = torchaudio.load(audio_file)\n",
    "            augmented_spectrogram = self.transforms(waveform)\n",
    "\n",
    "            if self.is_train:\n",
    "                message = self.messeges[index]\n",
    "                #Получение списка индексов секта - как требует CTC los\n",
    "                '''\n",
    "                При обработке dataloader labels будут выравниваться по макс длине для выравнивания батча\n",
    "                Т.е. будет padding 0. что в будующем будет пустым значением для ctc loss\n",
    "                '''\n",
    "                label = torch.LongTensor([self.morse_alp.find(c) + 1 for c in message])\n",
    "                label_len = torch.LongTensor([len(label)])\n",
    "                return augmented_spectrogram, label, label_len, message\n",
    "            else:\n",
    "                return augmented_spectrogram\n",
    "        except Exception as ex:\n",
    "            print(str(ex))\n",
    "        \n",
    "    def change_time(self, audio_file, max_len = 384000):\n",
    "        waveform, sample_rate = torchaudio.load(audio_file)\n",
    "        cahanal, sig_len = waveform.shape\n",
    "\n",
    "        if sig_len < max_len:\n",
    "            pad_len = torch.zeros(max_len - sig_len).unsqueeze(0)\n",
    "            waveform = torch.cat([waveform, pad_len], dim=1)\n",
    "\n",
    "        return waveform\n",
    "    \n",
    "FIRST_FE_COUNT = 16\n",
    "SECOND_FE_COUNT = 32\n",
    "THIRD_FE_COUNT = 32\n",
    "QAD_FE_COUNT = 32\n",
    "PADDING = 'same'\n",
    "MAXPOOL_KERNEL = 2\n",
    "KERTNEL_SIZE = 3\n",
    "NERON_COUNT = 128\n",
    "GRU_HIDEN = 256\n",
    "DROPOUT = 0.45\n",
    "\n",
    "NUM_HEADS = 8#?\n",
    "# Start with 4 transforms\n",
    "class MorseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, \n",
    "                      out_channels=FIRST_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(FIRST_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((1, 2), (1, 2)), # [batch, FIRST_FE_COUNT = 16, 64, 960]\n",
    "\n",
    "            nn.Conv2d(in_channels=FIRST_FE_COUNT, \n",
    "                      out_channels=SECOND_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(SECOND_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)), # [batch, SECOND_FE_COUNT = 32, 32, 480]\n",
    "\n",
    "            nn.Conv2d(in_channels=SECOND_FE_COUNT, \n",
    "                      out_channels=THIRD_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(THIRD_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((2, 2), (2, 2)), # [batch, THIRD_FE_COUNT = 32, 16, 240]\n",
    "\n",
    "            nn.Conv2d(in_channels=THIRD_FE_COUNT, \n",
    "                      out_channels=QAD_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(QAD_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)) # [batch=32, QAD_FE_COUNT = 32, 8, 80](что юы сохраниить большще признаков по горизонтали)\n",
    "        )\n",
    "        with torch.no_grad(): \n",
    "            dummy_input = torch.randn(1, 1, N_MELS, 480); \n",
    "            cnn_out = self.net_conv(dummy_input); \n",
    "            self.cnn_output_features = cnn_out.shape[1] * cnn_out.shape[2]\n",
    "        print(f\"CNN размерность выхода: {cnn_out.shape}\"); \n",
    "        print(f\"CNN число фичей: {self.cnn_output_features}\")\n",
    "\n",
    "        # Добавлен лоейный слой и функция активации. Для чего? расписать потом \n",
    "        self.layer1 = nn.Linear(self.cnn_output_features, N_MELS*2); \n",
    "        self.gelu = nn.LeakyReLU()\n",
    "        print(f\"Проекция из {self.cnn_output_features} в {GRU_HIDEN*2}\")\n",
    "        self.rnn = nn.GRU(input_size=N_MELS*2,hidden_size=GRU_HIDEN, num_layers=3 ,bidirectional=True,dropout=DROPOUT)\n",
    "        \n",
    "        self.embed_dim = GRU_HIDEN * 2\n",
    "        self.mha = nn.MultiheadAttention(self.embed_dim, NUM_HEADS, dropout=DROPOUT, batch_first=True)\n",
    "        self.dropout_mha = nn.Dropout(DROPOUT)\n",
    "        self.layer_norm = nn.LayerNorm(self.embed_dim)      \n",
    "        self.layer2 = nn.Linear(self.embed_dim, len(MORSEALP))       \n",
    "        # self.layer3 = nn.Linear(GRU_HIDEN, GRU_HIDEN // 2)       \n",
    "        # self.layer4 = nn.Linear(GRU_HIDEN // 2, 45)             \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net_conv(x)\n",
    "\n",
    "        batch, channels, reduced_mels, reduced_time = x.shape\n",
    "        x = x.permute(0, 3, 1, 2)  # [batch, time, channels, mels]\n",
    "        # В частности, каждый вектор признаков в последовательности признаков генерируется \n",
    "        # слева направо на картах признаков. Это означает, что i-й вектор признаков представляет \n",
    "        # собой объединение столбцов всех карт. \n",
    "        # Таким образом, форма тензора может быть изменена, например, на (размер_пакета, 80, 256)\n",
    "        x = x.reshape(batch, reduced_time, -1)  # to GRU [batch=32, seq_len=80, features=256]\n",
    "        x = self.layer1(x)\n",
    "        x = self.gelu(x)\n",
    "        self.rnn.flatten_parameters()\n",
    "        x = self.rnn(x)\n",
    "        x, _ = x # берем информацию со всез состояний\n",
    "        attn_output, _ = self.mha(x, x, x)\n",
    "        x = x + self.dropout_mha(attn_output)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.layer2(x)\n",
    "        x = nn.functional.log_softmax(x.permute(1, 0, 2), dim=2)\n",
    "        # x = self.layer3(x)\n",
    "        # x = self.layer4(x)\n",
    "        # x = x.log_softmax(dim=2) #log_softmax дает лучше распределение вероятностей чем softmax\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS),\n",
    "    # transforms.AmplitudeToDB(top_db=TOP_DB),\n",
    "    transforms.FrequencyMasking(freq_mask_param=FREQ_MASK),\n",
    "    transforms.TimeMasking(time_mask_param=TIME_MASK),\n",
    "    # v2.RandomCrop((N_MELS, 1920)) # Обрезает последний кадр спектрограммы, в идеале надобы считать а не прописывать число\n",
    "    ) # заметка - Данные трансформации не создают довых обучаемых параметров. Но есть и те что создают. В будущем это стоит учитывать\n",
    "\n",
    "valid_audio_transforms = nn.Sequential(\n",
    "    transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS),\n",
    "    # transforms.AmplitudeToDB(top_db=TOP_DB),\n",
    "    # v2.CenterCrop((N_MELS, 1920)) \n",
    "    )\n",
    "\n",
    "train_dataframe, val_dataframe = train_test_split(train_data, test_size=0.15, random_state=SEED)\n",
    "\n",
    "train_ds = MosreDataset(df=train_dataframe,\n",
    "                        data_patch=DATASET_PATCH,\n",
    "                        train=True,\n",
    "                        transforms=train_audio_transforms)\n",
    "\n",
    "val_ds = MosreDataset(df=val_dataframe,\n",
    "                        data_patch=DATASET_PATCH,\n",
    "                        train=True,\n",
    "                        transforms=valid_audio_transforms)\n",
    "\n",
    "def my_collate(batch):\n",
    "    data = torch.stack([item[0] for item in batch])\n",
    "    target = torch.nn.utils.rnn.pad_sequence(\n",
    "                                            [item[1] for item in batch], \n",
    "                                            batch_first=True, \n",
    "                                            padding_value=0) # выравнивает последовательность до макс \n",
    "                                                            # длины в датче заполняя пропуски нулем\n",
    "    label_len = torch.stack([item[2] for item in batch])\n",
    "    msg = [item[3] for item in batch]\n",
    "    \n",
    "    return [data, target, label_len, msg]\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=my_collate)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=my_collate)\n",
    "\n",
    "test, test_target, _, mess = next(iter(train_dl))\n",
    "test, test_target= test.to(DIVICE), test_target.to(DIVICE)\n",
    "\n",
    "test_val, val_target, _, val_mess = next(iter(val_dl))\n",
    "test_val, val_target = test_val.to(DIVICE), val_target.to(DIVICE)\n",
    "# test.shape \n",
    "\n",
    "#===== начало обучения =====\n",
    "model = MorseNet().to(DIVICE)\n",
    "model = DataParallel(model)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=6, verbose=True)\n",
    "loss_func = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True).to(DIVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nMorseNet - инициалицация модели. Число обучаемых параметров: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be51c697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 128, 356])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3376b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaf999d",
   "metadata": {},
   "source": [
    "# Класс модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66eee39",
   "metadata": {},
   "source": [
    "Переменные для обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad235125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.2037, -4.7267, -4.2663,  ..., -4.2396, -3.8800, -4.7673],\n",
       "         [-4.3004, -4.1307, -3.9769,  ..., -3.7541, -3.5193, -4.1914],\n",
       "         [-3.9482, -4.0669, -3.6103,  ..., -4.0812, -3.4989, -4.3134],\n",
       "         ...,\n",
       "         [-4.1580, -3.6626, -4.4798,  ..., -3.6862, -3.7639, -3.8585],\n",
       "         [-3.6008, -3.5724, -4.2793,  ..., -3.8804, -3.8946, -4.3018],\n",
       "         [-4.0057, -3.6203, -4.5168,  ..., -3.9536, -3.4638, -3.9795]],\n",
       "\n",
       "        [[-4.6096, -3.4990, -3.6985,  ..., -3.6755, -3.6458, -3.9684],\n",
       "         [-4.2195, -3.7391, -3.7206,  ..., -3.8120, -4.3029, -4.1506],\n",
       "         [-4.1432, -3.9196, -4.0357,  ..., -3.3182, -3.6239, -3.7192],\n",
       "         ...,\n",
       "         [-3.7996, -2.9632, -4.4639,  ..., -3.8727, -3.0264, -3.9271],\n",
       "         [-4.0798, -3.2055, -3.9844,  ..., -3.8883, -3.5489, -3.5206],\n",
       "         [-3.9002, -3.7827, -4.2671,  ..., -4.1030, -3.9059, -3.4705]],\n",
       "\n",
       "        [[-4.1263, -3.1938, -4.3421,  ..., -4.0233, -3.5581, -4.3454],\n",
       "         [-4.3282, -2.9915, -4.0521,  ..., -4.2108, -3.2545, -4.1331],\n",
       "         [-4.3661, -3.6688, -4.4206,  ..., -3.8817, -2.8928, -4.7443],\n",
       "         ...,\n",
       "         [-3.8971, -3.9034, -4.4220,  ..., -4.1092, -4.1079, -4.1900],\n",
       "         [-3.9264, -3.6740, -4.5030,  ..., -4.1714, -3.8145, -3.7845],\n",
       "         [-3.9949, -3.9060, -4.2288,  ..., -3.5974, -3.4212, -3.8188]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-3.7570, -3.2167, -3.9282,  ..., -3.7151, -4.4799, -4.1895],\n",
       "         [-3.5601, -2.9807, -4.4855,  ..., -2.9099, -4.1184, -3.3301],\n",
       "         [-4.0387, -3.6232, -3.9352,  ..., -3.5281, -3.7572, -3.9023],\n",
       "         ...,\n",
       "         [-4.0330, -3.2132, -4.3981,  ..., -3.4144, -4.1173, -4.5782],\n",
       "         [-3.6061, -3.7285, -5.0187,  ..., -3.6412, -3.6079, -4.7389],\n",
       "         [-4.0644, -3.1561, -4.3274,  ..., -3.9846, -3.8927, -4.6646]],\n",
       "\n",
       "        [[-4.6694, -3.5636, -4.0389,  ..., -3.8832, -3.8690, -4.1673],\n",
       "         [-4.0062, -3.7327, -3.7757,  ..., -3.4132, -3.8298, -4.0888],\n",
       "         [-4.0009, -3.3462, -3.9201,  ..., -3.6712, -3.7879, -4.2463],\n",
       "         ...,\n",
       "         [-4.4621, -3.5245, -4.3842,  ..., -3.1280, -4.0860, -4.3119],\n",
       "         [-4.1643, -3.3400, -4.7309,  ..., -3.9041, -3.6826, -4.0776],\n",
       "         [-4.2194, -3.3237, -4.3624,  ..., -3.4538, -3.5169, -3.6101]],\n",
       "\n",
       "        [[-3.7224, -3.5406, -3.8501,  ..., -3.5668, -4.1244, -3.9506],\n",
       "         [-3.5272, -4.0388, -3.7553,  ..., -3.9316, -4.3987, -3.3831],\n",
       "         [-4.2348, -3.6491, -3.9291,  ..., -3.8158, -3.7333, -4.1431],\n",
       "         ...,\n",
       "         [-4.4298, -3.5563, -4.3668,  ..., -4.0375, -3.7061, -4.4017],\n",
       "         [-3.9774, -3.4284, -4.6761,  ..., -3.5727, -3.4073, -4.3294],\n",
       "         [-4.0025, -3.4404, -4.0119,  ..., -3.3929, -3.4010, -3.9801]]],\n",
       "       device='cuda:0', grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a = model(test_val)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa7f726",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "422ea6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 1/20 =====\n",
      "Mean grad norm: nan\n",
      "Max grad norm: nan\n",
      "Min grad norm: nan\n",
      "Train Loss: nan\n",
      "Val Loss: nan\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m pr = []\n\u001b[32m     12\u001b[39m train_tqdm = tqdm(train_dl, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mЭпоха \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m [Обучение]\u001b[39m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmel_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_lens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_tqdm\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmel_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_lens\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmel_spec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDIVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDIVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_lens\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDIVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#===== считатем длинну mel_spec для передачи в CTC loss =====\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mMosreDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     77\u001b[39m     audio_file = \u001b[38;5;28mself\u001b[39m.audio_paths / \u001b[38;5;28mself\u001b[39m.df.id.values[index]\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     waveform, sample_rate = \u001b[43mtorchaudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     augmented_spectrogram = \u001b[38;5;28mself\u001b[39m.transforms(waveform)\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_train:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:205\u001b[39m, in \u001b[36mget_load_func.<locals>.load\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[32m    129\u001b[39m \n\u001b[32m    130\u001b[39m \u001b[33;03mBy default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    202\u001b[39m \u001b[33;03m        `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    204\u001b[39m backend = dispatcher(uri, \u001b[38;5;28mformat\u001b[39m, backend)\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torchaudio\\_backend\\soundfile.py:27\u001b[39m, in \u001b[36mSoundfileBackend.load\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     19\u001b[39m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     buffer_size: \u001b[38;5;28mint\u001b[39m = \u001b[32m4096\u001b[39m,\n\u001b[32m     26\u001b[39m ) -> Tuple[torch.Tensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msoundfile_backend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:230\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[39m\n\u001b[32m    227\u001b[39m         dtype = _SUBTYPE2DTYPE[file_.subtype]\n\u001b[32m    229\u001b[39m     frames = file_._prepare_read(frame_offset, \u001b[38;5;28;01mNone\u001b[39;00m, num_frames)\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     waveform = \u001b[43mfile_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malways_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m     sample_rate = file_.samplerate\n\u001b[32m    233\u001b[39m waveform = torch.from_numpy(waveform)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\soundfile.py:942\u001b[39m, in \u001b[36mSoundFile.read\u001b[39m\u001b[34m(self, frames, dtype, always_2d, fill_value, out)\u001b[39m\n\u001b[32m    940\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m frames < \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m frames > \u001b[38;5;28mlen\u001b[39m(out):\n\u001b[32m    941\u001b[39m         frames = \u001b[38;5;28mlen\u001b[39m(out)\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m frames = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_array_io\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mread\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) > frames:\n\u001b[32m    944\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\soundfile.py:1394\u001b[39m, in \u001b[36mSoundFile._array_io\u001b[39m\u001b[34m(self, action, array, frames)\u001b[39m\n\u001b[32m   1392\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m array.dtype.itemsize == _ffi.sizeof(ctype)\n\u001b[32m   1393\u001b[39m cdata = _ffi.cast(ctype + \u001b[33m'\u001b[39m\u001b[33m*\u001b[39m\u001b[33m'\u001b[39m, array.__array_interface__[\u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m1394\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cdata_io\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\soundfile.py:1403\u001b[39m, in \u001b[36mSoundFile._cdata_io\u001b[39m\u001b[34m(self, action, data, ctype, frames)\u001b[39m\n\u001b[32m   1401\u001b[39m     curr = \u001b[38;5;28mself\u001b[39m.tell()\n\u001b[32m   1402\u001b[39m func = \u001b[38;5;28mgetattr\u001b[39m(_snd, \u001b[33m'\u001b[39m\u001b[33msf_\u001b[39m\u001b[33m'\u001b[39m + action + \u001b[33m'\u001b[39m\u001b[33mf_\u001b[39m\u001b[33m'\u001b[39m + ctype)\n\u001b[32m-> \u001b[39m\u001b[32m1403\u001b[39m frames = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1404\u001b[39m _error_check(\u001b[38;5;28mself\u001b[39m._errorcode)\n\u001b[32m   1405\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.seekable():\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "lst_loss_train = []\n",
    "lst_loss_val = []\n",
    "p = []\n",
    "p_Val = []\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    pr = []\n",
    "\n",
    "    train_tqdm = tqdm(train_dl, desc=f\"Эпоха {epoch+1}/{EPOCHS} [Обучение]\", leave=False)\n",
    "    for mel_spec, labels, label_lens, _ in train_tqdm:\n",
    "        mel_spec, labels, label_lens = mel_spec.to(DIVICE), labels.to(DIVICE), label_lens.to(DIVICE)\n",
    "\n",
    "        #===== считатем длинну mel_spec для передачи в CTC loss =====\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predict = model(mel_spec) # (N=batch,T,C)\n",
    "        pr.append(predict)\n",
    "        N = predict.shape[1]\n",
    "        T = predict.shape[0]\n",
    "        predict_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "\n",
    "        # print(N, T, predict.shape, labels.shape, predict_lengths.shape, label_lens.shape)\n",
    "        # break\n",
    "        loss = loss_func(predict, labels, predict_lengths, label_lens)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        train_loss = epoch_train_loss / len(train_data)\n",
    "\n",
    "    # ======== Валидация ========\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_mel_spec, val_labels, val_label_lensin, _ in tqdm(\n",
    "                                                        val_dl, \n",
    "                                                        desc=f\"Эпоха {epoch+1}/{EPOCHS} [Валидация]\", \n",
    "                                                        leave=False):\n",
    "            val_mel_spec, val_labels, val_label_lensin = val_mel_spec.to(DIVICE), val_labels.to(DIVICE), val_label_lensin.to(DIVICE)\n",
    "            val_predict = model(val_mel_spec)\n",
    "\n",
    "            p_Val.append(val_predict)\n",
    "            val_N = val_predict.shape[1]\n",
    "            val_T = val_predict.shape[0]\n",
    "            predict_val_lengths = torch.full(size=(val_N,), fill_value=val_T, dtype=torch.long)\n",
    "            val_loss += loss_func(val_predict, val_labels, predict_val_lengths, val_label_lensin).item()\n",
    "\n",
    "\n",
    "\n",
    "    lst_loss_train.append(train_loss)\n",
    "    lst_loss_val.append(val_loss)\n",
    "\n",
    "            \n",
    "    print(f\"\\n===== Эпоха {epoch+1}/{EPOCHS} =====\")\n",
    "    # print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    grad_norms = [param.grad.norm().item() for param in model.parameters() if param.grad is not None]\n",
    "    if grad_norms:\n",
    "        print(f\"Mean grad norm: {np.mean(grad_norms):.6f}\")\n",
    "        print(f\"Max grad norm: {np.max(grad_norms):.6f}\")\n",
    "        print(f\"Min grad norm: {np.min(grad_norms):.6f}\")\n",
    "    else:\n",
    "        print(\"No gradients computed yet.\")\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1547a2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGdCAYAAADuR1K7AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAIwZJREFUeJzt3X9QlWX+//EXyAEkQxZ/cEQxc9dNMtMNB8RpxkoEd91JytViTI0YXSepNlxXKdOs/QxrPwxTymk2a9pydXVbdyvXZFHLzSMm9sPf0+5WlnRAcxHThBNc3z/8craTR8SG+wAXz8eM03Cf6z7nut+D+ZxzboYwY4wRAACAJcLbegMAAACtibgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYJWItt5AW2hsbFRlZaUuv/xyhYWFtfV2AABACxhjdOrUKSUmJio8/MLvz3TKuKmsrFRSUlJbbwMAAHwPn332mfr163fBxztl3Fx++eWSzg0nNja2jXfT9nw+nzZv3qzMzEy5XK623o61mHNoMOfQYM6hwZwD1dbWKikpyf/v+IV0yrhp+igqNjaWuNG5vzwxMTGKjY3lL4+DmHNoMOfQYM6hwZyDu9gtJdxQDAAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqIYmbkpISDRgwQNHR0UpLS9OuXbuaXb9u3ToNHjxY0dHRGjp0qDZu3HjBtbNmzVJYWJiKi4tbedcAAKAjcjxu1q5dq4KCAi1atEh79uzRsGHDlJWVperq6qDrd+zYoZycHOXl5em9995Tdna2srOztW/fvvPW/uUvf9HOnTuVmJjo9GUAAIAOwvG4Wbp0qWbMmKHc3FxdffXVWrlypWJiYrRq1aqg65ctW6Zx48Zp7ty5Sk5O1qOPPqrrrrtOK1asCFh39OhR3XPPPXrllVfkcrmcvgwAANBBRDj55PX19aqoqFBhYaH/WHh4uDIyMuTxeIKe4/F4VFBQEHAsKytLGzZs8H/d2NioqVOnau7cuRoyZMhF91FXV6e6ujr/17W1tZIkn88nn893KZdkpaYZMAtnMefQYM6hwZxDgzkHaukcHI2b48ePq6GhQQkJCQHHExISdOjQoaDneL3eoOu9Xq//6yVLligiIkL33ntvi/ZRVFSkxYsXn3d88+bNiomJadFzdAalpaVtvYVOgTmHBnMODeYcGsz5nDNnzrRonaNx44SKigotW7ZMe/bsUVhYWIvOKSwsDHg3qLa2VklJScrMzFRsbKxTW+0wfD6fSktLNXbsWD7icxBzDg3mHBrMOTSYc6CmT14uxtG46dmzp7p06aKqqqqA41VVVXK73UHPcbvdza7fvn27qqur1b9/f//jDQ0NmjNnjoqLi/XJJ5+c95xRUVGKioo677jL5eKb5VuYR2gw59BgzqHBnEODOZ/T0hk4ekNxZGSkUlJSVFZW5j/W2NiosrIypaenBz0nPT09YL107u24pvVTp07Vhx9+qPfff9//JzExUXPnztWbb77p3MUAAIAOwfGPpQoKCjR9+nSNGDFCqampKi4u1unTp5WbmytJmjZtmvr27auioiJJ0n333afRo0frySef1Pjx47VmzRrt3r1bzz33nCSpR48e6tGjR8BruFwuud1uXXXVVU5fDgAAaOccj5vbbrtNx44d08KFC+X1ejV8+HBt2rTJf9PwkSNHFB7+vzeQRo0apdWrV2vBggV64IEHNGjQIG3YsEHXXHON01sFAAAWCMkNxfn5+crPzw/62LZt2847NmnSJE2aNKnFzx/sPhsAANA58bulAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYJSdyUlJRowIABio6OVlpamnbt2tXs+nXr1mnw4MGKjo7W0KFDtXHjRv9jPp9P8+bN09ChQ3XZZZcpMTFR06ZNU2VlpdOXAQAAOgDH42bt2rUqKCjQokWLtGfPHg0bNkxZWVmqrq4Oun7Hjh3KyclRXl6e3nvvPWVnZys7O1v79u2TJJ05c0Z79uzRQw89pD179ujVV1/V4cOHdfPNNzt9KQAAoANwPG6WLl2qGTNmKDc3V1dffbVWrlypmJgYrVq1Kuj6ZcuWady4cZo7d66Sk5P16KOP6rrrrtOKFSskSd27d1dpaakmT56sq666SiNHjtSKFStUUVGhI0eOOH05AACgnYtw8snr6+tVUVGhwsJC/7Hw8HBlZGTI4/EEPcfj8aigoCDgWFZWljZs2HDB1zl58qTCwsIUFxcX9PG6ujrV1dX5v66trZV07iMun8/XwquxV9MMmIWzmHNoMOfQYM6hwZwDtXQOjsbN8ePH1dDQoISEhIDjCQkJOnToUNBzvF5v0PVerzfo+rNnz2revHnKyclRbGxs0DVFRUVavHjxecc3b96smJiYllxKp1BaWtrWW+gUmHNoMOfQYM6hwZzPOXPmTIvWORo3TvP5fJo8ebKMMXr22WcvuK6wsDDg3aDa2lolJSUpMzPzgkHUmfh8PpWWlmrs2LFyuVxtvR1rMefQYM6hwZxDgzkHavrk5WIcjZuePXuqS5cuqqqqCjheVVUlt9sd9By3292i9U1h8+mnn2rLli3NRkpUVJSioqLOO+5yufhm+RbmERrMOTSYc2gw59Bgzue0dAaO3lAcGRmplJQUlZWV+Y81NjaqrKxM6enpQc9JT08PWC+dezvu2+ubwuajjz7SP/7xD/Xo0cOZCwAAAB2O4x9LFRQUaPr06RoxYoRSU1NVXFys06dPKzc3V5I0bdo09e3bV0VFRZKk++67T6NHj9aTTz6p8ePHa82aNdq9e7eee+45SefC5he/+IX27Nmj119/XQ0NDf77ceLj4xUZGen0JQEAgHbM8bi57bbbdOzYMS1cuFBer1fDhw/Xpk2b/DcNHzlyROHh/3sDadSoUVq9erUWLFigBx54QIMGDdKGDRt0zTXXSJKOHj2qv/3tb5Kk4cOHB7zW1q1bdcMNNzh9SQAAoB0LyQ3F+fn5ys/PD/rYtm3bzjs2adIkTZo0Kej6AQMGyBjTmtsDAAAW4XdLAQAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKwSkrgpKSnRgAEDFB0drbS0NO3atavZ9evWrdPgwYMVHR2toUOHauPGjQGPG2O0cOFC9enTR127dlVGRoY++ugjJy8BAAB0EI7Hzdq1a1VQUKBFixZpz549GjZsmLKyslRdXR10/Y4dO5STk6O8vDy99957ys7OVnZ2tvbt2+df89hjj+npp5/WypUrVV5erssuu0xZWVk6e/as05cDAADaOcfjZunSpZoxY4Zyc3N19dVXa+XKlYqJidGqVauCrl+2bJnGjRunuXPnKjk5WY8++qiuu+46rVixQtK5d22Ki4u1YMECTZgwQddee61eeuklVVZWasOGDU5fDgAAaOcinHzy+vp6VVRUqLCw0H8sPDxcGRkZ8ng8Qc/xeDwqKCgIOJaVleUPl48//lher1cZGRn+x7t37660tDR5PB7dfvvt5z1nXV2d6urq/F/X1tZKknw+n3w+3/e+Pls0zYBZOIs5hwZzDg3mHBrMOVBL5+Bo3Bw/flwNDQ1KSEgIOJ6QkKBDhw4FPcfr9QZd7/V6/Y83HbvQmu8qKirS4sWLzzu+efNmxcTEtOxiOoHS0tK23kKnwJxDgzmHBnMODeZ8zpkzZ1q0ztG4aS8KCwsD3g2qra1VUlKSMjMzFRsb24Y7ax98Pp9KS0s1duxYuVyutt6OtZhzaDDn0GDOocGcAzV98nIxjsZNz5491aVLF1VVVQUcr6qqktvtDnqO2+1udn3Tf6uqqtSnT5+ANcOHDw/6nFFRUYqKijrvuMvl4pvlW5hHaDDn0GDOocGcQ4M5n9PSGTh6Q3FkZKRSUlJUVlbmP9bY2KiysjKlp6cHPSc9PT1gvXTu7bim9VdeeaXcbnfAmtraWpWXl1/wOQEAQOfh+MdSBQUFmj59ukaMGKHU1FQVFxfr9OnTys3NlSRNmzZNffv2VVFRkSTpvvvu0+jRo/Xkk09q/PjxWrNmjXbv3q3nnntOkhQWFqZf/epX+u1vf6tBgwbpyiuv1EMPPaTExERlZ2c7fTkAAKCdczxubrvtNh07dkwLFy6U1+vV8OHDtWnTJv8NwUeOHFF4+P/eQBo1apRWr16tBQsW6IEHHtCgQYO0YcMGXXPNNf41v/nNb3T69GnNnDlTNTU1uv7667Vp0yZFR0c7fTkAAKCdC8kNxfn5+crPzw/62LZt2847NmnSJE2aNOmCzxcWFqZHHnlEjzzySGttEQAAWILfLQUAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwimNxc+LECU2ZMkWxsbGKi4tTXl6evvrqq2bPOXv2rGbPnq0ePXqoW7dumjhxoqqqqvyPf/DBB8rJyVFSUpK6du2q5ORkLVu2zKlLAAAAHZBjcTNlyhTt379fpaWlev311/X2229r5syZzZ5z//3367XXXtO6dev01ltvqbKyUrfeeqv/8YqKCvXu3Vsvv/yy9u/frwcffFCFhYVasWKFU5cBAAA6mAgnnvTgwYPatGmT3n33XY0YMUKStHz5cv3sZz/TE088ocTExPPOOXnypJ5//nmtXr1aN910kyTphRdeUHJysnbu3KmRI0fqrrvuCjhn4MCB8ng8evXVV5Wfn+/EpQAAgA7GkXduPB6P4uLi/GEjSRkZGQoPD1d5eXnQcyoqKuTz+ZSRkeE/NnjwYPXv318ej+eCr3Xy5EnFx8e33uYBAECH5sg7N16vV7179w58oYgIxcfHy+v1XvCcyMhIxcXFBRxPSEi44Dk7duzQ2rVr9cYbbzS7n7q6OtXV1fm/rq2tlST5fD75fL6LXY71mmbALJzFnEODOYcGcw4N5hyopXO4pLiZP3++lixZ0uyagwcPXspTfm/79u3ThAkTtGjRImVmZja7tqioSIsXLz7v+ObNmxUTE+PUFjuc0tLStt5Cp8CcQ4M5hwZzDg3mfM6ZM2datO6S4mbOnDm68847m10zcOBAud1uVVdXBxz/5ptvdOLECbnd7qDnud1u1dfXq6amJuDdm6qqqvPOOXDggMaMGaOZM2dqwYIFF913YWGhCgoK/F/X1tYqKSlJmZmZio2Nvej5tvP5fCotLdXYsWPlcrnaejvWYs6hwZxDgzmHBnMO1PTJy8VcUtz06tVLvXr1uui69PR01dTUqKKiQikpKZKkLVu2qLGxUWlpaUHPSUlJkcvlUllZmSZOnChJOnz4sI4cOaL09HT/uv379+umm27S9OnT9X//938t2ndUVJSioqLOO+5yufhm+RbmERrMOTSYc2gw59Bgzue0dAaO3FCcnJyscePGacaMGdq1a5feeecd5efn6/bbb/f/pNTRo0c1ePBg7dq1S5LUvXt35eXlqaCgQFu3blVFRYVyc3OVnp6ukSNHSjr3UdSNN96ozMxMFRQUyOv1yuv16tixY05cBgAA6IAcuaFYkl555RXl5+drzJgxCg8P18SJE/X000/7H/f5fDp8+HDA52dPPfWUf21dXZ2ysrL0zDPP+B9fv369jh07ppdfflkvv/yy//gVV1yhTz75xKlLAQAAHYhjcRMfH6/Vq1df8PEBAwbIGBNwLDo6WiUlJSopKQl6zsMPP6yHH364NbcJAAAsw++WAgAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFZxLG5OnDihKVOmKDY2VnFxccrLy9NXX33V7Dlnz57V7Nmz1aNHD3Xr1k0TJ05UVVVV0LVffvml+vXrp7CwMNXU1DhwBQAAoCNyLG6mTJmi/fv3q7S0VK+//rrefvttzZw5s9lz7r//fr322mtat26d3nrrLVVWVurWW28NujYvL0/XXnutE1sHAAAdmCNxc/DgQW3atEm///3vlZaWpuuvv17Lly/XmjVrVFlZGfSckydP6vnnn9fSpUt10003KSUlRS+88IJ27NihnTt3Bqx99tlnVVNTo1//+tdObB8AAHRgEU48qcfjUVxcnEaMGOE/lpGRofDwcJWXl+uWW24575yKigr5fD5lZGT4jw0ePFj9+/eXx+PRyJEjJUkHDhzQI488ovLycv3nP/9p0X7q6upUV1fn/7q2tlaS5PP55PP5vtc12qRpBszCWcw5NJhzaDDn0GDOgVo6B0fixuv1qnfv3oEvFBGh+Ph4eb3eC54TGRmpuLi4gOMJCQn+c+rq6pSTk6PHH39c/fv3b3HcFBUVafHixecd37x5s2JiYlr0HJ1BaWlpW2+hU2DOocGcQ4M5hwZzPufMmTMtWndJcTN//nwtWbKk2TUHDx68lKe8JIWFhUpOTtYdd9xxyecVFBT4v66trVVSUpIyMzMVGxvb2tvscHw+n0pLSzV27Fi5XK623o61mHNoMOfQYM6hwZwDNX3ycjGXFDdz5szRnXfe2eyagQMHyu12q7q6OuD4N998oxMnTsjtdgc9z+12q76+XjU1NQHv3lRVVfnP2bJli/bu3av169dLkowxkqSePXvqwQcfDPrujCRFRUUpKirqvOMul4tvlm9hHqHBnEODOYcGcw4N5nxOS2dwSXHTq1cv9erV66Lr0tPTVVNTo4qKCqWkpEg6FyaNjY1KS0sLek5KSopcLpfKyso0ceJESdLhw4d15MgRpaenS5L+/Oc/6+uvv/af8+677+quu+7S9u3b9cMf/vBSLgUAAFjKkXtukpOTNW7cOM2YMUMrV66Uz+dTfn6+br/9diUmJkqSjh49qjFjxuill15Samqqunfvrry8PBUUFCg+Pl6xsbG65557lJ6e7r+Z+LsBc/z4cf/rffdeHQAA0Dk5EjeS9Morryg/P19jxoxReHi4Jk6cqKefftr/uM/n0+HDhwNuDnrqqaf8a+vq6pSVlaVnnnnGqS0CAAALORY38fHxWr169QUfHzBggP+emSbR0dEqKSlRSUlJi17jhhtuOO85AABA58bvlgIAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVSLaegNtwRgjSaqtrW3jnbQPPp9PZ86cUW1trVwuV1tvx1rMOTSYc2gw59BgzoGa/t1u+nf8Qjpl3Jw6dUqSlJSU1MY7AQAAl+rUqVPq3r37BR8PMxfLHws1NjaqsrJSl19+ucLCwtp6O22utrZWSUlJ+uyzzxQbG9vW27EWcw4N5hwazDk0mHMgY4xOnTqlxMREhYdf+M6aTvnOTXh4uPr169fW22h3YmNj+csTAsw5NJhzaDDn0GDO/9PcOzZNuKEYAABYhbgBAABWIW6gqKgoLVq0SFFRUW29Fasx59BgzqHBnEODOX8/nfKGYgAAYC/euQEAAFYhbgAAgFWIGwAAYBXiBgAAWIW46QROnDihKVOmKDY2VnFxccrLy9NXX33V7Dlnz57V7Nmz1aNHD3Xr1k0TJ05UVVVV0LVffvml+vXrp7CwMNXU1DhwBR2DE3P+4IMPlJOTo6SkJHXt2lXJyclatmyZ05fS7pSUlGjAgAGKjo5WWlqadu3a1ez6devWafDgwYqOjtbQoUO1cePGgMeNMVq4cKH69Omjrl27KiMjQx999JGTl9AhtOacfT6f5s2bp6FDh+qyyy5TYmKipk2bpsrKSqcvo91r7e/nb5s1a5bCwsJUXFzcyrvuYAysN27cODNs2DCzc+dOs337dvOjH/3I5OTkNHvOrFmzTFJSkikrKzO7d+82I0eONKNGjQq6dsKECeanP/2pkWT++9//OnAFHYMTc37++efNvffea7Zt22b+/e9/mz/84Q+ma9euZvny5U5fTruxZs0aExkZaVatWmX2799vZsyYYeLi4kxVVVXQ9e+8847p0qWLeeyxx8yBAwfMggULjMvlMnv37vWv+d3vfme6d+9uNmzYYD744ANz8803myuvvNJ8/fXXobqsdqe151xTU2MyMjLM2rVrzaFDh4zH4zGpqakmJSUllJfV7jjx/dzk1VdfNcOGDTOJiYnmqaeecvhK2jfixnIHDhwwksy7777rP/b3v//dhIWFmaNHjwY9p6amxrhcLrNu3Tr/sYMHDxpJxuPxBKx95plnzOjRo01ZWVmnjhun5/xtd999t7nxxhtbb/PtXGpqqpk9e7b/64aGBpOYmGiKioqCrp88ebIZP358wLG0tDTzy1/+0hhjTGNjo3G73ebxxx/3P15TU2OioqLMH//4RweuoGNo7TkHs2vXLiPJfPrpp62z6Q7IqTl//vnnpm/fvmbfvn3miiuu6PRxw8dSlvN4PIqLi9OIESP8xzIyMhQeHq7y8vKg51RUVMjn8ykjI8N/bPDgwerfv788Ho//2IEDB/TII4/opZdeavYXmHUGTs75u06ePKn4+PjW23w7Vl9fr4qKioAZhYeHKyMj44Iz8ng8AeslKSsry7/+448/ltfrDVjTvXt3paWlNTt3mzkx52BOnjypsLAwxcXFtcq+Oxqn5tzY2KipU6dq7ty5GjJkiDOb72A6979InYDX61Xv3r0DjkVERCg+Pl5er/eC50RGRp73P6CEhAT/OXV1dcrJydHjjz+u/v37O7L3jsSpOX/Xjh07tHbtWs2cObNV9t3eHT9+XA0NDUpISAg43tyMvF5vs+ub/nspz2k7J+b8XWfPntW8efOUk5PTaX8BpFNzXrJkiSIiInTvvfe2/qY7KOKmg5o/f77CwsKa/XPo0CHHXr+wsFDJycm64447HHuN9qCt5/xt+/bt04QJE7Ro0SJlZmaG5DWB1uDz+TR58mQZY/Tss8+29XasUlFRoWXLlunFF19UWFhYW2+n3Yho6w3g+5kzZ47uvPPOZtcMHDhQbrdb1dXVAce/+eYbnThxQm63O+h5brdb9fX1qqmpCXhXoaqqyn/Oli1btHfvXq1fv17SuZ8+kaSePXvqwQcf1OLFi7/nlbUvbT3nJgcOHNCYMWM0c+ZMLViw4HtdS0fUs2dPdenS5byf1As2oyZut7vZ9U3/raqqUp8+fQLWDB8+vBV333E4MecmTWHz6aefasuWLZ32XRvJmTlv375d1dXVAe+gNzQ0aM6cOSouLtYnn3zSuhfRUbT1TT9wVtONrrt37/Yfe/PNN1t0o+v69ev9xw4dOhRwo+u//vUvs3fvXv+fVatWGUlmx44dF7zr32ZOzdkYY/bt22d69+5t5s6d69wFtGOpqakmPz/f/3VDQ4Pp27dvszdg/vznPw84lp6eft4NxU888YT/8ZMnT3JDcSvP2Rhj6uvrTXZ2thkyZIiprq52ZuMdTGvP+fjx4wH/L967d69JTEw08+bNM4cOHXLuQto54qYTGDdunPnJT35iysvLzT//+U8zaNCggB9R/vzzz81VV11lysvL/cdmzZpl+vfvb7Zs2WJ2795t0tPTTXp6+gVfY+vWrZ36p6WMcWbOe/fuNb169TJ33HGH+eKLL/x/OtM/FGvWrDFRUVHmxRdfNAcOHDAzZ840cXFxxuv1GmOMmTp1qpk/f75//TvvvGMiIiLME088YQ4ePGgWLVoU9EfB4+LizF//+lfz4YcfmgkTJvCj4K085/r6enPzzTebfv36mffffz/g+7eurq5NrrE9cOL7+bv4aSniplP48ssvTU5OjunWrZuJjY01ubm55tSpU/7HP/74YyPJbN261X/s66+/Nnfffbf5wQ9+YGJiYswtt9xivvjiiwu+BnHjzJwXLVpkJJ3354orrgjhlbW95cuXm/79+5vIyEiTmppqdu7c6X9s9OjRZvr06QHr//SnP5kf//jHJjIy0gwZMsS88cYbAY83Njaahx56yCQkJJioqCgzZswYc/jw4VBcSrvWmnNu+n4P9ufbfwc6o9b+fv4u4saYMGP+/80SAAAAFuCnpQAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFb5f8IuAFJ59SyiAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lst_loss_train)\n",
    "plt.plot(lst_loss_val)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Morse_decoder_V2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
