{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d761d8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN размерность выхода: torch.Size([1, 32, 16, 120])\n",
      "CNN число фичей: 512\n",
      "Проекция из 512 в 512\n",
      "\n",
      "MorseNet - инициалицация модели. Число обучаемых параметров: 3,333,901\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path as pt\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchaudio import transforms\n",
    "import torchaudio.transforms as T\n",
    "from torchvision.transforms import v2\n",
    "# from Moduls.MosreDataset import MosreDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.nn.parallel import DataParallel\n",
    "\n",
    "DIVICE = torch.device(\"cuda\")\n",
    "\n",
    "MAIN = pt(os.getcwd())\n",
    "DATASET_PATCH = MAIN / 'morse_dataset'\n",
    "AUDIO_FILES = DATASET_PATCH / 'morse_dataset'\n",
    "\n",
    "# Поятоянные значения выявленные в процессе анализа\n",
    "MORSEALP = \"АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ 1234567890#\"\n",
    "MAX_TIME = 48\n",
    "SAMPLE_RATE = 8000\n",
    "N_MELS = 128\n",
    "N_FFT = 400\n",
    "HOP_LENGTH = 180\n",
    "TOP_DB = 80\n",
    "FREQ_MASK = 30\n",
    "TIME_MASK = 40\n",
    "\n",
    "# Гиперпараметы обучения\n",
    "SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001 #2e-4\n",
    "WEIGHT_DECAY = 0.0001\n",
    "# int_to_alph = dict(enumerate(MORSEALP))\n",
    "# alph_to_int = {char:enum for enum, char in int_to_alph.items()}\n",
    "\n",
    "#===== Import data =====\n",
    "train_data = pd.read_csv(pt.joinpath(DATASET_PATCH,'train.csv'))\n",
    "test_data = pd.read_csv(pt.joinpath(DATASET_PATCH,'test.csv'))\n",
    "sample_data = pd.read_csv(pt.joinpath(DATASET_PATCH,'sample_submission.csv'))\n",
    "\n",
    "class MorseDataset(Dataset):\n",
    "    def __init__(self, df, audio_dir, char_map, audio_transform=None, is_test=False):\n",
    "        self.df = df; \n",
    "        self.audio_dir = audio_dir; \n",
    "        self.char_map = char_map\n",
    "        self.audio_transform = audio_transform.cpu() if audio_transform is not None else None\n",
    "        self.is_test = is_test; \n",
    "        self.int_to_char = {v: k for k, v in char_map.items()}\n",
    "    def __len__(self): \n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        file_id = row['id']\n",
    "        audio_path = os.path.join(self.audio_dir, file_id)\n",
    "        # print(audio_path)\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        if sr != SAMPLE_RATE:\n",
    "            waveform = T.Resample(sr, SAMPLE_RATE)(waveform)\n",
    "\n",
    "        current_transform = self.audio_transform\n",
    "        spectrogram = current_transform(waveform).squeeze(0).unsqueeze(0) # (1, M, T)\n",
    "        if not self.is_test:\n",
    "            text = row['message']; \n",
    "            target = torch.tensor([self.char_map[char] for char in text], dtype=torch.long); \n",
    "            target_len = torch.tensor(len(target), dtype=torch.long)\n",
    "            # Возвращаем spec_length, хотя он будет одинаковым для трейна\n",
    "            return spectrogram, target, spectrogram.shape[-1], target_len\n",
    "        else: \n",
    "            return spectrogram, spectrogram.shape[-1], file_id\n",
    "\n",
    "\n",
    "    \n",
    "FIRST_FE_COUNT = 16\n",
    "SECOND_FE_COUNT = 32\n",
    "THIRD_FE_COUNT = 32\n",
    "QAD_FE_COUNT = 32\n",
    "PADDING = 'same'\n",
    "MAXPOOL_KERNEL = 2\n",
    "KERTNEL_SIZE = 3\n",
    "NERON_COUNT = 128\n",
    "GRU_HIDEN = 256\n",
    "# Start with 4 transforms\n",
    "from collections import Counter, defaultdict\n",
    "all_chars = Counter(\"\".join(train_data['message']))\n",
    "\n",
    "BLANK_CHAR = \"_\"\n",
    "vocab_list = sorted(all_chars.keys()) + [BLANK_CHAR]\n",
    "vocab_size = len(vocab_list)\n",
    "char_to_int = {char: i for i, char in enumerate(vocab_list)}\n",
    "int_to_char = {i: char for i, char in enumerate(vocab_list)}\n",
    "BLANK_IDX = char_to_int[BLANK_CHAR]\n",
    "class MorseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, \n",
    "                      out_channels=FIRST_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(FIRST_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((1, 2), (1, 2)), # [batch, FIRST_FE_COUNT = 16, 64, 960]\n",
    "\n",
    "            nn.Conv2d(in_channels=FIRST_FE_COUNT, \n",
    "                      out_channels=SECOND_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(SECOND_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)), # [batch, SECOND_FE_COUNT = 32, 32, 480]\n",
    "\n",
    "            nn.Conv2d(in_channels=SECOND_FE_COUNT, \n",
    "                      out_channels=THIRD_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(THIRD_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((2, 2), (2, 2)), # [batch, THIRD_FE_COUNT = 32, 16, 240]\n",
    "\n",
    "            nn.Conv2d(in_channels=THIRD_FE_COUNT, \n",
    "                      out_channels=QAD_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(QAD_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)) # [batch=32, QAD_FE_COUNT = 32, 8, 80](что юы сохраниить большще признаков по горизонтали)\n",
    "        )\n",
    "        with torch.no_grad(): \n",
    "            dummy_input = torch.randn(1, 1, N_MELS, 480); \n",
    "            cnn_out = self.net_conv(dummy_input); \n",
    "            self.cnn_output_features = cnn_out.shape[1] * cnn_out.shape[2]\n",
    "        print(f\"CNN размерность выхода: {cnn_out.shape}\"); \n",
    "        print(f\"CNN число фичей: {self.cnn_output_features}\")\n",
    "\n",
    "        # Добавлен лоейный слой и функция активации. Для чего? расписать потом \n",
    "        self.layer1 = nn.Linear(self.cnn_output_features, N_MELS*2); \n",
    "        self.gelu = nn.LeakyReLU()\n",
    "        print(f\"Проекция из {self.cnn_output_features} в {GRU_HIDEN*2}\")\n",
    "        self.rnn = nn.GRU(input_size=N_MELS*2,hidden_size=GRU_HIDEN, num_layers=3 ,bidirectional=True)\n",
    "        \n",
    "        self.embed_dim = GRU_HIDEN * 2\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(self.embed_dim)      \n",
    "        self.layer2 = nn.Linear(self.embed_dim, len(MORSEALP))       \n",
    "        # self.layer3 = nn.Linear(GRU_HIDEN, GRU_HIDEN // 2)       \n",
    "        # self.layer4 = nn.Linear(GRU_HIDEN // 2, 45)             \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net_conv(x)\n",
    "\n",
    "        batch, channels, reduced_mels, reduced_time = x.shape\n",
    "        x = x.permute(0, 3, 1, 2)  # [batch, time, channels, mels]\n",
    "        # В частности, каждый вектор признаков в последовательности признаков генерируется \n",
    "        # слева направо на картах признаков. Это означает, что i-й вектор признаков представляет \n",
    "        # собой объединение столбцов всех карт. \n",
    "        # Таким образом, форма тензора может быть изменена, например, на (размер_пакета, 80, 256)\n",
    "        x = x.reshape(batch, reduced_time, -1)  # to GRU [batch=32, seq_len=80, features=256]\n",
    "        x = self.layer1(x)\n",
    "        x = self.gelu(x)\n",
    "        self.rnn.flatten_parameters()\n",
    "        x = self.rnn(x)\n",
    "        x, _ = x # берем информацию со всез состояний\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.layer2(x)\n",
    "        x = nn.functional.log_softmax(x.permute(1, 0, 2), dim=2)\n",
    "        # x = self.layer3(x)\n",
    "        # x = self.layer4(x)\n",
    "        # x = x.log_softmax(dim=2) #log_softmax дает лучше распределение вероятностей чем softmax\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS),\n",
    "    # transforms.AmplitudeToDB(top_db=TOP_DB),\n",
    "    transforms.FrequencyMasking(freq_mask_param=FREQ_MASK),\n",
    "    transforms.TimeMasking(time_mask_param=TIME_MASK),\n",
    "    # v2.RandomCrop((N_MELS, 1920)) # Обрезает последний кадр спектрограммы, в идеале надобы считать а не прописывать число\n",
    "    ) # заметка - Данные трансформации не создают довых обучаемых параметров. Но есть и те что создают. В будущем это стоит учитывать\n",
    "\n",
    "valid_audio_transforms = nn.Sequential(\n",
    "    transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS),\n",
    "    transforms.AmplitudeToDB(top_db=TOP_DB),\n",
    "    v2.CenterCrop((N_MELS, 1920)) \n",
    "    )\n",
    "\n",
    "train_dataframe, val_dataframe = train_test_split(train_data, test_size=0.15, random_state=SEED)\n",
    "\n",
    "train_ds = MorseDataset(df=train_dataframe,\n",
    "                        audio_dir=AUDIO_FILES,\n",
    "                        char_map=char_to_int,\n",
    "                        audio_transform=train_audio_transforms,is_test=True)\n",
    "\n",
    "val_ds = MorseDataset(df=val_dataframe,\n",
    "                        audio_dir=AUDIO_FILES,\n",
    "                        char_map=char_to_int,\n",
    "                        audio_transform=train_audio_transforms)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    valid_batch = [item for item in batch if item[0] is not None]\n",
    "    if not valid_batch: return None, None, None, None, None\n",
    "    is_test_batch = len(valid_batch[0]) == 3\n",
    "    if not is_test_batch: spectrograms, targets, spec_lengths, target_lengths = zip(*valid_batch); ids = None\n",
    "    else: spectrograms, spec_lengths, ids = zip(*valid_batch); targets, target_lengths = None, None\n",
    "    spectrograms_permuted = [s.squeeze(0).permute(1, 0) for s in spectrograms]\n",
    "    spectrograms_padded = nn.utils.rnn.pad_sequence(spectrograms_permuted, batch_first=True, padding_value=0.0)\n",
    "    spectrograms_padded = spectrograms_padded.permute(0, 2, 1).unsqueeze(1)\n",
    "    spec_lengths_tensor = torch.tensor(spec_lengths, dtype=torch.long)\n",
    "    if not is_test_batch:\n",
    "        targets_padded = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=BLANK_IDX)\n",
    "        target_lengths_tensor = torch.tensor(target_lengths, dtype=torch.long)\n",
    "        return spectrograms_padded, spec_lengths_tensor, targets_padded, target_lengths_tensor, None\n",
    "    else: return spectrograms_padded, spec_lengths_tensor, None, None, list(ids)\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "spec, spec_lengths, targets_padded, target_lengths_tensor, _ = next(iter(train_dl))\n",
    "\n",
    "# test_val, val_target, _ = next(iter(val_dl))\n",
    "# test_val, val_target = test_val.to(DIVICE), val_target.to(DIVICE)\n",
    "# # test.shape \n",
    "\n",
    "# #===== начало обучения =====\n",
    "model = MorseNet().to(DIVICE)\n",
    "model = DataParallel(model)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=6, verbose=True)\n",
    "loss_func = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True).to(DIVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nMorseNet - инициалицация модели. Число обучаемых параметров: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8817712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 128, 356])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94731e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([89, 32, 45])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model(spec)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be51c697",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m(spec).shape\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model(spec).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaf999d",
   "metadata": {},
   "source": [
    "# Класс модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66eee39",
   "metadata": {},
   "source": [
    "Переменные для обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad235125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = model(test)\n",
    "# a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa7f726",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "422ea6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 1/20 =====\n",
      "Mean grad norm: 0.100248\n",
      "Max grad norm: 2.922087\n",
      "Min grad norm: 0.000000\n",
      "Train Loss: 0.1075\n",
      "Val Loss: 531.0693\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m pr = []\n\u001b[32m     12\u001b[39m train_tqdm = tqdm(train_dl, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mЭпоха \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m [Обучение]\u001b[39m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmel_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_lens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_tqdm\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmel_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_lens\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmel_spec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDIVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDIVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_lens\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDIVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#===== считатем длинну mel_spec для передачи в CTC loss =====\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 79\u001b[39m, in \u001b[36mMosreDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     77\u001b[39m audio_file = \u001b[38;5;28mself\u001b[39m.audio_paths / \u001b[38;5;28mself\u001b[39m.df.id.values[index]\n\u001b[32m     78\u001b[39m waveform = \u001b[38;5;28mself\u001b[39m.change_time(audio_file)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m augmented_spectrogram = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_train:\n\u001b[32m     82\u001b[39m     message = \u001b[38;5;28mself\u001b[39m.messeges[index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\nn\\modules\\container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torchaudio\\transforms\\_transforms.py:1204\u001b[39m, in \u001b[36m_AxisMasking.forward\u001b[39m\u001b[34m(self, specgram, mask_value)\u001b[39m\n\u001b[32m   1200\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.mask_along_axis_iid(\n\u001b[32m   1201\u001b[39m         specgram, \u001b[38;5;28mself\u001b[39m.mask_param, mask_value, \u001b[38;5;28mself\u001b[39m.axis + specgram.dim() - \u001b[32m3\u001b[39m, p=\u001b[38;5;28mself\u001b[39m.p\n\u001b[32m   1202\u001b[39m     )\n\u001b[32m   1203\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1204\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmask_along_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspecgram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmask_param\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecgram\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torchaudio\\functional\\functional.py:949\u001b[39m, in \u001b[36mmask_along_axis\u001b[39m\u001b[34m(specgram, mask_param, mask_value, axis, p)\u001b[39m\n\u001b[32m    946\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask_end - mask_start >= mask_param:\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNumber of columns to be masked should be less than mask_param\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m949\u001b[39m specgram = \u001b[43mspecgram\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[38;5;66;03m# unpack batch\u001b[39;00m\n\u001b[32m    952\u001b[39m specgram = specgram.reshape(shape[:-\u001b[32m2\u001b[39m] + specgram.shape[-\u001b[32m2\u001b[39m:])\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "lst_loss_train = []\n",
    "lst_loss_val = []\n",
    "p = []\n",
    "p_Val = []\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    pr = []\n",
    "\n",
    "    train_tqdm = tqdm(train_dl, desc=f\"Эпоха {epoch+1}/{EPOCHS} [Обучение]\", leave=False)\n",
    "    for mel_spec, labels, label_lens, _ in train_tqdm:\n",
    "        mel_spec, labels, label_lens = mel_spec.to(DIVICE), labels.to(DIVICE), label_lens.to(DIVICE)\n",
    "\n",
    "        #===== считатем длинну mel_spec для передачи в CTC loss =====\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predict = model(mel_spec) # (N=batch,T,C)\n",
    "        pr.append(predict)\n",
    "        N = predict.shape[1]\n",
    "        T = predict.shape[0]\n",
    "        predict_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "\n",
    "        # print(N, T, predict.shape, labels.shape, predict_lengths.shape, label_lens.shape)\n",
    "        # break\n",
    "        loss = loss_func(predict, labels, predict_lengths, label_lens)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        train_loss = epoch_train_loss / len(train_data)\n",
    "\n",
    "    # ======== Валидация ========\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_mel_spec, val_labels, val_label_lensin, _ in tqdm(\n",
    "                                                        val_dl, \n",
    "                                                        desc=f\"Эпоха {epoch+1}/{EPOCHS} [Валидация]\", \n",
    "                                                        leave=False):\n",
    "            val_mel_spec, val_labels, val_label_lensin = val_mel_spec.to(DIVICE), val_labels.to(DIVICE), val_label_lensin.to(DIVICE)\n",
    "            val_predict = model(val_mel_spec)\n",
    "\n",
    "            p_Val.append(val_predict)\n",
    "            val_N = val_predict.shape[1]\n",
    "            val_T = val_predict.shape[0]\n",
    "            predict_val_lengths = torch.full(size=(val_N,), fill_value=val_T, dtype=torch.long)\n",
    "            val_loss += loss_func(val_predict, val_labels, predict_val_lengths, val_label_lensin).item()\n",
    "\n",
    "\n",
    "\n",
    "    lst_loss_train.append(train_loss)\n",
    "    lst_loss_val.append(val_loss)\n",
    "\n",
    "            \n",
    "    print(f\"\\n===== Эпоха {epoch+1}/{EPOCHS} =====\")\n",
    "    # print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    grad_norms = [param.grad.norm().item() for param in model.parameters() if param.grad is not None]\n",
    "    if grad_norms:\n",
    "        print(f\"Mean grad norm: {np.mean(grad_norms):.6f}\")\n",
    "        print(f\"Max grad norm: {np.max(grad_norms):.6f}\")\n",
    "        print(f\"Min grad norm: {np.min(grad_norms):.6f}\")\n",
    "    else:\n",
    "        print(\"No gradients computed yet.\")\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1547a2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lst_loss_train)\n",
    "plt.plot(lst_loss_val)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Morse_decoder_V2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
