{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b8de53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21753</th>\n",
       "      <td>21754.opus</td>\n",
       "      <td>РГШ28Ф</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>252.opus</td>\n",
       "      <td>ЦЧАЩ3ДМ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22941</th>\n",
       "      <td>22942.opus</td>\n",
       "      <td>2 М #ЮНЭОП</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>619.opus</td>\n",
       "      <td>ТЪ5Ю6Ы0БХ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17090</th>\n",
       "      <td>17091.opus</td>\n",
       "      <td>ФТЗСК ЙЭФТ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29802</th>\n",
       "      <td>29803.opus</td>\n",
       "      <td>С ТЪ7Е#Л0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>5391.opus</td>\n",
       "      <td>СИРЙ 3Ю1ЯЫ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>861.opus</td>\n",
       "      <td>Х#ЦБНЙМТРХИ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>15796.opus</td>\n",
       "      <td>ФЛЯ ОСОЩШ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23654</th>\n",
       "      <td>23655.opus</td>\n",
       "      <td>6ЯЕ0Ы10И</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id      message\n",
       "21753  21754.opus       РГШ28Ф\n",
       "251      252.opus      ЦЧАЩ3ДМ\n",
       "22941  22942.opus   2 М #ЮНЭОП\n",
       "618      619.opus    ТЪ5Ю6Ы0БХ\n",
       "17090  17091.opus   ФТЗСК ЙЭФТ\n",
       "...           ...          ...\n",
       "29802  29803.opus    С ТЪ7Е#Л0\n",
       "5390    5391.opus   СИРЙ 3Ю1ЯЫ\n",
       "860      861.opus  Х#ЦБНЙМТРХИ\n",
       "15795  15796.opus    ФЛЯ ОСОЩШ\n",
       "23654  23655.opus     6ЯЕ0Ы10И\n",
       "\n",
       "[24000 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path as pt\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchaudio import transforms\n",
    "from torchvision.transforms import v2\n",
    "# from Moduls.MosreDataset import MosreDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.nn.parallel import DataParallel\n",
    "from collections import Counter\n",
    "\n",
    "DIVICE = torch.device(\"cuda\")\n",
    "\n",
    "MAIN = pt(os.getcwd())\n",
    "DATASET_PATH = MAIN / 'morse_dataset'\n",
    "AUDIO_FILES = DATASET_PATH / 'morse_dataset'\n",
    "SAVE_DIR = MAIN / 'spectrogram'\n",
    "\n",
    "# Поятоянные значения выявленные в процессе анализа\n",
    "MORSEALP = \"АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ 1234567890#\"\n",
    "MAX_TIME = 48\n",
    "SAMPLE_RATE = 8000\n",
    "N_MELS = 128\n",
    "N_FFT = 400\n",
    "HOP_LENGTH = 180\n",
    "TOP_DB = 80\n",
    "FREQ_MASK = 30\n",
    "TIME_MASK = 40\n",
    "\n",
    "# Гиперпараметы обучения\n",
    "SEED = 42\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.0002 #2e-4\n",
    "WEIGHT_DECAY = 0.001\n",
    "# int_to_alph = dict(enumerate(MORSEALP))\n",
    "# alph_to_int = {char:enum for enum, char in int_to_alph.items()}\n",
    "def preload(df, transforms, fodl_name):\n",
    "    specs = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        file_id = row[\"id\"]\n",
    "        audio_path = DATASET_PATH / file_id\n",
    "\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(audio_path)\n",
    "            mel_spec = transforms(waveform)\n",
    "            torch.save(mel_spec, SAVE_DIR /fodl_name/ f\"{file_id}.pt\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка в {file_id}: {e}\")\n",
    "\n",
    "#===== Import data =====\n",
    "train_data = pd.read_csv(pt.joinpath(DATASET_PATH,'train.csv'))\n",
    "test_data = pd.read_csv(pt.joinpath(DATASET_PATH,'test.csv'))\n",
    "sample_data = pd.read_csv(pt.joinpath(DATASET_PATH,'sample_submission.csv'))\n",
    "\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS),\n",
    "    transforms.AmplitudeToDB(top_db=TOP_DB),\n",
    "    transforms.FrequencyMasking(freq_mask_param=FREQ_MASK), \n",
    "    transforms.TimeMasking(time_mask_param=TIME_MASK), \n",
    "    )\n",
    "\n",
    "valid_audio_transforms = nn.Sequential(\n",
    "    transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS),\n",
    "    transforms.AmplitudeToDB(top_db=TOP_DB),\n",
    "    )\n",
    "\n",
    "train_dataframe, val_dataframe = train_test_split(train_data, test_size=0.2, random_state=SEED)\n",
    "train_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4594d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b241ddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(train_ds, \n",
    "                                       batch_size=BATCH_SIZE, \n",
    "                                       shuffle=True, \n",
    "                                       collate_fn=my_collate, \n",
    "                                       drop_last=True,\n",
    "                                       num_workers=2)\n",
    "for i, batch in enumerate(train_dl):\n",
    "    print(f\"Batch {i} загружен\")\n",
    "    if i == 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d761d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path as pt\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchaudio import transforms\n",
    "from torchvision.transforms import v2\n",
    "# from Moduls.MosreDataset import MosreDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.nn.parallel import DataParallel\n",
    "from collections import Counter\n",
    "\n",
    "DIVICE = torch.device(\"cuda\")\n",
    "\n",
    "MAIN = pt(os.getcwd())\n",
    "DATASET_PATH = MAIN / 'morse_dataset'\n",
    "AUDIO_FILES = DATASET_PATH / 'morse_dataset'\n",
    "\n",
    "# Поятоянные значения выявленные в процессе анализа\n",
    "MORSEALP = \"АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ 1234567890#\"\n",
    "MAX_TIME = 48\n",
    "SAMPLE_RATE = 8000\n",
    "N_MELS = 128\n",
    "N_FFT = 400\n",
    "HOP_LENGTH = 180\n",
    "TOP_DB = 80\n",
    "FREQ_MASK = 30\n",
    "TIME_MASK = 40\n",
    "\n",
    "# Гиперпараметы обучения\n",
    "SEED = 42\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.0002 #2e-4\n",
    "WEIGHT_DECAY = 0.001\n",
    "# int_to_alph = dict(enumerate(MORSEALP))\n",
    "# alph_to_int = {char:enum for enum, char in int_to_alph.items()}\n",
    "\n",
    "#===== Import data =====\n",
    "train_data = pd.read_csv(pt.joinpath(DATASET_PATH,'train.csv'))\n",
    "test_data = pd.read_csv(pt.joinpath(DATASET_PATH,'test.csv'))\n",
    "sample_data = pd.read_csv(pt.joinpath(DATASET_PATH,'sample_submission.csv'))\n",
    "\n",
    "all_chars = Counter(\"\".join(train_data['message']))\n",
    "BLANK_CHAR = \"_\"\n",
    "vocab_list = sorted(all_chars.keys()) + [BLANK_CHAR]\n",
    "num_classes = len(vocab_list)\n",
    "char_to_int = {char: i for i, char in enumerate(vocab_list)}\n",
    "int_to_char = {i: char for i, char in enumerate(vocab_list)}\n",
    "BLANK_IDX = char_to_int[BLANK_CHAR]\n",
    "\n",
    "class MosreDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Класс для обработки \n",
    "    \"\"\"\n",
    "    def __init__(self, df, data_patch,char_to_int, train=True, transforms=None, prev_chars = 1):\n",
    "        self.df = df\n",
    "        self.is_train = train\n",
    "\n",
    "        self.data_path = data_patch\n",
    "        self.audio_paths = self.data_path / 'morse_dataset'\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.char_to_int = char_to_int\n",
    "        self.prev_chars = prev_chars\n",
    "\n",
    "        if self.is_train:\n",
    "            self.messeges = self.df.message.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #Получение аугментрованых спектрограмм\n",
    "        try:\n",
    "            audio_file = self.audio_paths / self.df.id.values[index]\n",
    "            waveform, sample_rate = torchaudio.load(audio_file)\n",
    "            augmented_spectrogram = self.transforms(waveform)\n",
    "            spec_lens = augmented_spectrogram.shape[-1]\n",
    "            if self.is_train:\n",
    "                message = self.messeges[index]\n",
    "                #Получение списка индексов секта - как требует CTC los\n",
    "                '''\n",
    "                При обработке dataloader labels будут выравниваться по макс длине для выравнивания батча\n",
    "                Т.е. будет padding 0. что в будующем будет пустым значением для ctc loss\n",
    "                '''\n",
    "                target = torch.tensor([self.char_to_int[char] for char in message], dtype=torch.long); \n",
    "                target_len = torch.tensor(len(target), dtype=torch.long)\n",
    "                return augmented_spectrogram, spec_lens, target ,target_len, message\n",
    "            else:\n",
    "                return augmented_spectrogram,spec_lens, None, None, None\n",
    "        except Exception as ex:\n",
    "            print(f\"Ошибка при загрузке файла {audio_file}: {ex}\")\n",
    "            return ex\n",
    "    \n",
    "FIRST_FE_COUNT = 64\n",
    "SECOND_FE_COUNT = 128\n",
    "THIRD_FE_COUNT = 128\n",
    "QAD_FE_COUNT = 128\n",
    "PADDING = 'same'\n",
    "MAXPOOL_KERNEL = 2\n",
    "KERTNEL_SIZE = 3\n",
    "NERON_COUNT = 128\n",
    "GRU_HIDEN = 512\n",
    "# Start with 4 transforms\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)  # [B, C, 1, 1]\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction),  # [B, C/reduction]\n",
    "            nn.GELU(),\n",
    "            nn.Linear(channels // reduction, channels),  # [B, C]\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, _, _ = x.shape\n",
    "        squeezed = self.squeeze(x).view(B, C)  # [B, C]\n",
    "        weights = self.excitation(squeezed).view(B, C, 1, 1)  # [B, C, 1, 1]\n",
    "        return x * weights # масштабирование\n",
    "    \n",
    "\n",
    "class MorseNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.net_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, \n",
    "                      out_channels=FIRST_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(FIRST_FE_COUNT),\n",
    "            nn.Dropout2d(0.2),  # добавлено\n",
    "            SEBlock(FIRST_FE_COUNT, 4),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((1, 2), (1, 2)), # [batch, FIRST_FE_COUNT = 16, 64, 960]\n",
    "\n",
    "            nn.Conv2d(in_channels=FIRST_FE_COUNT, \n",
    "                      out_channels=SECOND_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(SECOND_FE_COUNT),\n",
    "            nn.Dropout2d(0.2),  # добавлено\n",
    "            nn.GELU(),\n",
    "            SEBlock(SECOND_FE_COUNT, 8),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)), # [batch, SECOND_FE_COUNT = 32, 32, 480]\n",
    "\n",
    "            nn.Conv2d(in_channels=SECOND_FE_COUNT, \n",
    "                      out_channels=THIRD_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(THIRD_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout2d(0.2),  # добавлено\n",
    "            SEBlock(THIRD_FE_COUNT, 16),\n",
    "            nn.MaxPool2d((2, 2), (2, 2)), # [batch, THIRD_FE_COUNT = 32, 16, 240]\n",
    "\n",
    "            nn.Conv2d(in_channels=THIRD_FE_COUNT, \n",
    "                      out_channels=QAD_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(QAD_FE_COUNT, 16),\n",
    "            nn.Dropout2d(0.2),  # добавлено\n",
    "            nn.GELU(),\n",
    "            SEBlock(QAD_FE_COUNT),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)) # [batch=32, QAD_FE_COUNT = 32, 16, 89](что юы сохраниить большще признаков по горизонтали)\n",
    "        )\n",
    "        with torch.no_grad(): \n",
    "            dummy_input = torch.randn(1, 1, N_MELS, 356); \n",
    "            cnn_out = self.net_conv(dummy_input); \n",
    "            self.cnn_output_features = cnn_out.shape[1] * cnn_out.shape[2]\n",
    "\n",
    "        print(f\"CNN размерность выхода: {cnn_out.shape}\"); \n",
    "        print(f\"CNN число фичей: {self.cnn_output_features}\")\n",
    "\n",
    "        self.layer1 = nn.Linear(self.cnn_output_features, N_MELS*2); \n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "        print(f\"Проекция из {self.cnn_output_features} в {GRU_HIDEN*2}\")\n",
    "        self.rnn = nn.LSTM(\n",
    "                input_size=N_MELS*2,\n",
    "                hidden_size=GRU_HIDEN,\n",
    "                num_layers=3,\n",
    "                bidirectional=True,\n",
    "                dropout=0.5,\n",
    "                batch_first=True \n",
    "            )\n",
    "\n",
    "        self.embed_dim = GRU_HIDEN * 2\n",
    "\n",
    "        self.mha = nn.MultiheadAttention(self.embed_dim, 8, dropout=0.3, batch_first=True)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(self.embed_dim)  \n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)   \n",
    "\n",
    "        self.layer2 = nn.Linear(self.embed_dim, num_classes)     \n",
    "\n",
    "    def _get_output_lengths(self, input_lengths):\n",
    "        output_lengths = torch.floor(input_lengths.float() / 4); \n",
    "        return torch.clamp(output_lengths.long(), min=1).to(DIVICE)\n",
    "    \n",
    "    def forward(self, x, input_lengths):\n",
    "        x = self.net_conv(x)\n",
    "\n",
    "        batch, channels, reduced_mels, reduced_time = x.shape\n",
    "        x = x.permute(0, 3, 1, 2)  # [batch, time, channels, mels]\n",
    "\n",
    "        # В частности, каждый вектор признаков в последовательности признаков генерируется \n",
    "        # слева направо на картах признаков. Это означает, что i-й вектор признаков представляет \n",
    "        # собой объединение столбцов всех карт. \n",
    "        # Таким образом, форма тензора может быть изменена, например, на (размер_пакета, 80, 256)\n",
    "        \n",
    "        x = x.reshape(batch, reduced_time, -1)  # to GRU [batch=32, seq_len=89, features/hiden_dim=512]\n",
    "        x = self.layer1(x)\n",
    "        x = self.gelu(x)\n",
    "        \n",
    "        output_lengths = self._get_output_lengths(input_lengths)\n",
    "        output_lengths.to(DIVICE)\n",
    "        self.rnn.flatten_parameters()\n",
    "        # x = self.layer_norm1(x)\n",
    "        x = self.rnn(x) # [batch=32, seq_len=89, features/hiden_dim=256 * 2]\n",
    "        x, _ = x # берем информацию со всез состояний\n",
    "\n",
    "        #att\n",
    "        max_len = reduced_time; \n",
    "        idx = torch.arange(max_len, device=DIVICE).unsqueeze(0); \n",
    "        key_padding_mask = (idx >= output_lengths.unsqueeze(1))\n",
    "        attenc, _ = self.mha(x, x, x, key_padding_mask=key_padding_mask)\n",
    "        x = x + self.dropout(attenc)\n",
    "        x = self.layer_norm(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = self.layer2(x) # logits - [batch, sequence, num_classes] \n",
    "        x = nn.functional.log_softmax(x.permute(1,0,2), dim=2) # pertime так как CTC loss требует на взод (sequence/T,batch/N,num_classes/C)\n",
    "        '''\n",
    "        по одному прогнозу для каждого из признаков в последовательности, \n",
    "        в итоге получается 89 прогнозов символов для каждой секунды звука.\n",
    "        '''\n",
    "        return x\n",
    "    \n",
    "\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS),\n",
    "    transforms.AmplitudeToDB(top_db=TOP_DB),\n",
    "    transforms.FrequencyMasking(freq_mask_param=FREQ_MASK), \n",
    "    transforms.TimeMasking(time_mask_param=TIME_MASK), \n",
    "    )\n",
    "\n",
    "valid_audio_transforms = nn.Sequential(\n",
    "    transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS),\n",
    "    transforms.AmplitudeToDB(top_db=TOP_DB),\n",
    "    )\n",
    "\n",
    "train_dataframe, val_dataframe = train_test_split(train_data, test_size=0.25, random_state=SEED)\n",
    "\n",
    "train_ds = MosreDataset(df=train_dataframe,\n",
    "                        data_patch=DATASET_PATH,\n",
    "                        char_to_int=char_to_int,\n",
    "                        train=True,\n",
    "                        transforms=train_audio_transforms)\n",
    "\n",
    "val_ds = MosreDataset(df=val_dataframe,\n",
    "                        data_patch=DATASET_PATH,\n",
    "                        char_to_int=char_to_int,\n",
    "                        train=True,\n",
    "                        transforms=valid_audio_transforms)\n",
    "\n",
    "\n",
    "def my_collate(batch):\n",
    "    spectrograms = [item[0].squeeze(0) for item in batch]\n",
    "    # Падинг спектрограмм по максимальной длине\n",
    "    spectrograms_permuted = [s.permute(1, 0) for s in spectrograms]\n",
    "    spectrograms_padded = nn.utils.rnn.pad_sequence(spectrograms_permuted, batch_first=True, padding_value=0.0)\n",
    "    spectrograms_padded = spectrograms_padded.permute(0, 2, 1).unsqueeze(1)\n",
    "    # spec_lens = torch.stack([item[0] for item in batch])\n",
    "    spec_lens = torch.tensor([item[1] for item in batch]).reshape(BATCH_SIZE)\n",
    "    if batch[0][3] is not None:\n",
    "        target = torch.nn.utils.rnn.pad_sequence(\n",
    "                                                [item[2] for item in batch], \n",
    "                                                batch_first=True, \n",
    "                                                padding_value=BLANK_IDX)# выравнивает последовательность до макс \n",
    "                                                                        # длины в батче заполняя пропуски нулем\n",
    "        label_len = torch.stack([item[3] for item in batch])\n",
    "        msg = [item[4] for item in batch]\n",
    "        \n",
    "        return [spectrograms_padded, spec_lens, target, label_len, msg]\n",
    "    else: \n",
    "        return [spectrograms_padded, spec_lens]\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, \n",
    "                                       batch_size=BATCH_SIZE, \n",
    "                                       shuffle=True, \n",
    "                                       collate_fn=my_collate, \n",
    "                                       drop_last=True,\n",
    "                                       num_workers=4, pin_memory=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, \n",
    "                                     batch_size=BATCH_SIZE, \n",
    "                                     shuffle=True, \n",
    "                                     collate_fn=my_collate, \n",
    "                                     drop_last=True,\n",
    "                                     num_workers=4, pin_memory=True)\n",
    "\n",
    "test, test_lens, test_target, _, mess = next(iter(train_dl))\n",
    "test, test_target= test.to(DIVICE), test_target.to(DIVICE)\n",
    "\n",
    "test_val, val_lens, val_target, __, val_mess = next(iter(val_dl))\n",
    "test_val, val_target = test_val.to(DIVICE), val_target.to(DIVICE)\n",
    "# test.shape \n",
    "\n",
    "#===== начало обучения =====\n",
    "model = MorseNet(num_classes=num_classes).to(DIVICE)\n",
    "# model = DataParallel(model)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=LEARNING_RATE)  # Было 0.002\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "loss_func = nn.CTCLoss(blank=BLANK_IDX, reduction='mean', zero_infinity=True).to(DIVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nMorseNet - инициалицация модели. Число обучаемых параметров: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bcbbd50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356,\n",
       "        356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356,\n",
       "        356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356,\n",
       "        356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356,\n",
       "        356, 356, 356, 356, 356, 356, 356, 356])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d2fbb29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([89, 64, 45])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model(test, test_lens)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa7f726",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c23dee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "def ctc_decoder(logits, int_char_map, blank_label_idx):\n",
    "    preds = []\n",
    "    logits_cpu = logits.cpu() \n",
    "    max_inds = torch.argmax(logits_cpu.detach(), dim=2).t().numpy() # арзмакс по лагитам и преобразование к словарю\n",
    "    \n",
    "    for ind in max_inds:\n",
    "        merged_inds = []\n",
    "        for idx in ind:\n",
    "            if idx != blank_label_idx: \n",
    "                merged_inds.append(idx)\n",
    "        text = \"\".join([int_char_map.get(i, '?') for i in merged_inds])\n",
    "        preds.append(text)\n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "422ea6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m     optimizer.zero_grad(); \n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m     39\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "lst_loss_train = []\n",
    "lst_loss_val = []\n",
    "best_val_loss = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "\n",
    "    epoch_train_loss = 0.0\n",
    "    train_predicts = []\n",
    "\n",
    "    train_tqdm = tqdm(train_dl, desc=f\"Эпоха {epoch+1}/{EPOCHS} [Обучение]\", leave=False)\n",
    "    for batch_ind, batch in enumerate(train_tqdm):\n",
    "        mel_spec, mel_spec_lens, targets, targets_lens, _ = batch\n",
    "        mel_spec = mel_spec.to(DIVICE)\n",
    "        mel_spec_lens = mel_spec_lens.to(DIVICE)\n",
    "        targets = targets.to(DIVICE)\n",
    "        targets_lens = targets_lens.to(DIVICE)\n",
    "\n",
    "        #===== считатем длинну mel_spec для передачи в CTC loss =====\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predict = model(mel_spec, mel_spec_lens) # (N=batch,T,C)\n",
    "        N = predict.shape[1]\n",
    "        T = predict.shape[0]\n",
    "        predict_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "\n",
    "        try:\n",
    "            loss = loss_func(predict, targets, predict_lengths, targets_lens.reshape(BATCH_SIZE))\n",
    "        except RuntimeError:\n",
    "            print(predict.shape, targets.shape, predict_lengths, targets_lens.reshape(BATCH_SIZE))\n",
    "            continue\n",
    "\n",
    "        if torch.isnan(loss) or torch.isinf(loss): \n",
    "            print(f\"\\nWarning: In batch-{batch_ind} loss train is NaN/Inf: {loss.item()}\"); \n",
    "            optimizer.zero_grad(); \n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "    total_train = epoch_train_loss / len(train_dl)\n",
    "\n",
    "    # ======== Валидация ========\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    total_val = 0\n",
    "    val_predicts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_tqdm = tqdm(val_dl, desc=f\"Эпоха {epoch+1}/{EPOCHS} [Валидация]\", leave=False)\n",
    "        for val_mel_spec, val_spec_lens, val_labels, val_label_lensin, _ in val_tqdm:\n",
    "\n",
    "            val_mel_spec = val_mel_spec.to(DIVICE)\n",
    "            val_spec_lens = val_spec_lens.to(DIVICE)\n",
    "            val_labels = val_labels.to(DIVICE)\n",
    "            val_label_lensin = val_label_lensin.to(DIVICE)\n",
    "\n",
    "            val_predict = model(val_mel_spec, val_spec_lens)\n",
    "            val_N = val_predict.shape[1]\n",
    "            val_T = val_predict.shape[0]\n",
    "\n",
    "            predict_val_lengths = torch.full(size=(val_N,), fill_value=val_T, dtype=torch.long)\n",
    "            val_loss += loss_func(val_predict, val_labels, predict_val_lengths, val_label_lensin).item()\n",
    "\n",
    "    total_val = val_loss / len(val_dl)\n",
    "\n",
    "    lst_loss_train.append(total_train)\n",
    "    lst_loss_val.append(total_val)\n",
    "\n",
    "    scheduler.step(total_val)\n",
    "\n",
    "    print(f\"\\n===== Эпоха {epoch+1}/{EPOCHS} =====\")\\\n",
    "    #===== Инфо про градиенты=====\n",
    "    grad_norms = [param.grad.norm().item() for param in model.parameters() if param.grad is not None]\n",
    "    if grad_norms:\n",
    "        print(f\"Mean grad norm: {np.mean(grad_norms):.6f}\")\n",
    "        print(f\"Max grad norm: {np.max(grad_norms):.6f}\")\n",
    "        print(f\"Min grad norm: {np.min(grad_norms):.6f}\")\n",
    "    else:\n",
    "        print(\"No gradients computed yet.\")\n",
    "    #===== Инфо про шаг обучения и данные по потерям =====\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Current LR: {current_lr:.6f}\")\n",
    "    print(f\"---- Train Loss: {total_train:.4f}\")\n",
    "    print(f\"---- Val Loss: {total_val:.4f}\")\n",
    "    if current_lr <= 1e-6:\n",
    "        print(\"Learning rate достиг минимума 1e-6, остановка обучения\")\n",
    "        break\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca52e14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'MorseNet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1547a2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAGdCAYAAABO2DpVAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAARK9JREFUeJzt3Xl8VPW9//HXmT2TZBKSQEIgbIqsogguuNQVvWJVaqu3altrW9tabLXe3rZ2UflVq7e9t7u1dtPe26LWW62t1w1RsNaFRUFQQZAdAiFAMkkms5/fH2dmkpCFmWSSmSTv5+NxHhMmZ858M180b77fz/l+DdM0TURERESywJbrBoiIiMjQoWAhIiIiWaNgISIiIlmjYCEiIiJZo2AhIiIiWaNgISIiIlmjYCEiIiJZo2AhIiIiWeMY6DeMx+Ps3buX4uJiDMMY6LcXERGRXjBNk6amJqqrq7HZuh+XGPBgsXfvXmpqagb6bUVERCQLdu3axdixY7v9/oAHi+LiYsBqmM/ny9p1I5EIzz//PBdeeCFOpzNr15XMqB/yg/ohP6gf8oP6ITv8fj81NTWp3+PdGfBgkZz+8Pl8WQ8WXq8Xn8+nvzg5pH7ID+qH/KB+yA/qh+w6WhmDijdFREQkaxQsREREJGsULERERCRrFCxEREQkaxQsREREJGsULERERCRrFCxEREQkazIKFnfeeSeGYXQ4pk6d2l9tExERkUEm4wWyZsyYwQsvvNB2AceAr7ElIiIieSrjVOBwOKiqquqPtoiIiMggl3Gw2Lx5M9XV1Xg8HubNm8c999zDuHHjuj0/FAoRCoVSf/b7/YC1xGokEulFk7uWvFY2rymZUz/kB/VDflA/5Af1Q3ak+/kZpmma6V70mWeeobm5mSlTplBbW8vixYvZs2cPGzZs6HZTkjvvvJPFixd3en7JkiV4vd5031pERERyKBAIcM0119DY2NjjXl8ZBYsjNTQ0MH78eH70ox/x2c9+tstzuhqxqKmpob6+PuubkC1dupT58+d3v8mMGcdY/xh4yzCPnZ+195Y2afWD9Dv1Q35QP+QH9UN2+P1+Kioqjhos+lR5WVpaynHHHceWLVu6PcftduN2uzs973Q6+6WDu71u0A9PfAE2PQ0Y8JnnYNypWX9/sfRX/0pm1A/5Qf2QH9QPfZPuZ9endSyam5v54IMPGD16dF8u0/8OfgC/m58IFQAm/PVGCAdy2iwREZGhJqNg8bWvfY0VK1awfft2Xn31VT7ykY9gt9u5+uqr+6t9fbflBfjNuXBgIxSPhk88DsXVcOgDePGuXLdORERkSMkoWOzevZurr76aKVOmcNVVV1FeXs7rr7/OyJEj+6t9vWOaUPs2LL0D/nQlBBth7Cnw+eVw7Plw2c+s817/Jex4NadNFRERGUoyqrF45JFH+qsdfRf0M/rwSuxPPQcfLIPmfW3fm/1JuOS/wJGo9Zg8H2Z/At76I/z1S3DjP8FVmJt2i4iIDCFDY9nMcAuOn07nlGiw7TmnFyadA8d/DGZcAYbR8TUXfR8+eAkOb4MXFsOCHwxok0VERIaioREsXIWYo0+kuW4H3hMuxz7lX2D86W0jFF3xlMBlP4c/XgErH4Bpl8LEswauzSIiIkPQkNndNPbxR3hx+n8Qn38XHHNuz6Ei6djzYc6nra8fuQbWPGTVZ4iIiEivDJlggauod6+78C6rsDPkh7/fDH+41Lo9VURERDI2dIJFb7mL4TPPWjUXTi9s/wfcfzr882cQi+a6dSIiIoOKggWAzQ7zFsGNr8LEsyEahKXfhaduyXXLREREBhUFi/bKJsKnnoRzv2P9ee/anDZHRERksFGwOJJhwIQzra8jWvJbREQkEwoWXXEltnNXsBAREcmIgkVXnAoWIiIivaFg0ZVksNDupyIiIhlRsOiKs8B6jEcgFsltW0RERAYRBYuuJEcsACKtuWuHiIjIIKNg0RWHG4zER6M6CxERkbQpWHTFMFTAKSIi0gsKFt1RAaeIiEjGFCy6kyzgVI2FiIhI2hQsuuMqtB41FSIiIpI2BYvupEYsFCxERETSpWDRHRVvioiIZEzBojsq3hQREcmYgkV3VLwpIiKSMQWL7qh4U0REJGMKFt1R8aaIiEjGFCy6o2AhIiKSMQWL7jgTUyEq3hQREUmbgkV3VLwpIiKSMQWL7qSKN1ty2w4REZFBRMGiOxqxEBERyZiCRXdSK28qWIiIiKRLwaI7qZU3NRUiIiKSLgWL7mgqREREJGMKFt1R8aaIiEjGFCy6oxELERGRjClYdEfFmyIiIhlTsOhO++JN08xtW0RERAYJBYvuJKdCMCEaymlTREREBgsFi+4kRyxAG5GJiIikScGiO3YH2F3W1woWIiIiaVGw6IkKOEVERDKiYNETrb4pIiKSEQWLnrg0YiEiIpIJBYuepBbJ0oiFiIhIOhQseqIaCxERkYwoWPQkVWOhu0JERETSoWDRk9SIhYKFiIhIOhQseqLiTRERkYwMmWDREoryzmEjuxdV8aaIiEhGHLluQDY0BiKc8cMVBEI2rj7cysRRzuxcWMWbIiIiGRkSIxYlXiezxpRgYvDEW3uzd2EVb4qIiGRkSAQLgI/Orgbg8bf2EI9naZtzFW+KiIhkZMgEiwunV+Kxm+xuCPL6toPZuaiKN0VERDIyZIJFgcvOSeXWSMVjq3dn56Kp4k2NWIiIiKRjyAQLgFNHxQF4ZkMt/mCk7xd0FlqPChYiIiJpGVLBYnwRHDOykGAkzlPravt+weSIhYo3RURE0jKkgoVhwEdPsoo4H1uzq+8XVPGmiIhIRoZUsABYeEI1dpvBWzsb2FLX1LeLqXhTREQkI0MuWIwsdnPulJFAFoo4VbwpIiKSkSEXLACunFsDwF/e3EMkFu/9hVS8KSIikpE+BYt7770XwzC45ZZbstSc7Dhv6ijKC13UN4dYselA7y+k4k0REZGM9DpYrFq1igceeIBZs2Zlsz1Z4bTb+MjsMQD8z+s72HqgmeZQtBcXStRYxEIQj2WxhSIiIkNTrzYha25u5tprr+U3v/kNd911V7bblBVXzq3ht69sY8X7Bzjvv1YA4HXZGVXsZtbYUj45bzxzx4/AMHrYETVZvAlWAae7qJ9bLSIiMrj1KlgsWrSISy65hAsuuOCowSIUChEKhVJ/9vv9AEQiESKRLCxilZC8VvJxUrmHz505gRfeq+NAU4iWcIxAOMb2gwG2Hwzwt3V7mVZVzCdPG8els6rwOO2dL2racWBgYBIJNILNnbX2DlVH9oPkhvohP6gf8oP6ITvS/fwM0zQz2rHrkUce4e6772bVqlV4PB7OOeccTjzxRH7yk590ef6dd97J4sWLOz2/ZMkSvF5vF6/oH6EYNIahIWzwZr3B6gMGEdMarSh0mJw9Os551SbOIyaHLll3A454iKXT/4uAe+SAtVdERCSfBAIBrrnmGhobG/H5fN2el1Gw2LVrF3PnzmXp0qWp2oqjBYuuRixqamqor6/vsWGZikQiLF26lPnz5+N0Oo96/uFAmMfW7GHJyl3saQgCMK6sgDs+PI0PTa5Inef48VSMQD2RG/4Bo6Zlrb1DVab9IP1D/ZAf1A/5Qf2QHX6/n4qKiqMGi4ymQtasWUNdXR0nnXRS6rlYLMbLL7/ML37xC0KhEHZ7xykFt9uN2915CsHpdPZLB6d73VElThaddxxfPGcyT729l+8//R47D7Xy2f9+kwXHV/HdD09ndEmBVWcRAKcZAf2FTFt/9a9kRv2QH9QP+UH90DfpfnYZBYvzzz+f9evXd3ju+uuvZ+rUqXzjG9/oFCoGA7vN4PITx3D+tEp+svR9Hnx1O0+v38fyTQe454rjuTy1rHdLbhsqIiIyCGQULIqLi5k5c2aH5woLCykvL+/0/GBT5HbwnQ9P56NzxvKdv25gzY7DfPMv61lQ7cEJWtZbREQkDUNy5c2+mDbax2NfmMfc8SNojcTY4U+UoGj1TRERkaPq1e2m7S1fvjwLzcgvNpvBHZfO4LL7XmFXMxxrR6tvioiIpEEjFt04fmwJV82pIYBVeBpXsBARETkqBYsefO2iKURtHgDWb9+b49aIiIjkPwWLHowsdjN57CgAXt+0h6agVm0TERHpiYLFUUypqQLADAf4xYtbctwaERGR/KZgcRT2xEZkBQT5/T+3sa1e61mIiIh0R8HiaBLBYmKJjUjM5FfLP8hxg0RERPKXgsXRJFbePKbU2rBsd4PuDhEREemOgsXRJIKF27Q2UmsKRnPZGhERkbymYHE0zgIAXAoWIiIiR6VgcTSJEQtnzNorxN+qW05FRES6o2BxNIniTUc8CGjEQkREpCcKFkeTGLGwR60Ri3AsTjASy2WLRERE8paCxdEkgoURbcWwbgzRqIWIiEg3FCyOJlG8aYQDFLmtzWC1tLeIiEjXFCyOJjFiQSSALxEs/BqxEBER6ZKCxdEkijcxY4ywNjrViIWIiEg3FCyOJjliAYx0W0WbqrEQERHpmoLF0didYHMCUOZKBguNWIiIiHRFwSIdiVGLMpcVKDRiISIi0jUFi3Qk6ixKHVag0OqbIiIiXVOwSEfiltMSZyJYaMRCRESkSwoW6XAmRyw0FSIiItITBYt0JIJFsT0ZLDQVIiIi0hUFi3QkpkKKbGFAIxYiIiLdUbBIh6sQgMJEsPBrxEJERKRLChbpSIxYeI0QoBELERGR7ihYpCMZLEhOhWjEQkREpCsKFulwWlMhHtpGLEzTzGWLRERE8pKCRToSIxbueBCAaNwkGInnskUiIiJ5ScEiHYniTUc8iN1mACrgFBER6YqCRToSIxZGJECR2wGozkJERKQrChbpSAQLIgF8BVaw0LLeIiIinSlYpCNRvEmklWK3tYW6bjkVERHpTMEiHckRi3CAYk9ixEI7nIqIiHSiYJEOV3LEIkCxRyMWIiIi3VGwSEf7GguPijdFRES6o2CRjsTupkRa8RVoxEJERKQ7ChbpSAWLthoLjViIiIh0pmCRjq6KNzViISIi0omCRTqSxZvRVorddkAjFiIiIl1RsEhHcsQCKHVaIxUasRAREelMwSIdjnbBwmEFChVvioiIdKZgkQ6bLRUuih3WFIimQkRERDpTsEhXYjqk2GYFCq28KSIi0pmCRboSBZzFtjAAzaEopmnmskUiIiJ5R8EiXYkRi0LDChZxE1rCsVy2SEREJO8oWKQrsUiWywzisBmA6ixERESOpGCRrkSwMCIBLestIiLSDQWLdLna9gvR1ukiIiJdU7BIV7sdTtv2C9GIhYiISHsKFulqvxGZ25oK8avGQkREpAMFi3Q5O0+FaMRCRESkIwWLdCWDRbhFxZsiIiLdULBIV1fFm5oKERER6UDBIl0dijeTIxYKFiIiIu0pWKSrXfGmTzUWIiIiXVKwSFeqxkK3m4qIiHRHwSJdyWARbcWnqRAREZEuKVikK1ljEW6rsfC3asRCRESkPQWLdHVxV4hGLERERDrKKFjcf//9zJo1C5/Ph8/nY968eTzzzDP91bb80n7lTdVYiIiIdCmjYDF27Fjuvfde1qxZw+rVqznvvPO4/PLLeeedd/qrffkjdbtpa2oqpDkcJR43c9goERGR/OLI5ORLL720w5/vvvtu7r//fl5//XVmzJiR1YblndSIRUtqxMI0oSkUpSSxEqeIiMhwl1GwaC8Wi/HYY4/R0tLCvHnzuj0vFAoRCoVSf/b7/QBEIhEikezVKCSvlc1rdmC4cAJmpBU7cVwOG+FonMPNrXh7/SkOPf3eD5IW9UN+UD/kB/VDdqT7+RmmaWY0lr9+/XrmzZtHMBikqKiIJUuWsGDBgm7Pv/POO1m8eHGn55csWYLX683krXPKGW1mwfovAfC3Ex/kW2tcNEcMvj4rypjCHDdORESknwUCAa655hoaGxvx+XzdnpdxsAiHw+zcuZPGxkb+93//l9/+9resWLGC6dOnd3l+VyMWNTU11NfX99iwTEUiEZYuXcr8+fNxOvthaiIaxPkfY633+to25t+/ju0HAyz57MmcPGFE9t9vkOr3fpC0qB/yg/ohP6gfssPv91NRUXHUYJHxIL7L5eLYY48FYM6cOaxatYqf/vSnPPDAA12e73a7cbvdnZ53Op390sH9dV0cDsAATJxmJLXDaWvU1F/ULvRbP0hG1A/5Qf2QH9QPfZPuZ9fndSzi8XiHEYkhyzDAlZjziARSq29qh1MREZE2GY1Y3HbbbVx88cWMGzeOpqYmlixZwvLly3nuuef6q335xVkA4eYjFsnSWhYiIiJJGQWLuro6PvWpT1FbW0tJSQmzZs3iueeeY/78+f3VvvzSYS0LBQsREZEjZRQsfve73/VXOwaHDmtZVACaChEREWlPe4VkwtnVfiEasRAREUlSsMhEu/1CUsWbrRqxEBERSVKwyESHrdM1YiEiInIkBYtMuNrvcGqNWGjrdBERkTYKFploV2Ph04iFiIhIJwoWmehi63QFCxERkTYKFplod7upr8AasdDtpiIiIm0ULDLR4XZTa8QiEI4RjcVz2CgREZH8oWCRidRUSNtdIQDNIU2HiIiIgIJFZtqNWDjtNjxO6+NTnYWIiIhFwSIT7daxAFLTIaqzEBERsShYZKLdtulA6pZTf6tGLEREREDBIjPtbjcFtEiWiIjIERQsMtEpWGiRLBERkfYULDLRbh0LILURmUYsRERELAoWmWh3VwhoxEJERORIChaZaLdtOoCvQHeFiIiItKdgkYkjaiySd4U0tipYiIiIgIJFZpLBIhaGWJSyQjcAh1rCOWyUiIhI/lCwyERyHQuASICyQhcABxUsREREAAWLzNhdYCQ+skgr5UVWsNCIhYiIiEXBIhOG0aGAMzlicahZwUJERAQULDLXbofT8kSwaApFCUVjOWyUiIhIflCwyFS7tSx8Hid2mwHA4RbdGSIiIqJgkal2UyE2m5GaDqlvDuWwUSIiIvlBwSJTR6xlkZwOUQGniIiIgkXmkiMWYWu/kDIFCxERkRQFi0y5Ou4XorUsRERE2ihYZKrdXSHQfipENRYiIiIKFplyHjlioWW9RUREkhQsMnXkiEVR8q4QBQsREREFi0wdsXW67goRERFpo2CRqU5TIQoWIiIiSQoWmUpOhYQ7ToUc1AJZIiIiChYZO2IqJFm86Q9GicTiuWqViIhIXlCwyNQR61iUFjhJbBfCYU2HiIjIMKdgkakjRiza7xeiRbJERGS4U7DI1BG3m0K71Td1y6mIiAxzChaZOmITMmi/rLcKOEVEZHhTsMiUs9B6bDdiUa7VN0VERAAFi8z1MGKhYCEiIsOdgkWmUtumtxuxKFLxpoiICChYZK6L4s3Ust4q3hQRkWFOwSJTyXUs4hGIRYC2RbJUvCkiIsOdgkWmklMh0Gm/EE2FiIjIcKdgkSm7C4zEx3bE1ukq3hQRkeFOwSJThtHFfiFWsGgIRIhqvxARERnGFCx644it00d4XRjJ/UICkRw1SkREJPcULHrjiLUs7DaDEV5Nh4iIiChY9EZqLYuW1FNa1ltERETBond62i9Ea1mIiMgwpmDRG66u9gvRVIiIiIiCRW/0uMOpgoWIiAxfCha90dWy3kXJHU5VYyEiIsOXgkVvHLGOBWgqREREBBQseueIdSxAxZsiIiKgYNE7PexwqhoLEREZzhQseiO1jkVbsCjTfiEiIiIKFr3Sw10hhwNhYnEzF60SERHJuYyCxT333MPJJ59McXExo0aNYuHChWzatKm/2pa/uljHoiyxpLdpQkNAoxYiIjI8ZRQsVqxYwaJFi3j99ddZunQpkUiECy+8kJaWlqO/eCjpYsTCYbdR6nUCmg4REZHhy5HJyc8++2yHPz/00EOMGjWKNWvW8KEPfSirDctrqbtCOgaqskIXDYEIB1vCTM5Bs0RERHIto2BxpMbGRgDKysq6PScUChEKtS0a5ff7AYhEIkQi2dtiPHmtbF6zO4bhxAHEwwFi7d6vzOtkK1DXGCAS8fV7O/LRQPaDdE/9kB/UD/lB/ZAd6X5+hmmavao0jMfjXHbZZTQ0NPDKK690e96dd97J4sWLOz2/ZMkSvF5vb94650b6N3D6Bz+g0VPD8ml3p57/3SYbbx+y8bGJMc6qUgGniIgMHYFAgGuuuYbGxkZ8vu7/8dzrYHHjjTfyzDPP8MorrzB27Nhuz+tqxKKmpob6+voeG5apSCTC0qVLmT9/Pk6nM2vX7YqxeyWOPyzAHDGR6JdWpZ7/zpPv8ujq3Xzl3GP48nnH9Gsb8tVA9oN0T/2QH9QP+UH9kB1+v5+KioqjBoteTYXcdNNNPPXUU7z88ss9hgoAt9uN2+3u9LzT6eyXDu6v63bgKQbAiLR2eK9RPg8ADcHosP/LOyD9IEelfsgP6of8oH7om3Q/u4yChWmafPnLX+aJJ55g+fLlTJw4sVeNG/S6WNIbtMOpiIhIRsFi0aJFLFmyhCeffJLi4mL27dsHQElJCQUFBf3SwLzk6rwJGbQFi0PaL0RERIapjNaxuP/++2lsbOScc85h9OjRqePRRx/tr/blp+Q6FvEIxNqqZMsLk1unK1iIiMjwlPFUiNA2FQLWqIW9BGg/FRLq6lUiIiJDnvYK6Q27C4zER9euzqK8KLlfSIS49gsREZFhSMGiNwwDnJ33CxmR2C8kFjdpbNVCLCIiMvwoWPRWss6i3dbpLocNn8eaXdKdISIiMhwpWPRWFxuRAZQXqYBTRESGLwWL3nIe5ZZTFXCKiMgwpGDRW66eF8mq11oWIiIyDClY9FY3W6dXJKZCahtbj3yFiIjIkKdg0Vvd1FjMGmutabFq2+GBbpGIiEjOKVj0VjfB4vRjygF4a9dhWsOxgW6ViIhITilY9FYX61gAjCvzUl3iIRIzWb3jUA4aJiIikjsKFr3VxToWAIZhMO+YCgBe/eDgQLdKREQkpxQseis1FRLo9K3kdIiChYiIDDcKFr3l7Pp2U4B5iWCxfncD/qCW9hYRkeFDwaK3ulnHAqC6tIAJ5V7iJqzapjoLEREZPhQsequbdSySknUWr2k6REREhhEFi97q5nbTpHmqsxARkWFIwaK3utkrJGneJCtYvFvr57A2JBMRkWFCwaK3eijeBBhZ7GbyqCIA3timUQsRERkeFCx6q5t1LNrTbaciIjLcKFj01lGmQgAtlCUiIsOOgkVvHaV4E+C0SWUYBmypa6auKThADRMREckdBYveciX3Cuk+WJR6XUwf7QN026mIiAwPCha9lRqx6Hodi6RknYWChYiIDAcKFr2VDBbxKMS6X7Y7uZ7Fa1sVLEREZOhTsOitZPEm9FjAefKEMuw2gx0HA+xp6H7aREREZChQsOgtuwsMu/V1D7ecFnuczBpbAsAz62sHomUiIiI5o2DRW4aR1i2nAFfOqQHgvpe20Niq3U5FRGToUrDoizRuOQW4au5YJo8q4nAgwi9f2jIADRMREckNBYu+SDNYOOw2vrVgGgAP/nM7uw71PMIhIiIyWClY9EVqLYuebzkFOGfKSM48toJwLM4PntvUzw0TERHJDQWLvkhzxALAMAy+tWAahgF/X7eXt3Ye7ufGiYiIDDwFi75Is3gzaXq1j4+dNBaAu//vPUzT7K+WiYiI5ISCRV9kMGKR9G8XTqHAaWf1jsM8u2FfPzVMREQkNxQs+iI5YtHDOhZHqirxcMOHJgFw77MbCUfj/dEyERGRnFCw6IsMp0KSvvChSYwsdrPjYIBf6PZTEREZQhQs+qIXUyEAhW4Hd1w6HbAWzVq/uzHbLRMREckJBYu+SAWLzNel+PCsai6ZNZpY3OTWP68lGIlluXEiIiIDT8GiL5LrWIT8vXr59y6fSUWRi811zfz4hfez2DAREZHcULDoi/Jjrcf97/bq5WWFLu65YhYAv3l5K2t2HMpWy0RERHJCwaIvqk+yHve9DbHebS42f3olV5w0hrgJX3vsbVrDmhIREZHBS8GiL8omgbsEokGoe6/Xl7nj0hlU+Txsq2/hP57dmMUGioiIDCwFi76w2aD6ROvrvW/2+jIlBU7+42PWlMhDr25n6bv7s9A4ERGRgadg0VdjEtMhe3ofLADOPm4knz59AgBffXQtm/c39bFhIiIiA0/Boq+SdRZ9GLFI+vYl0zh1YhnNoSg3/PdqGgO9q9sQERHJFQWLvkqOWOx/N+OFso7ktNv45bUnMaa0gO0HA9z08JtEY1ryW0REBg8Fi77yjYHCUWDGYN/6Pl+uvMjNrz81hwKnnX9srucHz23KQiNFREQGhoJFXxlG1uoskmZUl/CfV54AwK9f3soTb+3OynVFRET6m4JFNmSxziLpklmjWXTuMQB84y/rWb1di2eJiEj+U7DIhiyPWCT92/wpzJ9eSTga53P/vZoPDjRn9foiIiLZpmCRDckRi4ObIZi9nUptNoOffXw2J9SU0hCI8OkHV3KgKZS164uIiGSbgkU2FJZD6Tjr671rs3rpAped3103l/HlXnYdauWzf1hFIBzN6nuIiIhki4JFtvRDnUVSRZGbh64/hbJCF2/vbuSmJW/pNlQREclLChbZ0k91FkkTKwr57XVzcTtsvLixjm8/sYFY3OyX9xIREektBYtsSY1YvNVvb3HSuBH87OrZGAY8unoXX/ifNbSENC0iIiL5Q8EiW6pPBAxo3AXNB/rtbS6aUcXPr56Ny2Hjhff2c9UDr7GvMdhv7yciIpIJBYtscRdDxXHW1/1QZ9Heh2dV8/ANp1Fe6OKdvX4W3vdP3tmbvbtRREREekvBIpv6uc6ivTnjR/DXRWdw7Kgi9vmDXPmr17TduoiI5JyCRTb1450hXakp8/KXG0/nzGMrCIRj3PDfq/nJC+8TV1GniIjkiIJFNrUfsTAH5pd7SYGTB68/mU+eNh6An7yw2dpyvVVbrouIyMBTsMimyplgc0Cg3iriHCBOu43vLZzJDz82C5fDxrKNdSy875+8v79pwNogIiICvQgWL7/8MpdeeinV1dUYhsFf//rXfmjWIOX0QOUM6+sBqLM40pVza/jLF09nTGkB2+pbWHjfP3n8zd2YAzR6IiIiknGwaGlp4YQTTuC+++7rj/YMfjWnWo9vPADxgV8d8/ixJfztpjM449hyAuEYt/55HZ9+cBW7DgUGvC0iIjL8ZBwsLr74Yu666y4+8pGP9Ed7Br/TvwzOQtj5Kqx5MCdNKC9y84frT+FrFx6Hy2FjxfsHuPDHL/Pbf2zVap0iItKvHP39BqFQiFCobUdOv98PQCQSIRLJXoFh8lrZvGavFI7Gdu63sT//LcyltxOddAH4qnPSlC+cNYH5U0fynb+9y6rth7nr/97jybV7uOPD0zhhbEm/vGfe9MMwp37ID+qH/KB+yI50Pz/D7MMEvGEYPPHEEyxcuLDbc+68804WL17c6fklS5bg9Xp7+9b5zYxz1ua7KGvZwj7fibwx6atgGDlrTtyE1+sM/rbDRmvMasfMEXEW1MQZU5izZomIyCASCAS45ppraGxsxOfzdXtevweLrkYsampqqK+v77FhmYpEIixdupT58+fjdDqzdt1eO7AJx+/OxYiFiS78NeaMK3LdIuqaQvzn8+/z5LpakjMiF8+o5MvnHcPkUUVZeY+864dhSv2QH9QP+UH9kB1+v5+KioqjBot+nwpxu9243e5Ozzudzn7p4P66bsaqZ8JZX4Pl38fx/Ldg8gVQWJ7TJo0pc/Ljj5/EovOa+emyzfx93V6eeWc/z767nwXHj+bGs49h5pjsTJHkTT8Mc+qH/KB+yA/qh75J97PTOhb96cyvwqjp1roWz92W69akHDuqiJ9fPZtnbzmLi2ZUYprwf2/X8uGfv8Inf/cGr35Qr1tURUSkVzIOFs3Nzaxdu5a1a9cCsG3bNtauXcvOnTuz3bbBz+GCy34Bhg3efhQ+eDHXLepgapWPBz45l2duPouFJ1Zjtxn8Y3M91/zmDRb+8lWeWV+ru0hERCQjGQeL1atXM3v2bGbPng3ArbfeyuzZs7n99tuz3rghYewcOOXz1tfP3w7xWG7b04Vpo3385OOzWf61c/jUvPG4HTbW7Wrgxj+9yfwfreDhlTsJRvKv3SIikn8yDhbnnHMOpml2Oh566KF+aN4QcfY3wFMC+9fD23/OdWu6VVPm5f9dPpN/fvM8bjr3WHweB1vrW7jt8fWc9YOXuO+lLdQ2tua6mSIiksdUYzEQvGVw1r9ZX794F0Ty+5dzRZGbr100hVdvO5/vXDKN0SUeDjSF+OFzm5h3z4tc8ct/8tt/bGVPQ37/HCIiMvAULAbKKV8A31jw77aW+x4EitwOPnfWJF7++rn815UncPKEERgGvLmzgbv+7z3OuPdFLv/FK/x46fus2XGYaGzglzAXEZH80u+3m0qC0wPnfQf++kX4x4/gpE9ZIxmDgNNu46NzxvLROWPZ7w/y7IZ9PL2+lpXbD7FudyPrdjfy02Wb8XkcnH5MOSWtBpPrmpk6uhSbLXcLg4mIyMBTsBhIs66C1+6zai1e/k/4l+/nukUZq/R5uO70CVx3+gTqmoIs33iAFZsP8MrmehpbIzz7zn7AzqM/f5WSAicnjStl7oQyTptUxuyaEQoaIiJDnILFQLLZYf5i+OMVsOo3cOrnYcSEXLeq10YVe7jq5BquOrmGWNxk3e4Glr+3n6fXbGZ3q4PG1ggvbTrAS5sOAFbtxvzplVw0o5LTj6nA5dBMnIjIUKNgMdCOPR8mnQtbX7IKOT/621y3KCvsNoOTxo3g+NFFHBPcxPyLzmPzgVZW7zjM6u2HeGVLPfXNIR5euZOHV+6k2O3g7Ckj+dBxIzn7uJFU+jy5/hFERCQLFCxyYf5ieGA5rH8MZn8CJp2T6xZlndNu44SaUk6oKeWzZ04kHI3z+taDPPfOPp5/dz8HmkI89XYtT71dC8DUqmI+dNxITptUxswxJYwqVtAQERmMFCxyYfQJVvHmm3+AP18HN7wI5cfkulX9yuWw8aHjrBGK710+k7d2NbBiUx0rNtfz9u4GNu5rYuO+Jn798lYAKn1ujh9TwswxJcwaW8LxY0oZWdx5zxkREckvCha5cvEPYP8G2LMGHr4aPrfUWkRrGLDZDOaMH8Gc8SO49cIpHGoJ88qWel5+/wDrdjWw5UAz+/0h9vvreOG9utTrRpd4OH5MCcePKWF6tY8pVcWMKS3AyOGW9CIi0pGCRa44PfDxJfCb86B+E/zvZ+DqR8E+/LqkrNDFZSdUc9kJ1QC0hKK8W+tn/e5GNuxpZP2eRrYcaKa2MUhtY5Dn392fem2xx8GUymKmVBUzaWQRE8q9jC8vpKasALfDnqsfSURk2Bp+v8XySXGVFS5+/y+w5QVYevugvAU12wrdDk6eUMbJE9rW+WgJRXlnr5+3dzewfk8jm/Y1saWumaZg1CoQ3XG4wzUMA6pLChhTWsCYEdZjdWkBNWUFTK3yaVpFRKSfKFjkWvWJ8JFfwWPXwev3waipVv2FdFDodnDKxDJOmdgWNsLROFvrm9lY28Sm/U3sONjC9voAOw620BKOsaeh1Vp2fHvn640qdjO92sf00T6mjfYxsaKQCRWFFLn1n4SISF/o/6L5YMZCOPAtWP59eOpWa22LiR/KdavynsthY2qVj6lVvg7Pm6bJwZYwOw62sPtwK3sbguxpCLDncCs7DgbYdrCFuqYQdZsOsDyxxkZSRZGbiRVexo7wUlLg7HBU+jxMrixiVLFbdR0iIt1QsMgXZ38dDmyEdx6HRz8Bn10KI6fkulWDkmEYVBS5qShyM2d85++3hKJs3NfEu7V+3t3r5/39TWyvb+FgS5j65hD1zSFWbT/c+YUJPo+D4yqLmVxZzDEjC5lQXsiERBjxOFXXISLDm4JFvjAMWHg/+PfArjfgT1fC55ZB0chct2zIKXQ7UneltNfYGmHHwRa21bewrzFIY2ukw7GnwRrx8B+lrqO61MOoYg8ji92MLHYzqtjNCK+LIo+DYo+DYrcTX4EDn8epJc5FZMhRsMgnyTtFfnsBHN4GD38cPv0UOAty3bJhoaTAyayxpcwaW9rtOaFojK0HWnh/fxOb9zezrb6F7Qdb2HEwQHMo2lbXkQaX3UZViYeqEg/VJR4qSzyUFiQCiNtBoduBz+NgbJmX0T6PQoiIDAoKFvmmsAKufcwKF3tWw+Ofhyv/ADbtq5EP3A470xIFn+211XUE2NcYpK4pyIGmkFXL0RSisTVCUzBCUzBKUzBCMBInHIuz81CAnYcCR31fl8PG+DLrVtqxIwpwO2zYbAYOm4HdZlDgtDOuzMu4xO22bv11EZEcUbDIRxWTrZGL/1kI7/0Nln4XLrzLGmuXvNS+riMd4Wic/f4g+/xB9ja0sq/R+ropGKU5GKU5FKUpFKUhEGbP4VbC0Tib65rZXNec1vUrilwUG3ZeaHmb8eVFjB1RQE2Zl/IiFw6bgc2wAonNMCjxOvF5nH358UVEUhQs8tWEM+Dy++DxG+C1X0DgEFz6E3Bo/YWhwOWwUVPmpabMe9Rzo7E4exuCiSmXFmobg0TjJtGYSSweJxo3aQ5F2XHQutX2cCBCfXOYegy2vb0vrfb4PA7GjPAydkQBY0cUMMLrwmm34bQbuBw2nHYbI7wuqkutqZuKQremZkSkSwoW+WzWVRDyw9Nfh3VL4NBW+Nc/qqBzmHHYbYwrt6Y54Oh939gaYet+P08u+ycjJ06l1h9i9+FWdh0K0BCIEDNNYvG2IxSN4w9G8df6ea/Wn1abnHaDUcUenHaDmGkSj0MsbmIYMKWqmFMmlnHqxHKOH1OCy6F5GZHhRMEi3538OSibBH/+NOx63VoC/OqHoWpmrlsmeaqkwMnMMT52VpgsOGsiTmfP0xyBcJQ9h1vZfbiV3YcD7D7cSlMoSjgaJxKzjlAkzsGWMLWNrdQ1hYjEzG6LVGsbg6n1QTxOGyeMLaXS5+mwJsiIQhfjy71MqiikrNCldUFEhhAFi8HgmPPgcy/Aw/9qjVr8/iJrtc5pl+a6ZTIEeF0OJifW5UhHJBbnQFOIff4g8biJzWZgN6x6jXAszrpdDazcdoiV2w9xqCXMG9sO9Xi9kgInk0YWMrrEQzASpyUUJRCO0RKKYhgwobyQiRWFTBpZxMSKQqpKrJESl92Gw27DYTdw2qxpG7vNSDukRGJx6ppClBe6tP6ISBYpWAwWI4+z1rV47DrY9rK1iNb0y+Gie6BkTK5bJ8OI026jOrH3SlfmjB/BZ86ciGmabKlrZt3uRhoC4Q5rghxsDrOtvoU9Da00tkZ4a2cDb3Xzfh8caMmwfQYOm40ijyNRUOtiZJGb8iIX/tYouw5bd+LUNgaJxU0cNoPJlcXMGlPCzLElzKj2UeC0t00XJaaOItE4oVicSNS6oyduQmmBk/IiF+WFbsoKXalpHzPxmmAkRjSeUfNFBj0Fi8HEWwafeByW/T947T5490nYsgzOuQ1O/QLYVdkv+cMwjKOOhLSGY2yrb2FrfTMHmkIUuhx43XYK3Q4KXQ4isbj1/QMtbKtvZmt9Cwebw6kpmrjZ+ZqRmEkkFqM1EuNAU6jHNtptBtG4yXuJ+pJHV+/q08/scdqsEBJr3zAHt69dRnmhmxGFLsoLXZQWOCl0Wz9rkcuB1+3A67LjdtjwONseY3GTQNgawQmEYwQjMcKxOKZJKviYWHcBjSvzMq7My5gR2tlXckvBYrCxO+HC71mFnU/dCrtXwvPfhrVL4MM/hnGn5rqFImkrcNmtzeCqfd2ec8axFd1+Lx43Ccfiibtk4kRiJtF4nEjUxB+McKA5RH1TiPrmMAebQxR7nNSUFTAucUfOyCI3tf4g63c3smFPI2/vaWTTPj+xONht4LDZsCfWCml/h4zLbo1MNAQiHGwJczgQToxQdD080RKK0RJKb82SvjIMGO2zFlwbVexmVLH1WFHsxu1ITB8l1kBx2m14XVaQK0oEnWK3E4/TlpW6l2gsjs0wdAfRMKNgMVhVHQ+feQ7W/tHabr3uHav24oyvwDnfslbxFBnibDYDj61v/zofU1rAmNIC/mVmVa+vEY9bQaYpGMWRmIpx2g3MeIxnn3ueOaefjT9kFcAearGmhQKhKC3hGIFwlOZQjNZwjFA0RigSJxi1RidshoHXZcfrclDgsuN12XHabVZNi80gkW/Y7w+xK7HYWiAcY29jkL2NwV7/PE67QUmBE1+BtcZJodsaPYnHSU0NGQapkOW0WyElHItzuCXM4UCEw4EwTcFo6g6i0YlVZkeXeLAZBs2hKC2hxM8eieJx2Cn1uij1OhnhtYp8nXZbKpjYDOsOqTGlHiaUd1/0G4nFaQ5GcTpsVpBqF2pC0TiHg0H8rVEaWyOYpplYdt9DgUujPNmiYDGY2WzWFutTLrFGLdY9DP/8Kbz/PFzxAIw+IdctFBkWbDYj8UvR1eH5SCSC1wETKwqPendONiRXgN15KECdP2it/OoPUdcU5GBz2BrdSY7qxEwisTit4RjNyYLZcBTTtKaT6pvD1DeH+9ym5B1E6S51n65it4PxFV6qfAX4gxEONlsjU42tkQ7nGQa4HTZiMTuR117o8XojfW5KC5ypkSm3w4bLYU1vtYSszyeQeHTYDLwuawrL63ZQ6LLjsNuwGWA3rCJim0HidmyTmGkF0FjcxGYjtUid3TDAsEJPKGJN4QUjcUzTpKbMm1pxd0KFl5ICF/sTi+rVNgapbWwlEI7h87TbiTkRys6aXIHXlZtf8QoWQ0FhedtdIn+/GQ68Z92WevY34cyvgl3dLDIcZLoC7JFM0yQQjnXagK81HEstIZ/8hWiaJtG4FU6sW5NNHHaDEV4XI7xOShOPoWic2sYg+xK/CPclRlKS0y+FbgeFbjut4RiHAxEaWsM0tFjvG41bdTRx0yRuQigSY/fhVvY2WrdEb9jjZ8OentdeMU0SU1RG4jOyQkSJ1wp6B5pCBCNxmkJRmg5Ee/W59Zc3dzb0+rWv3XaegoVkwdRLoOZUeOoWeO/v8NJd8PYjMO8mOOHj2sxMRHpkGEbiF72j27t+eiOb1wIIRmLsOhRg+8EA+/1BShJ351h3/7jxeRxE4yahSJxQNEZLMMxLL73EZRfPZ0RRAfZ20yOmadIUiqZGdpqC1hou4cTdP+FoHLvNoNBtTUkVJqalTNO0prJCyeLaKNFkQW27MGQzSC2f77BbIxlmu0Xq4qZ1frJgN3nETTP1M+5IbHTY2Bqh0uemurSA0SUeRpcUUOR24A+2hcCGgPVYWuDq4RPsXwoWQ01hBVz1P7D+MXj63+HgFitovHiXdefI3M9aIxwiIoOUx2k/6h1HDjuJ9UmcRArslHusNVPsRxSSGoaBz2PVkhw7qqifWz48aK3docgwrLtGvrohsc5FDQTq4aW74ccz4MmbYNcqa4xQREQkixQshjJ3Mcz7EnxlLXz0d1A1C6Kt8Nb/wO8ugF/Og9d+CS0Hc91SEREZIhQshgO7A47/GHzhZbj+GTjhanAUWEWez90GP5oKf7oK3vxvaD6Q69aKiMggphqL4cQwYPzp1vEv98KG/7XCRO062PycdRg2qDkNpi6AMXOhcjp4SnLdchERGSQULIarglJr59STPwd178HGp+C9p6B2Lex81TqSSsdB5UxrKmXyhVA921pDQ0RE5AgKFgKjplnHh/4dGnfDxqfhg2WwbwP4d0PDTuvY9DSsuBeKR8OUBdbtrRPOAkfubmsSEZH8omAhHZWMhVM/bx0AgUOw/x3r2PkabHkBmmph9e+sw1UEY06CsSdbR+WJOW2+iIjkloKF9MxbBhPPso7TvgjRkLVt+8anrJGNljrrz9teBsAJzHdVYG98yNrqvfxYqJgMFcdZIx1Z2NhIRETyl4KFZMbhhsnzreOSH1t3luxeZR27VkH9Jrzhetj6onW05y6BUVNh5FRr6qV0PHjLrfDiLQdPqWo3REQGOQUL6T2bDSpnWMecTwMQaarn9b/9nnmTK3A0bIX6LXBwMxzaBqFG2PWGdXTFsMGIiTBmTttRdbx2ahURGUQULCS7PCUcKpqCOXsBtN/NMRqC+s1wYKN11L0H/r3Qesiq4wj5wYzDoQ+sY/2frdfZnFA4EtxFVj2HuwhcxdZdLQUjrNGOgjLra5vDmmoxbIABdqd1N0txZS4+CRGRYUnBQgaGww1VM62jK9EwBA5C3Tuw503YswZ2r7aWIm/aC019eO/ScYni0lOg+kQrhLgKraDiKrQCiIiIZIWCheQHhwt8o63j2Aus50zTuv01UA+hZgg3W48hPwQbrJGO1sPWY7AB4jHAtEY+TNM6v35z2+2yG/7S9XvbXdbOr05v22PBiLZpnsoZMHIauLzWdSMBCPoh1ATxCBh2sCUOw25tBOcqHKAPTkQkvyhYSP4yDCitsY7eCvrbRj92r7SmYEJNVuiIR61zYmHrCDZ2fO32f7Rri83aeyXUDGbs6O9bPNq6I6ZsEpQfY42OYFrBxIxb57iLE8WrFdaUTmGF9ZyIyCCmYCFDm8cHx5xrHUeKhq2AEW6BaNAaiYi0Wo/+Wqh7F/ath/0brGma9sEjGTRsTitoxOOJx6h1raZa62gfTtLh9oFvjLWeSMlY8FVb00g2h/VeNjs4PFBclThvTFsYicetdvr3YBzexZhDr2K8b4B3hFWb4vZZ4UVLtItIP1KwkOHL4QJHmTVa0BPThOb9VrBw+6xf5K7C7tfkCByCQ1vh4AdwcItVjBoNWWEkVVyKNZoSqLfOb6m3dp4N+eGA37qNN13uEqtNzfutqRms/7DnAuz4Vefzi6sTt/1Osx6dXuu1TbXQtM86TDNRKJusRSmyXpsc3YlHrUfDbtWo2J3WlJLDbd1GnLyl+GifrYgMOQoWIkdjGNYIQXFVeud7E2Fl7NzM3ifUbN0p07gL/Hus+pKmWohFEr/IE4+RgPXLv3GPdQtv8rAaC0WjiBeP5mBTmHKfB1u4xZr+CTVBuClRDLsXPnixx+ZkReFIKJ9sfR6eRABy+6x6lXis3c8VsdruLbdeUzgSCssToyuJAJcMcjZHoh7GawWZnhZdi4bb7jxqPWzV0BRVWtfv7VL0yfqdSKt1R5Jd/xsVaU//RYjkC3eRtVrpyOPSf02oyQojoSbrF2ZxFdidxCIRXn36aRYsWICt/W2/rQ1wYJM1IlK30XqMhhPBaXRbgLLZEwWzLYnpomasW3hdiSMxSmHGE6MYEesxHLBGaA5stApmWw5YR78xrIBhdx4xImRYASzc3P1LC8qgaJT1epvDuobNYR1m3Ao98ZgVemJha4Qp2GgdqTobwwopyc+toMz6XiwCsQj2aIjT6vZjf/gPYLe3tc3hSgQtn7UwnKfE+swjrdYRbW2blosEE39OPMbjnX8WV6FVcFxQal2vw9elHd8j2W6wPi+7E+zujiHNNK2fIRq0jli4rSjajFuH3Wld01Wshe2kAwULkcHMXQwjp6R/fkEpjDvVOvpbuAXq37emhIINVvgJ+q3pnnDA+iVnd1q1I8mQEjjYFkZaDlrngvULLSkWTk35gAmRFogc+ebtGLa2X7aRVmsZ+njUGsloPdTHH9K0rtdSB/ve7vRdG1AJfbtdeiDZEuEqFmorMj4qw6pl8pRYr49FrNfHwlZoTV7HMKxzDaPdnVSJIGezJ0JqYvQqOZrlLEisX+Nrm5pLnZcIfWbcCkZOj1V/5HBb4bddCLLHY8yt3Yv98b8kRpjaTUmaMetayVopsM6xtTtMsy1gxpLTgO1/jsQdYZiJQBpvd904qcJtEn+P21+7faDt6nrJ17V/facusLUdyZ/t3G9Z/ZIDChYi0j9chVA92zqyLRZtV2zbYv2iSf0POPEva2eBNQXjLun4L+p43JoWad5vBYJIMPHLIGJdNx5t+5976heAs+2XZ/Jf/w63FYSaaqEpUaMSbEj8snCBzUEUG2+/vYFZJ8zCYbO1/Ws/GrKmr4LtjngsMcXjSUzzeNr+7Choe0yNOmD9cktOzbQ2WD9XMPHY2pD4OvEY9Lf9kutOPNIutLVjd3X8pWUYbfU2mG0/Q7ZFAtZn3Ec2YAxAQ58vNXic+VUFCxGRtNkdYPf17n+cNptVv1FYDkzvWzuKRlnH6K6/bUYi7NrzNMfPOmIl2nwRT05lhazRhVjIClYOT7ujhzqWSPCIcBRJTKu4rEd74k6mI//VbSb/Zd/uMOwd/wVv2KzgGG5uu0U83NL2r/r250VDbdM2kcTUTbt/xcdMeOedd5gxfTp2m9FxFCE5OmCzJUYJ6NiuWCQxZeRqG11LLqqXnC5LTp0l3zM5CmPY24IYkJqCSk6XJUdnYpG2u8raj3a0H+Vp/9hBu9vYkwdmTtfSUbAQERmubDaweXq/H48z8do8XzY/Homwre5ppp28AHs+BrwhRhU3IiIikjUKFiIiIpI1ChYiIiKSNQoWIiIikjUKFiIiIpI1ChYiIiKSNQoWIiIikjW9Chb33XcfEyZMwOPxcOqpp7Jy5cpst0tEREQGoYyDxaOPPsqtt97KHXfcwZtvvskJJ5zARRddRF1dXX+0T0RERAaRjIPFj370I2644Qauv/56pk+fzq9+9Su8Xi+///3v+6N9IiIiMohktKR3OBxmzZo13HbbbannbDYbF1xwAa+99lqXrwmFQoRCodSf/X5rt8JIJEIk0tOWhJlJXiub15TMqR/yg/ohP6gf8oP6ITvS/fwyChb19fXEYjEqKzuuC19ZWcnGjRu7fM0999zD4sWLOz3//PPP4/V6M3n7tCxdujTr15TMqR/yg/ohP6gf8oP6oW8CgUBa5/X7JmS33XYbt956a+rPfr+fmpoaLrzwQny+7G3pGolEWLp0KfPnz8epTWZyRv2QH9QP+UH9kB/UD9mRnHE4moyCRUVFBXa7nf3793d4fv/+/VRVVXX5GrfbjdvtTv3ZNE0AWltbs9rBkUiEQCBAa2sr0Wg0a9eVzKgf8oP6IT+oH/KD+iE7Wltbgbbf493JKFi4XC7mzJnDsmXLWLhwIQDxeJxly5Zx0003pXWNpqYmAGpqajJ5axEREckDTU1NlJSUdPv9jKdCbr31Vq677jrmzp3LKaecwk9+8hNaWlq4/vrr03p9dXU1u3btori4GMMwMn37biWnWHbt2pXVKRbJjPohP6gf8oP6IT+oH7LDNE2ampqorq7u8byMg8W//uu/cuDAAW6//Xb27dvHiSeeyLPPPtupoLM7NpuNsWPHZvq2afP5fPqLkwfUD/lB/ZAf1A/5Qf3Qdz2NVCT1qnjzpptuSnvqQ0RERIYP7RUiIiIiWTNkgoXb7eaOO+7ocAeKDDz1Q35QP+QH9UN+UD8MLMM82n0jIiIiImkaMiMWIiIiknsKFiIiIpI1ChYiIiKSNQoWIiIikjVDJljcd999TJgwAY/Hw6mnnsrKlStz3aQh65577uHkk0+muLiYUaNGsXDhQjZt2tThnGAwyKJFiygvL6eoqIiPfvSjnfaYkey69957MQyDW265JfWc+mFg7Nmzh0984hOUl5dTUFDA8ccfz+rVq1PfN02T22+/ndGjR1NQUMAFF1zA5s2bc9jioScWi/Hd736XiRMnUlBQwDHHHMP3vve9DvtaqB8GiDkEPPLII6bL5TJ///vfm++88455ww03mKWlpeb+/ftz3bQh6aKLLjIffPBBc8OGDebatWvNBQsWmOPGjTObm5tT53zxi180a2pqzGXLlpmrV682TzvtNPP000/PYauHtpUrV5oTJkwwZ82aZd58882p59UP/e/QoUPm+PHjzU9/+tPmG2+8YW7dutV87rnnzC1btqTOuffee82SkhLzr3/9q7lu3TrzsssuMydOnGi2trbmsOVDy913322Wl5ebTz31lLlt2zbzscceM4uKisyf/vSnqXPUDwNjSASLU045xVy0aFHqz7FYzKyurjbvueeeHLZq+KirqzMBc8WKFaZpmmZDQ4PpdDrNxx57LHXOe++9ZwLma6+9lqtmDllNTU3m5MmTzaVLl5pnn312KlioHwbGN77xDfPMM8/s9vvxeNysqqoyf/jDH6aea2hoMN1ut/nwww8PRBOHhUsuucT8zGc+0+G5K664wrz22mtN01Q/DKRBPxUSDodZs2YNF1xwQeo5m83GBRdcwGuvvZbDlg0fjY2NAJSVlQGwZs0aIpFIhz6ZOnUq48aNU5/0g0WLFnHJJZd0+LxB/TBQ/va3vzF37lyuvPJKRo0axezZs/nNb36T+v62bdvYt29fh34oKSnh1FNPVT9k0emnn86yZct4//33AVi3bh2vvPIKF198MaB+GEi92iskn9TX1xOLxTptglZZWcnGjRtz1KrhIx6Pc8stt3DGGWcwc+ZMAPbt24fL5aK0tLTDuZWVlezbty8HrRy6HnnkEd58801WrVrV6Xvqh4GxdetW7r//fm699Va+9a1vsWrVKr7yla/gcrm47rrrUp91V/+PUj9kzze/+U38fj9Tp07FbrcTi8W4++67ufbaawHUDwNo0AcLya1FixaxYcMGXnnllVw3ZdjZtWsXN998M0uXLsXj8eS6OcNWPB5n7ty5fP/73wdg9uzZbNiwgV/96ldcd911OW7d8PHnP/+ZP/3pTyxZsoQZM2awdu1abrnlFqqrq9UPA2zQT4VUVFRgt9s7Vbrv37+fqqqqHLVqeLjpppt46qmneOmllxg7dmzq+aqqKsLhMA0NDR3OV59k15o1a6irq+Okk07C4XDgcDhYsWIFP/vZz3A4HFRWVqofBsDo0aOZPn16h+emTZvGzp07AVKftf4f1b/+/d//nW9+85t8/OMf5/jjj+eTn/wkX/3qV7nnnnsA9cNAGvTBwuVyMWfOHJYtW5Z6Lh6Ps2zZMubNm5fDlg1dpmly00038cQTT/Diiy8yceLEDt+fM2cOTqezQ59s2rSJnTt3qk+y6Pzzz2f9+vWsXbs2dcydO5drr7029bX6of+dccYZnW63fv/99xk/fjwAEydOpKqqqkM/+P1+3njjDfVDFgUCAWy2jr/S7HY78XgcUD8MqFxXj2bDI488YrrdbvOhhx4y3333XfPzn/+8WVpaau7bty/XTRuSbrzxRrOkpMRcvny5WVtbmzoCgUDqnC9+8YvmuHHjzBdffNFcvXq1OW/ePHPevHk5bPXw0P6uENNUPwyElStXmg6Hw7z77rvNzZs3m3/6059Mr9dr/vGPf0ydc++995qlpaXmk08+ab799tvm5Zdfrtscs+y6664zx4wZk7rd9PHHHzcrKirMr3/966lz1A8DY0gEC9M0zZ///OfmuHHjTJfLZZ5yyinm66+/nusmDVlAl8eDDz6YOqe1tdX80pe+ZI4YMcL0er3mRz7yEbO2tjZ3jR4mjgwW6oeB8fe//92cOXOm6Xa7zalTp5q//vWvO3w/Ho+b3/3ud83KykrT7Xab559/vrlp06YctXZo8vv95s0332yOGzfO9Hg85qRJk8xvf/vbZigUSp2jfhgY2jZdREREsmbQ11iIiIhI/lCwEBERkaxRsBAREZGsUbAQERGRrFGwEBERkaxRsBAREZGsUbAQERGRrFGwEBERkaxRsBAREZGsUbAQERGRrFGwEBERkaxRsBAREZGs+f+i2m9qFrbhQgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lst_loss_train)\n",
    "plt.plot(lst_loss_val)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45f65295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN размерность выхода: torch.Size([1, 16, 16, 89])\n",
      "CNN число фичей: 256\n",
      "Проекция из 256 в 512\n",
      "Mean accurasu by The Levenshtein in train is : 0.8673547668538801\n",
      "Mean accurasu by The Levenshtein in validate is : 0.9347409836731654\n"
     ]
    }
   ],
   "source": [
    "def ctc_decoder(logits, int_char_map, blank_label_idx):\n",
    "    preds = []\n",
    "    logits_cpu = logits.cpu() \n",
    "    max_inds = torch.argmax(logits_cpu.detach(), dim=2).t().numpy() # арзмакс по лагитам и преобразование к словарю\n",
    "    \n",
    "    for ind in max_inds:\n",
    "        merged_inds = []\n",
    "        for idx in ind:\n",
    "            if idx != blank_label_idx: \n",
    "                merged_inds.append(idx)\n",
    "        text = \"\".join([int_char_map.get(i, '?') for i in merged_inds])\n",
    "        preds.append(text)\n",
    "\n",
    "    return preds\n",
    "\n",
    "model_load = MorseNet(num_classes=num_classes).to(DIVICE)\n",
    "model_load.load_state_dict(torch.load('MorseNet.pth'))\n",
    "model_load.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_mess = []\n",
    "    train_predicts = []\n",
    "    for loader in train_dl:\n",
    "        seq,seq_lens, test_target, _, mess = loader\n",
    "        seq = seq.to(DIVICE)\n",
    "        seq_lens = seq_lens.to(DIVICE)\n",
    "        train_mess.extend(mess)\n",
    "\n",
    "        logits = model_load(seq,seq_lens)\n",
    "        predicted_values = ctc_decoder(logits, int_to_char, BLANK_IDX)\n",
    "        train_predicts.extend(predicted_values)\n",
    "\n",
    "    val_mess = []\n",
    "    val_predicts = []\n",
    "    for loader in val_dl:\n",
    "        seq,seq_lens, test_target, _, mess = loader\n",
    "        seq = seq.to(DIVICE)\n",
    "        seq_lens = seq_lens.to(DIVICE)\n",
    "        val_mess.extend(mess)\n",
    "\n",
    "        logits= model_load(seq,seq_lens)\n",
    "        predicted_values = ctc_decoder(logits, int_to_char, BLANK_IDX)\n",
    "        val_predicts.extend(predicted_values)\n",
    "\n",
    "import Levenshtein\n",
    "\n",
    "mean_acc_test = np.mean([Levenshtein.ratio(test_pred, train_mess[ind]) for ind, test_pred in enumerate(train_predicts)])\n",
    "mean_acc_val = np.mean([Levenshtein.ratio(val_pred, val_mess[ind]) for ind, val_pred in enumerate(val_predicts)])\n",
    "\n",
    "\n",
    "print(f\"Mean accurasu by The Levenshtein in train is : {mean_acc_test}\")\n",
    "print(f\"Mean accurasu by The Levenshtein in validate is : {mean_acc_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49ca828",
   "metadata": {},
   "source": [
    "# Сбор sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85e44a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN размерность выхода: torch.Size([1, 16, 16, 89])\n",
      "CNN число фичей: 256\n",
      "Проекция из 256 в 512\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[128]' is invalid for input of size 8",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     51\u001b[39m     test_predicts = []\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_dl\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_lens\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDIVICE\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mmy_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m     31\u001b[39m spectrograms_padded = spectrograms_padded.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m).unsqueeze(\u001b[32m1\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# spec_lens = torch.stack([item[0] for item in batch])\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m spec_lens = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m*\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[32m0\u001b[39m][\u001b[32m3\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     35\u001b[39m     target = torch.nn.utils.rnn.pad_sequence(\n\u001b[32m     36\u001b[39m                                             [item[\u001b[32m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch], \n\u001b[32m     37\u001b[39m                                             batch_first=\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[32m     38\u001b[39m                                             padding_value=BLANK_IDX)\u001b[38;5;66;03m# выравнивает последовательность до макс \u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: shape '[128]' is invalid for input of size 8"
     ]
    }
   ],
   "source": [
    "def ctc_decoder(logits, int_char_map, blank_label_idx):\n",
    "    preds = []\n",
    "    logits_cpu = logits.cpu() \n",
    "    max_inds = torch.argmax(logits_cpu.detach(), dim=2).t().numpy() # арзмакс по лагитам и преобразование к словарю\n",
    "    \n",
    "    for ind in max_inds:\n",
    "        merged_inds = []\n",
    "        for idx in ind:\n",
    "            if idx != blank_label_idx: \n",
    "                merged_inds.append(idx)\n",
    "        text = \"\".join([int_char_map.get(i, '?') for i in merged_inds])\n",
    "        preds.append(text)\n",
    "\n",
    "    return preds\n",
    "\n",
    "model_load = MorseNet(num_classes=num_classes).to(DIVICE)\n",
    "model_load.load_state_dict(torch.load('MorseNet.pth'))\n",
    "model_load.eval()\n",
    "\n",
    "test_ds = MosreDataset(df=sample_data,\n",
    "                        data_patch=DATASET_PATH,\n",
    "                        char_to_int=char_to_int,\n",
    "                        train=False,\n",
    "                        transforms=valid_audio_transforms)\n",
    "\n",
    "def my_collate(batch):\n",
    "    spectrograms = [item[0].squeeze(0) for item in batch]\n",
    "    # Падинг спектрограмм по максимальной длине\n",
    "    spectrograms_permuted = [s.permute(1, 0) for s in spectrograms]\n",
    "    spectrograms_padded = nn.utils.rnn.pad_sequence(spectrograms_permuted, batch_first=True, padding_value=0.0)\n",
    "    spectrograms_padded = spectrograms_padded.permute(0, 2, 1).unsqueeze(1)\n",
    "    # spec_lens = torch.stack([item[0] for item in batch])\n",
    "    spec_lens = torch.tensor([item[1] for item in batch]).reshape(BATCH_SIZE*2)\n",
    "    if batch[0][3] is not None:\n",
    "        target = torch.nn.utils.rnn.pad_sequence(\n",
    "                                                [item[2] for item in batch], \n",
    "                                                batch_first=True, \n",
    "                                                padding_value=BLANK_IDX)# выравнивает последовательность до макс \n",
    "                                                                        # длины в батче заполняя пропуски нулем\n",
    "        label_len = torch.stack([item[3] for item in batch])\n",
    "        msg = [item[4] for item in batch]\n",
    "        \n",
    "        return [spectrograms_padded, spec_lens, target, label_len, msg]\n",
    "    else: \n",
    "        return [spectrograms_padded, spec_lens]\n",
    "    \n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=BATCH_SIZE*2, shuffle=False, collate_fn=my_collate)\n",
    "model_load.eval()\n",
    "next(iter(test_dl))\n",
    "with torch.no_grad():\n",
    "    test_predicts = []\n",
    "    for loader in test_dl:\n",
    "        seq, seq_lens = loader\n",
    "        seq = seq.to(DIVICE)\n",
    "        seq_lens = seq_lens.to(DIVICE)\n",
    "\n",
    "        logits = model_load(seq,seq_lens)\n",
    "        predicted_values = ctc_decoder(logits, int_to_char, BLANK_IDX)\n",
    "        test_predicts.extend(predicted_values)\n",
    "\n",
    "sample_data.message = test_predicts\n",
    "sample_data.to_csv('sample_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Morse_decoder_V2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
