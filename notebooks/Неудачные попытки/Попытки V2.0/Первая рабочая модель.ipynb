{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d761d8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN размерность выхода: torch.Size([1, 32, 16, 89])\n",
      "CNN число фичей: 512\n",
      "Проекция из 512 в 512\n",
      "\n",
      "MorseNet - инициалицация модели. Число обучаемых параметров: 968,461\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path as pt\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchaudio import transforms\n",
    "from torchvision.transforms import v2\n",
    "# from Moduls.MosreDataset import MosreDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.nn.parallel import DataParallel\n",
    "from collections import Counter\n",
    "\n",
    "DIVICE = torch.device(\"cuda\")\n",
    "\n",
    "MAIN = pt(os.getcwd())\n",
    "DATASET_PATCH = MAIN / 'morse_dataset'\n",
    "AUDIO_FILES = DATASET_PATCH / 'morse_dataset'\n",
    "\n",
    "# Поятоянные значения выявленные в процессе анализа\n",
    "MORSEALP = \"АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ 1234567890#\"\n",
    "MAX_TIME = 48\n",
    "SAMPLE_RATE = 8000\n",
    "N_MELS = 128\n",
    "N_FFT = 400\n",
    "HOP_LENGTH = 180\n",
    "TOP_DB = 80\n",
    "FREQ_MASK = 30\n",
    "TIME_MASK = 40\n",
    "\n",
    "# Гиперпараметы обучения\n",
    "SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.002 #2e-4\n",
    "WEIGHT_DECAY = 0.00001\n",
    "# int_to_alph = dict(enumerate(MORSEALP))\n",
    "# alph_to_int = {char:enum for enum, char in int_to_alph.items()}\n",
    "\n",
    "#===== Import data =====\n",
    "train_data = pd.read_csv(pt.joinpath(DATASET_PATCH,'train.csv'))\n",
    "test_data = pd.read_csv(pt.joinpath(DATASET_PATCH,'test.csv'))\n",
    "sample_data = pd.read_csv(pt.joinpath(DATASET_PATCH,'sample_submission.csv'))\n",
    "\n",
    "all_chars = Counter(\"\".join(train_data['message']))\n",
    "BLANK_CHAR = \"_\"\n",
    "vocab_list = sorted(all_chars.keys()) + [BLANK_CHAR]\n",
    "num_classes = len(vocab_list)\n",
    "char_to_int = {char: i for i, char in enumerate(vocab_list)}\n",
    "int_to_char = {i: char for i, char in enumerate(vocab_list)}\n",
    "BLANK_IDX = char_to_int[BLANK_CHAR]\n",
    "\n",
    "class MosreDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Класс для обработки \n",
    "    \"\"\"\n",
    "    def __init__(self, df, data_patch,char_to_int, train=True, transforms=None, prev_chars = 1):\n",
    "        self.df = df\n",
    "        self.is_train = train\n",
    "\n",
    "        self.data_path = data_patch\n",
    "        self.audio_paths = self.data_path / 'morse_dataset'\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.char_to_int = char_to_int\n",
    "        self.prev_chars = prev_chars\n",
    "\n",
    "        if self.is_train:\n",
    "            self.messeges = self.df.message.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #Получение аугментрованых спектрограмм\n",
    "        try:\n",
    "            audio_file = self.audio_paths / self.df.id.values[index]\n",
    "            waveform, sample_rate = torchaudio.load(audio_file)\n",
    "            augmented_spectrogram = self.transforms(waveform)\n",
    "\n",
    "            if self.is_train:\n",
    "                message = self.messeges[index]\n",
    "                #Получение списка индексов секта - как требует CTC los\n",
    "                '''\n",
    "                При обработке dataloader labels будут выравниваться по макс длине для выравнивания батча\n",
    "                Т.е. будет padding 0. что в будующем будет пустым значением для ctc loss\n",
    "                '''\n",
    "                target = torch.tensor([self.char_to_int[char] for char in message], dtype=torch.long); \n",
    "                target_len = torch.tensor(len(target), dtype=torch.long)\n",
    "                return augmented_spectrogram, target, target_len, message\n",
    "            else:\n",
    "                return augmented_spectrogram\n",
    "        except Exception as ex:\n",
    "            print(str(ex))\n",
    "        \n",
    "    def change_time(self, audio_file, max_len = 384000):\n",
    "        waveform, sample_rate = torchaudio.load(audio_file)\n",
    "        cahanal, sig_len = waveform.shape\n",
    "\n",
    "        if sig_len < max_len:\n",
    "            pad_len = torch.zeros(max_len - sig_len).unsqueeze(0)\n",
    "            waveform = torch.cat([waveform, pad_len], dim=1)\n",
    "\n",
    "        return waveform\n",
    "    \n",
    "FIRST_FE_COUNT = 16\n",
    "SECOND_FE_COUNT = 32\n",
    "THIRD_FE_COUNT = 32\n",
    "QAD_FE_COUNT = 32\n",
    "PADDING = 'same'\n",
    "MAXPOOL_KERNEL = 2\n",
    "KERTNEL_SIZE = 3\n",
    "NERON_COUNT = 128\n",
    "GRU_HIDEN = 256\n",
    "# Start with 4 transforms\n",
    "class MorseNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.net_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, \n",
    "                      out_channels=FIRST_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(FIRST_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((1, 2), (1, 2)), # [batch, FIRST_FE_COUNT = 16, 64, 960]\n",
    "\n",
    "            nn.Conv2d(in_channels=FIRST_FE_COUNT, \n",
    "                      out_channels=SECOND_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(SECOND_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)), # [batch, SECOND_FE_COUNT = 32, 32, 480]\n",
    "\n",
    "            nn.Conv2d(in_channels=SECOND_FE_COUNT, \n",
    "                      out_channels=THIRD_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(THIRD_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((2, 2), (2, 2)), # [batch, THIRD_FE_COUNT = 32, 16, 240]\n",
    "\n",
    "            nn.Conv2d(in_channels=THIRD_FE_COUNT, \n",
    "                      out_channels=QAD_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(QAD_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)) # [batch=32, QAD_FE_COUNT = 32, 8, 80](что юы сохраниить большще признаков по горизонтали)\n",
    "        )\n",
    "        with torch.no_grad(): \n",
    "            dummy_input = torch.randn(1, 1, N_MELS, 356); \n",
    "            cnn_out = self.net_conv(dummy_input); \n",
    "            self.cnn_output_features = cnn_out.shape[1] * cnn_out.shape[2]\n",
    "\n",
    "        print(f\"CNN размерность выхода: {cnn_out.shape}\"); \n",
    "        print(f\"CNN число фичей: {self.cnn_output_features}\")\n",
    "\n",
    "        # Добавлен лоейный слой и функция активации. Для чего? расписать потом \n",
    "        self.layer1 = nn.Linear(self.cnn_output_features, N_MELS*2); \n",
    "        self.gelu = nn.GELU()\n",
    "        print(f\"Проекция из {self.cnn_output_features} в {GRU_HIDEN*2}\")\n",
    "        self.rnn = nn.GRU(\n",
    "                input_size=N_MELS*2,\n",
    "                hidden_size=GRU_HIDEN,\n",
    "                num_layers=1,\n",
    "                bidirectional=True,\n",
    "                batch_first=True \n",
    "            )\n",
    "\n",
    "        \n",
    "        self.embed_dim = GRU_HIDEN * 2\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(self.embed_dim)      \n",
    "        self.layer2 = nn.Linear(self.embed_dim, num_classes)       \n",
    "        # self.layer3 = nn.Linear(GRU_HIDEN, GRU_HIDEN // 2)       \n",
    "        # self.layer4 = nn.Linear(GRU_HIDEN // 2, 45)             \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net_conv(x)\n",
    "\n",
    "        batch, channels, reduced_mels, reduced_time = x.shape\n",
    "        x = x.permute(0, 3, 1, 2)  # [batch, time, channels, mels]\n",
    "\n",
    "        # В частности, каждый вектор признаков в последовательности признаков генерируется \n",
    "        # слева направо на картах признаков. Это означает, что i-й вектор признаков представляет \n",
    "        # собой объединение столбцов всех карт. \n",
    "        # Таким образом, форма тензора может быть изменена, например, на (размер_пакета, 80, 256)\n",
    "        \n",
    "        x = x.reshape(batch, reduced_time, -1)  # to GRU [batch=32, seq_len=89, features/hiden_dim=512]\n",
    "        x = self.layer1(x)\n",
    "        x = self.gelu(x)\n",
    "\n",
    "        self.rnn.flatten_parameters()\n",
    "\n",
    "        x = self.rnn(x) # [batch=32, seq_len=89, features/hiden_dim=256 * 2]\n",
    "        x, _ = x # берем информацию со всез состояний\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.layer2(x) # logits - [batch, sequence, num_classes] \n",
    "        x = nn.functional.log_softmax(x.permute(1,0,2), dim=2) # pertime так как CTC loss требует на взод (sequence/T,batch/N,num_classes/C)\n",
    "        '''\n",
    "        по одному прогнозу для каждого из признаков в последовательности, \n",
    "        в итоге получается 89 прогнозов символов для каждой секунды звука.\n",
    "        '''\n",
    "        return x\n",
    "    \n",
    "\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS),\n",
    "    transforms.AmplitudeToDB(top_db=TOP_DB),\n",
    "    transforms.FrequencyMasking(freq_mask_param=FREQ_MASK),\n",
    "    transforms.TimeMasking(time_mask_param=TIME_MASK),\n",
    "    # v2.RandomCrop((N_MELS, 1920)) # Обрезает последний кадр спектрограммы, в идеале надобы считать а не прописывать число\n",
    "    ) # заметка - Данные трансформации не создают довых обучаемых параметров. Но есть и те что создают. В будущем это стоит учитывать\n",
    "\n",
    "valid_audio_transforms = nn.Sequential(\n",
    "    transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS),\n",
    "    transforms.AmplitudeToDB(top_db=TOP_DB),\n",
    "    # v2.CenterCrop((N_MELS, 1920)) \n",
    "    )\n",
    "\n",
    "train_dataframe, val_dataframe = train_test_split(train_data, test_size=0.15, random_state=SEED)\n",
    "\n",
    "train_ds = MosreDataset(df=train_dataframe,\n",
    "                        data_patch=DATASET_PATCH,\n",
    "                        char_to_int=char_to_int,\n",
    "                        train=True,\n",
    "                        transforms=train_audio_transforms)\n",
    "\n",
    "val_ds = MosreDataset(df=val_dataframe,\n",
    "                        data_patch=DATASET_PATCH,\n",
    "                        char_to_int=char_to_int,\n",
    "                        train=True,\n",
    "                        transforms=valid_audio_transforms)\n",
    "\n",
    "\n",
    "def my_collate(batch):\n",
    "    spectrograms = torch.stack([item[0] for item in batch])\n",
    "    # Бадинг спектрограмм по максимальной длине\n",
    "    spectrograms_permuted = [s.squeeze(0).permute(1, 0) for s in spectrograms]\n",
    "    spectrograms_padded = nn.utils.rnn.pad_sequence(spectrograms_permuted, batch_first=True, padding_value=0.0)\n",
    "    spectrograms_padded = spectrograms_padded.permute(0, 2, 1).unsqueeze(1)\n",
    "\n",
    "    target = torch.nn.utils.rnn.pad_sequence(\n",
    "                                            [item[1] for item in batch], \n",
    "                                            batch_first=True, \n",
    "                                            padding_value=BLANK_IDX) # выравнивает последовательность до макс \n",
    "                                                            # длины в датче заполняя пропуски нулем\n",
    "    label_len = torch.stack([item[2] for item in batch])\n",
    "    msg = [item[3] for item in batch]\n",
    "    \n",
    "    return [spectrograms_padded, target, label_len, msg]\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=my_collate, drop_last=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=my_collate, drop_last=True)\n",
    "\n",
    "test, test_target, _, mess = next(iter(train_dl))\n",
    "test, test_target= test.to(DIVICE), test_target.to(DIVICE)\n",
    "\n",
    "test_val, val_target, __, val_mess = next(iter(val_dl))\n",
    "test_val, val_target = test_val.to(DIVICE), val_target.to(DIVICE)\n",
    "# test.shape \n",
    "\n",
    "#===== начало обучения =====\n",
    "model = MorseNet(num_classes=num_classes).to(DIVICE)\n",
    "model = DataParallel(model)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=6)\n",
    "loss_func = nn.CTCLoss(blank=BLANK_IDX, reduction='mean', zero_infinity=True).to(DIVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nMorseNet - инициалицация модели. Число обучаемых параметров: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08e52f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLANK_CHAR = \"_\"\n",
    "# vocab_list = sorted(all_chars.keys()) + [BLANK_CHAR]\n",
    "# num_classes = len(vocab_list)\n",
    "# char_to_int = {char: i for i, char in enumerate(vocab_list)}\n",
    "# int_to_char = {i: char for i, char in enumerate(vocab_list)}\n",
    "# BLANK_IDX = char_to_int[BLANK_CHAR]\n",
    "# vocab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be51c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a= model(test)\n",
    "# a.shape, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "297d2357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8, 12,  7, 11,  9, 10,  8, 11,  8,  9,  8,  8, 11, 10,  9, 11, 11,  9,\n",
       "         7,  8,  8,  9, 11,  8,  9,  9,  9,  8, 10,  8,  6, 11])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_.reshape(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa77922d",
   "metadata": {},
   "source": [
    "Подсказка по ctc loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "222c0bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\homer\\AppData\\Local\\Temp\\ipykernel_20744\\2733710114.py:15: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  loss.grad\n"
     ]
    }
   ],
   "source": [
    "# Target are to be un-padded\n",
    "T = 50      # Input sequence length\n",
    "C = 20      # Number of classes (including blank)\n",
    "N = 16      # Batch size\n",
    "# Initialize random batch of input vectors, for *size = (T,N,C)\n",
    "input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()\n",
    "input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "# Initialize random batch of targets (0 = blank, 1:C = classes)\n",
    "target_lengths = torch.randint(low=1, high=T, size=(N,), dtype=torch.long)\n",
    "target = torch.randint(low=1, high=C, size=(sum(target_lengths),), dtype=torch.long)\n",
    "ctc_loss = nn.CTCLoss()\n",
    "loss = ctc_loss(input, target, input_lengths, target_lengths)\n",
    "loss.backward()\n",
    "# input.detach().numpy().shape\n",
    "loss.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89dc4bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 16, 20]), torch.Size([334]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape, target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaf999d",
   "metadata": {},
   "source": [
    "# Декодировщик предсказаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad235125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = model(test)\n",
    "# a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa7f726",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "422ea6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 1/20 =====\n",
      "Mean grad norm: 0.104309\n",
      "Max grad norm: 0.658864\n",
      "Min grad norm: 0.000000\n",
      "Train Loss: 0.0578\n",
      "Val Loss: 48.3605\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 2/20 =====\n",
      "Mean grad norm: 0.102349\n",
      "Max grad norm: 0.757571\n",
      "Min grad norm: 0.000000\n",
      "Train Loss: 0.0184\n",
      "Val Loss: 32.9807\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 3/20 =====\n",
      "Mean grad norm: 0.095136\n",
      "Max grad norm: 0.832072\n",
      "Min grad norm: 0.000000\n",
      "Train Loss: 0.0161\n",
      "Val Loss: 31.8272\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 4/20 =====\n",
      "Mean grad norm: 0.107455\n",
      "Max grad norm: 0.636687\n",
      "Min grad norm: 0.000000\n",
      "Train Loss: 0.0151\n",
      "Val Loss: 25.9317\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 5/20 =====\n",
      "Mean grad norm: 0.094243\n",
      "Max grad norm: 0.853069\n",
      "Min grad norm: 0.000000\n",
      "Train Loss: 0.0144\n",
      "Val Loss: 27.9790\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 6/20 =====\n",
      "Mean grad norm: 0.089706\n",
      "Max grad norm: 0.457383\n",
      "Min grad norm: 0.000000\n",
      "Train Loss: 0.0140\n",
      "Val Loss: 25.1614\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 7/20 =====\n",
      "Mean grad norm: 0.082045\n",
      "Max grad norm: 0.479678\n",
      "Min grad norm: 0.000000\n",
      "Train Loss: 0.0136\n",
      "Val Loss: 24.2656\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 8/20 =====\n",
      "Mean grad norm: 0.091411\n",
      "Max grad norm: 0.414358\n",
      "Min grad norm: 0.000000\n",
      "Train Loss: 0.0133\n",
      "Val Loss: 24.2452\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 9/20 =====\n",
      "Mean grad norm: 0.117757\n",
      "Max grad norm: 0.655404\n",
      "Min grad norm: 0.000000\n",
      "Train Loss: 0.0130\n",
      "Val Loss: 23.4414\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 10/20 =====\n",
      "Mean grad norm: 0.091358\n",
      "Max grad norm: 0.622864\n",
      "Min grad norm: 0.000000\n",
      "Train Loss: 0.0129\n",
      "Val Loss: 25.0027\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 11/20 =====\n",
      "Mean grad norm: 0.107993\n",
      "Max grad norm: 0.578055\n",
      "Min grad norm: 0.000000\n",
      "Train Loss: 0.0127\n",
      "Val Loss: 22.7734\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 12/20 =====\n",
      "Mean grad norm: 0.109382\n",
      "Max grad norm: 0.792908\n",
      "Min grad norm: 0.000000\n",
      "Train Loss: 0.0126\n",
      "Val Loss: 21.6865\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 13/20 =====\n",
      "Mean grad norm: 0.090788\n",
      "Max grad norm: 0.685757\n",
      "Min grad norm: 0.000000\n",
      "Train Loss: 0.0125\n",
      "Val Loss: 21.0000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 14/20 =====\n",
      "Mean grad norm: 0.117883\n",
      "Max grad norm: 0.738168\n",
      "Min grad norm: 0.000000\n",
      "Train Loss: 0.0124\n",
      "Val Loss: 22.0391\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 15/20 =====\n",
      "Mean grad norm: 0.106601\n",
      "Max grad norm: 0.805095\n",
      "Min grad norm: 0.000000\n",
      "Train Loss: 0.0123\n",
      "Val Loss: 22.3734\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 16/20 =====\n",
      "Mean grad norm: 0.099476\n",
      "Max grad norm: 0.657725\n",
      "Min grad norm: 0.000000\n",
      "Train Loss: 0.0121\n",
      "Val Loss: 21.2027\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 17/20 =====\n",
      "Mean grad norm: 0.068785\n",
      "Max grad norm: 0.264479\n",
      "Min grad norm: 0.000000\n",
      "Train Loss: 0.0121\n",
      "Val Loss: 21.3559\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 18/20 =====\n",
      "Mean grad norm: 0.114831\n",
      "Max grad norm: 0.664165\n",
      "Min grad norm: 0.000000\n",
      "Train Loss: 0.0121\n",
      "Val Loss: 20.9946\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 19/20 =====\n",
      "Mean grad norm: 0.118237\n",
      "Max grad norm: 0.717334\n",
      "Min grad norm: 0.000000\n",
      "Train Loss: 0.0120\n",
      "Val Loss: 21.8543\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Эпоха 20/20 =====\n",
      "Mean grad norm: 0.092612\n",
      "Max grad norm: 0.233791\n",
      "Min grad norm: 0.000000\n",
      "Train Loss: 0.0121\n",
      "Val Loss: 21.7869\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "lst_loss_train = []\n",
    "lst_loss_val = []\n",
    "p = []\n",
    "p_Val = []\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    pr = []\n",
    "\n",
    "    train_tqdm = tqdm(train_dl, desc=f\"Эпоха {epoch+1}/{EPOCHS} [Обучение]\", leave=False)\n",
    "    for batch_ind, batch in enumerate(train_tqdm):\n",
    "        mel_spec, targets, targets_lens, _ = batch\n",
    "        mel_spec, targets, targets_lens = mel_spec.to(DIVICE), targets.to(DIVICE), targets_lens.to(DIVICE)\n",
    "\n",
    "        #===== считатем длинну mel_spec для передачи в CTC loss =====\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predict = model(mel_spec) # (N=batch,T,C)\n",
    "        pr.append(predict)\n",
    "        N = predict.shape[1]\n",
    "        T = predict.shape[0]\n",
    "        predict_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "\n",
    "    #     print(\"Predict shape:\", predict.shape) # [T, N, C]\n",
    "    #     print(\"Labels shape:\", targets.shape)   # [N, max_label_len]\n",
    "    #     print(\"Predict lengths:\", predict_lengths) # [N]\n",
    "    #     print(\"Target lengths:\", targets_lens.reshape(BATCH_SIZE))   # [N]\n",
    "    #     break\n",
    "    # break\n",
    "        try:\n",
    "            loss = loss_func(predict, targets, predict_lengths, targets_lens.reshape(BATCH_SIZE))\n",
    "        except RuntimeError:\n",
    "            print(predict.shape, targets.shape, predict_lengths, targets_lens.reshape(BATCH_SIZE))\n",
    "            continue\n",
    "        # print(loss)\n",
    "        if torch.isnan(loss) or torch.isinf(loss): \n",
    "            print(f\"\\nWarning: In batch-{batch_ind} loss train is NaN/Inf: {loss.item()}\"); \n",
    "            optimizer.zero_grad(); \n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        train_loss = epoch_train_loss / len(train_data)\n",
    "\n",
    "    # ======== Валидация ========\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_mel_spec, val_labels, val_label_lensin, _ in tqdm(\n",
    "                                                        val_dl, \n",
    "                                                        desc=f\"Эпоха {epoch+1}/{EPOCHS} [Валидация]\", \n",
    "                                                        leave=False):\n",
    "            val_mel_spec, val_labels, val_label_lensin = val_mel_spec.to(DIVICE), val_labels.to(DIVICE), val_label_lensin.to(DIVICE)\n",
    "            val_predict = model(val_mel_spec)\n",
    "\n",
    "            p_Val.append(val_predict)\n",
    "            val_N = val_predict.shape[1]\n",
    "            val_T = val_predict.shape[0]\n",
    "            predict_val_lengths = torch.full(size=(val_N,), fill_value=val_T, dtype=torch.long)\n",
    "            val_loss += loss_func(val_predict, val_labels, predict_val_lengths, val_label_lensin).item()\n",
    "\n",
    "    lst_loss_train.append(train_loss)\n",
    "    lst_loss_val.append(val_loss)\n",
    "\n",
    "    # scheduler.step(val_loss)\n",
    "\n",
    "\n",
    "            \n",
    "    print(f\"\\n===== Эпоха {epoch+1}/{EPOCHS} =====\")\n",
    "    # print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    grad_norms = [param.grad.norm().item() for param in model.parameters() if param.grad is not None]\n",
    "    if grad_norms:\n",
    "        print(f\"Mean grad norm: {np.mean(grad_norms):.6f}\")\n",
    "        print(f\"Max grad norm: {np.max(grad_norms):.6f}\")\n",
    "        print(f\"Min grad norm: {np.min(grad_norms):.6f}\")\n",
    "    else:\n",
    "        print(\"No gradients computed yet.\")\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca52e14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'MorseNet_fish_compit.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1547a2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lst_loss_train)\n",
    "plt.plot(lst_loss_val)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f65295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_decoder(logits, int_char_map, blank_label_idx):\n",
    "    preds = []\n",
    "    logits_cpu = logits.cpu() \n",
    "    max_inds = torch.argmax(logits_cpu.detach(), dim=2).t().numpy() # арзмакс по лагитам и преобразование к словарю\n",
    "    \n",
    "    for ind in max_inds:\n",
    "        merged_inds = []\n",
    "        for idx in ind:\n",
    "            if idx != blank_label_idx: \n",
    "                merged_inds.append(idx)\n",
    "        text = \"\".join([int_char_map.get(i, '?') for i in merged_inds])\n",
    "        preds.append(text)\n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "logit_probs = model(test)\n",
    "predicted_values = ctc_decoder()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Morse_decoder_V2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
