{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d761d8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN размерность выхода: torch.Size([1, 32, 16, 89])\n",
      "CNN число фичей: 512\n",
      "Проекция из 512 в 512\n",
      "\n",
      "MorseNet - инициалицация модели. Число обучаемых параметров: 3,333,901\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path as pt\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchaudio import transforms\n",
    "from torchvision.transforms import v2\n",
    "# from Moduls.MosreDataset import MosreDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.nn.parallel import DataParallel\n",
    "\n",
    "DIVICE = torch.device(\"cuda\")\n",
    "\n",
    "MAIN = pt(os.getcwd())\n",
    "DATASET_PATCH = MAIN / 'morse_dataset'\n",
    "AUDIO_FILES = DATASET_PATCH / 'morse_dataset'\n",
    "\n",
    "# Поятоянные значения выявленные в процессе анализа\n",
    "MORSEALP = \"АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ 1234567890#\"\n",
    "MAX_TIME = 48\n",
    "SAMPLE_RATE = 8000\n",
    "N_MELS = 128\n",
    "N_FFT = 400\n",
    "HOP_LENGTH = 180\n",
    "TOP_DB = 80\n",
    "FREQ_MASK = 30\n",
    "TIME_MASK = 40\n",
    "\n",
    "# Гиперпараметы обучения\n",
    "SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001 #2e-4\n",
    "WEIGHT_DECAY = 0.0001\n",
    "# int_to_alph = dict(enumerate(MORSEALP))\n",
    "# alph_to_int = {char:enum for enum, char in int_to_alph.items()}\n",
    "\n",
    "#===== Import data =====\n",
    "train_data = pd.read_csv(pt.joinpath(DATASET_PATCH,'train.csv'))\n",
    "test_data = pd.read_csv(pt.joinpath(DATASET_PATCH,'test.csv'))\n",
    "sample_data = pd.read_csv(pt.joinpath(DATASET_PATCH,'sample_submission.csv'))\n",
    "\n",
    "class MosreDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Класс для обработки \n",
    "    \"\"\"\n",
    "    def __init__(self, df, data_patch, train=True, transforms=None, prev_chars = 1):\n",
    "        self.df = df\n",
    "        self.is_train = train\n",
    "\n",
    "        self.data_path = data_patch\n",
    "        self.audio_paths = self.data_path / 'morse_dataset'\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.morse_alp = \"АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ 1234567890#\"\n",
    "        self.int_to_alph = dict(enumerate(MORSEALP, start=1)) # 0 - Выводим под пустое\n",
    "        self.alph_to_int = {char:enum+1 for enum, char in self.int_to_alph.items()}\n",
    "        self.prev_chars = prev_chars\n",
    "\n",
    "        if self.is_train:\n",
    "            self.messeges = self.df.message.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #Получение аугментрованых спектрограмм\n",
    "        try:\n",
    "            audio_file = self.audio_paths / self.df.id.values[index]\n",
    "            waveform, sample_rate = torchaudio.load(audio_file)\n",
    "            augmented_spectrogram = self.transforms(waveform)\n",
    "\n",
    "            if self.is_train:\n",
    "                message = self.messeges[index]\n",
    "                #Получение списка индексов секта - как требует CTC los\n",
    "                '''\n",
    "                При обработке dataloader labels будут выравниваться по макс длине для выравнивания батча\n",
    "                Т.е. будет padding 0. что в будующем будет пустым значением для ctc loss\n",
    "                '''\n",
    "                label = torch.LongTensor([self.morse_alp.find(c) + 1 for c in message])\n",
    "                label_len = torch.LongTensor([len(label)])\n",
    "                return augmented_spectrogram, label, label_len, message\n",
    "            else:\n",
    "                return augmented_spectrogram\n",
    "        except Exception as ex:\n",
    "            print(str(ex))\n",
    "        \n",
    "    def change_time(self, audio_file, max_len = 384000):\n",
    "        waveform, sample_rate = torchaudio.load(audio_file)\n",
    "        cahanal, sig_len = waveform.shape\n",
    "\n",
    "        if sig_len < max_len:\n",
    "            pad_len = torch.zeros(max_len - sig_len).unsqueeze(0)\n",
    "            waveform = torch.cat([waveform, pad_len], dim=1)\n",
    "\n",
    "        return waveform\n",
    "    \n",
    "FIRST_FE_COUNT = 16\n",
    "SECOND_FE_COUNT = 32\n",
    "THIRD_FE_COUNT = 32\n",
    "QAD_FE_COUNT = 32\n",
    "PADDING = 'same'\n",
    "MAXPOOL_KERNEL = 2\n",
    "KERTNEL_SIZE = 3\n",
    "NERON_COUNT = 128\n",
    "GRU_HIDEN = 256\n",
    "# Start with 4 transforms\n",
    "class MorseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, \n",
    "                      out_channels=FIRST_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(FIRST_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((1, 2), (1, 2)), # [batch, FIRST_FE_COUNT = 16, 64, 960]\n",
    "\n",
    "            nn.Conv2d(in_channels=FIRST_FE_COUNT, \n",
    "                      out_channels=SECOND_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(SECOND_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)), # [batch, SECOND_FE_COUNT = 32, 32, 480]\n",
    "\n",
    "            nn.Conv2d(in_channels=SECOND_FE_COUNT, \n",
    "                      out_channels=THIRD_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(THIRD_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((2, 2), (2, 2)), # [batch, THIRD_FE_COUNT = 32, 16, 240]\n",
    "\n",
    "            nn.Conv2d(in_channels=THIRD_FE_COUNT, \n",
    "                      out_channels=QAD_FE_COUNT, \n",
    "                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),\n",
    "            nn.BatchNorm2d(QAD_FE_COUNT),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)) # [batch=32, QAD_FE_COUNT = 32, 8, 80](что юы сохраниить большще признаков по горизонтали)\n",
    "        )\n",
    "        with torch.no_grad(): \n",
    "            dummy_input = torch.randn(1, 1, N_MELS, 356); \n",
    "            cnn_out = self.net_conv(dummy_input); \n",
    "            self.cnn_output_features = cnn_out.shape[1] * cnn_out.shape[2]\n",
    "\n",
    "        print(f\"CNN размерность выхода: {cnn_out.shape}\"); \n",
    "        print(f\"CNN число фичей: {self.cnn_output_features}\")\n",
    "\n",
    "        # Добавлен лоейный слой и функция активации. Для чего? расписать потом \n",
    "        self.layer1 = nn.Linear(self.cnn_output_features, N_MELS*2); \n",
    "        self.gelu = nn.LeakyReLU()\n",
    "        print(f\"Проекция из {self.cnn_output_features} в {GRU_HIDEN*2}\")\n",
    "        self.rnn = nn.GRU(input_size=N_MELS*2,hidden_size=GRU_HIDEN, num_layers=3 ,bidirectional=True)\n",
    "        \n",
    "        self.embed_dim = GRU_HIDEN * 2\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(self.embed_dim)      \n",
    "        self.layer2 = nn.Linear(self.embed_dim, len(MORSEALP))       \n",
    "        # self.layer3 = nn.Linear(GRU_HIDEN, GRU_HIDEN // 2)       \n",
    "        # self.layer4 = nn.Linear(GRU_HIDEN // 2, 45)             \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net_conv(x)\n",
    "\n",
    "        batch, channels, reduced_mels, reduced_time = x.shape\n",
    "        x = x.permute(0, 3, 1, 2)  # [batch, time, channels, mels]\n",
    "\n",
    "        # В частности, каждый вектор признаков в последовательности признаков генерируется \n",
    "        # слева направо на картах признаков. Это означает, что i-й вектор признаков представляет \n",
    "        # собой объединение столбцов всех карт. \n",
    "        # Таким образом, форма тензора может быть изменена, например, на (размер_пакета, 80, 256)\n",
    "        \n",
    "        x = x.reshape(batch, reduced_time, -1)  # to GRU [batch=32, seq_len=80, features/hiden_dim=256]\n",
    "        x = self.layer1(x)\n",
    "        x = self.gelu(x)\n",
    "\n",
    "        self.rnn.flatten_parameters()\n",
    "\n",
    "        x = self.rnn(x) # [batch=32, seq_len=80, features/hiden_dim=256 * 2]\n",
    "        x, _ = x # берем информацию со всез состояний\n",
    "        # x = self.layer_norm(x)\n",
    "        # x = self.layer2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS),\n",
    "    transforms.AmplitudeToDB(top_db=TOP_DB),\n",
    "    transforms.FrequencyMasking(freq_mask_param=FREQ_MASK),\n",
    "    transforms.TimeMasking(time_mask_param=TIME_MASK),\n",
    "    # v2.RandomCrop((N_MELS, 1920)) # Обрезает последний кадр спектрограммы, в идеале надобы считать а не прописывать число\n",
    "    ) # заметка - Данные трансформации не создают довых обучаемых параметров. Но есть и те что создают. В будущем это стоит учитывать\n",
    "\n",
    "valid_audio_transforms = nn.Sequential(\n",
    "    transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS),\n",
    "    transforms.AmplitudeToDB(top_db=TOP_DB),\n",
    "    v2.CenterCrop((N_MELS, 1920)) \n",
    "    )\n",
    "\n",
    "train_dataframe, val_dataframe = train_test_split(train_data, test_size=0.15, random_state=SEED)\n",
    "\n",
    "train_ds = MosreDataset(df=train_dataframe,\n",
    "                        data_patch=DATASET_PATCH,\n",
    "                        train=True,\n",
    "                        transforms=train_audio_transforms)\n",
    "\n",
    "val_ds = MosreDataset(df=val_dataframe,\n",
    "                        data_patch=DATASET_PATCH,\n",
    "                        train=True,\n",
    "                        transforms=valid_audio_transforms)\n",
    "\n",
    "def my_collate(batch):\n",
    "    data = torch.stack([item[0] for item in batch])\n",
    "    target = torch.nn.utils.rnn.pad_sequence(\n",
    "                                            [item[1] for item in batch], \n",
    "                                            batch_first=True, \n",
    "                                            padding_value=0) # выравнивает последовательность до макс \n",
    "                                                            # длины в датче заполняя пропуски нулем\n",
    "    label_len = torch.stack([item[2] for item in batch])\n",
    "    msg = [item[3] for item in batch]\n",
    "    \n",
    "    return [data, target, label_len, msg]\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=my_collate)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=my_collate)\n",
    "\n",
    "test, test_target, _, mess = next(iter(train_dl))\n",
    "test, test_target= test.to(DIVICE), test_target.to(DIVICE)\n",
    "\n",
    "test_val, val_target, _, val_mess = next(iter(val_dl))\n",
    "test_val, val_target = test_val.to(DIVICE), val_target.to(DIVICE)\n",
    "# test.shape \n",
    "\n",
    "#===== начало обучения =====\n",
    "model = MorseNet().to(DIVICE)\n",
    "model = DataParallel(model)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=6, verbose=True)\n",
    "loss_func = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True).to(DIVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nMorseNet - инициалицация модели. Число обучаемых параметров: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be51c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = model(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18e69f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 89, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3affb68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 89, 256])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaf999d",
   "metadata": {},
   "source": [
    "# Класс модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66eee39",
   "metadata": {},
   "source": [
    "Переменные для обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad235125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = model(test)\n",
    "# a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa7f726",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422ea6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "target_lengths must be of size batch_size",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     23\u001b[39m predict_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# print(N, T, predict.shape, labels.shape, predict_lengths.shape, label_lens.shape)\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# break\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m loss = \u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m loss.backward()\n\u001b[32m     30\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1984\u001b[39m, in \u001b[36mCTCLoss.forward\u001b[39m\u001b[34m(self, log_probs, targets, input_lengths, target_lengths)\u001b[39m\n\u001b[32m   1977\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   1978\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1979\u001b[39m     log_probs: Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1982\u001b[39m     target_lengths: Tensor,\n\u001b[32m   1983\u001b[39m ) -> Tensor:\n\u001b[32m-> \u001b[39m\u001b[32m1984\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1985\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1986\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1987\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1989\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1990\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1991\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mzero_infinity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1992\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\homer\\OneDrive\\Desktop\\Morse_Decoder_V2\\Morse_decoder_V2\\Lib\\site-packages\\torch\\nn\\functional.py:3079\u001b[39m, in \u001b[36mctc_loss\u001b[39m\u001b[34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[39m\n\u001b[32m   3067\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(log_probs, targets, input_lengths, target_lengths):\n\u001b[32m   3068\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m   3069\u001b[39m         ctc_loss,\n\u001b[32m   3070\u001b[39m         (log_probs, targets, input_lengths, target_lengths),\n\u001b[32m   (...)\u001b[39m\u001b[32m   3077\u001b[39m         zero_infinity=zero_infinity,\n\u001b[32m   3078\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m3079\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3084\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3085\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3086\u001b[39m \u001b[43m    \u001b[49m\u001b[43mzero_infinity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3087\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: target_lengths must be of size batch_size"
     ]
    }
   ],
   "source": [
    "lst_loss_train = []\n",
    "lst_loss_val = []\n",
    "p = []\n",
    "p_Val = []\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    pr = []\n",
    "\n",
    "    train_tqdm = tqdm(train_dl, desc=f\"Эпоха {epoch+1}/{EPOCHS} [Обучение]\", leave=False)\n",
    "    for mel_spec, labels, label_lens, _ in train_tqdm:\n",
    "        mel_spec, labels, label_lens = mel_spec.to(DIVICE), labels.to(DIVICE), label_lens.to(DIVICE)\n",
    "\n",
    "        #===== считатем длинну mel_spec для передачи в CTC loss =====\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predict = model(mel_spec) # (N=batch,T,C)\n",
    "        pr.append(predict)\n",
    "        N = predict.shape[1]\n",
    "        T = predict.shape[0]\n",
    "        predict_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "\n",
    "        # print(N, T, predict.shape, labels.shape, predict_lengths.shape, label_lens.shape)\n",
    "        # break\n",
    "        loss = loss_func(predict, labels, predict_lengths, label_lens)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        train_loss = epoch_train_loss / len(train_data)\n",
    "    scheduler.step()\n",
    "    # ======== Валидация ========\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_mel_spec, val_labels, val_label_lensin, _ in tqdm(\n",
    "                                                        val_dl, \n",
    "                                                        desc=f\"Эпоха {epoch+1}/{EPOCHS} [Валидация]\", \n",
    "                                                        leave=False):\n",
    "            val_mel_spec, val_labels, val_label_lensin = val_mel_spec.to(DIVICE), val_labels.to(DIVICE), val_label_lensin.to(DIVICE)\n",
    "            val_predict = model(val_mel_spec)\n",
    "\n",
    "            p_Val.append(val_predict)\n",
    "            val_N = val_predict.shape[1]\n",
    "            val_T = val_predict.shape[0]\n",
    "            predict_val_lengths = torch.full(size=(val_N,), fill_value=val_T, dtype=torch.long)\n",
    "            val_loss += loss_func(val_predict, val_labels, predict_val_lengths, val_label_lensin).item()\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "\n",
    "    lst_loss_train.append(train_loss)\n",
    "    lst_loss_val.append(val_loss)\n",
    "\n",
    "            \n",
    "    print(f\"\\n===== Эпоха {epoch+1}/{EPOCHS} =====\")\n",
    "    # print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    grad_norms = [param.grad.norm().item() for param in model.parameters() if param.grad is not None]\n",
    "    if grad_norms:\n",
    "        print(f\"Mean grad norm: {np.mean(grad_norms):.6f}\")\n",
    "        print(f\"Max grad norm: {np.max(grad_norms):.6f}\")\n",
    "        print(f\"Min grad norm: {np.min(grad_norms):.6f}\")\n",
    "    else:\n",
    "        print(\"No gradients computed yet.\")\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1547a2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lst_loss_train)\n",
    "plt.plot(lst_loss_val)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Morse_decoder_V2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
