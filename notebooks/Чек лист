0) Сохдать отдельный трансформер на валинацию => Изменить class Dataset 
1) Обучить на 1-м батче
2) изменить LEARNING_RATE и WEIGHT_DECAY 

3) попробовать убрать выравнивание по длинне аудио - появляется много пустого места => 
модет плохо сказываться на обучение - Пошли взрывы градиента + непонятно что делать с ращной длиной
4) Дабавить нормализацию данных + Линейный слой для уменьшения признаков - 
5) добавить инициализацию весов/переписать архитектуру - 

6) Добавлена ифнормация перед запуском обучения + инфо о градиентах(Сейчас происходит затухание так как nan в preicts), 
    Попытна сгладит зибухание путем добалвения LeakyReLU для просчета отрицательных значений    
    Изменена свертка - есть предполодение что очень резкая вертка, делаем ее плавнее и оставляем больше признаков.
    # Уже стало лучше - градиенты не затухают а и ишибки стали алекватнее

7) Возможно подрезка кадра не нужна, так как принудительно ухудшает данные# не понятно есть ли вэлью, но я вно стало медленне обучаться
7.1) Порробую рапарралелить обучение (DataParallel to gpu) и использую flatten_parameters # ну хз

8) попробую порабгоать с моделью без удлинения сигнала по времени.
 До этого я удлинял короткие сигналы в 5 раз))


Подтянул знания в Rnn сетях:
2.1) Думаю что нужно реализовать модель encoder-decoder,
где в encoder будет использоваться CNN сеть для извлечения фич
далее пойдет рекурентный декодер. Кажется что испольщование attention не 
целесообразно так как нет зависимости в последовательности.
Цель по фото декодировать хаотичный набор символов. т.е. Будем плучать одну метку класса 
для КАЖДОГО скрытого состояния полученного на слое encoder
nn.functional.log_softmax(x.permute(1,0,2), dim=2) - добавил .detach().requires_grad_() - градиент не взрывается???


2.2) 
Train Loss: 0.8667
Val Loss: 4546.3094 - может быть предикт не соответствует таргету

2.3) поставил клипинг, но есть nan следоваетльно градиенты затухают резко.

2.4) Возможно косяк был в неправильном падинге последовательности. 
У меня симпол пропуска кодировался нулум так же как и нулевой элемент помледовательности - переделал 
на отдельный симпол и дабавил его в словарь



В версии 2 получена модель которая дает качество 1.01 в kagle

V3) Улучщение качества модели. Цель сделать хорошее качетво не увеличивая колличества эпох

3.1) Увеличил колличество рекурентных слоев num_layers=3. -> 
Train Loss: 0.1411 
Val Loss: 565.5488 и не падает особо - переобучение из-за высокой глубины слоев.
Для решения попробую длюавить dropout=0.2 в Gru + поставлю 
LEARNING_RATE = 2e-4 
WEIGHT_DECAY = 1e-4
- Стало лучше но все раво на трейне в первую эпоху переобучение

3.2) Уменьшу GRU hiden c 258 до 128 + добавлю dropout=0.3 перед линейныи слоем после GRU + 
доваблю больше аугментаций(сейчас они применяются ко все данным одинакого, мб стоит делать 
рандомные + более резкие)
Еще по идее странсформации в текухей реализации тоже обучабтся, мб это не надо - как я понял изучаю
вопрос -  FrequencyMasking и TimeMasking не обучаются. 
- что-то все равно лосс уже на первых эпохах маленький + разрыв огромный. 

3.3) Мб num_layers сделать 2 и увеличить dropout -> Похоже я просто выводил не усредненный val_loss))

3.4) Верну  num_layers=3 dropout=0.2 LEARNING_RATE = 2e-4 WEIGHT_DECAY = 1e-4 и отключу
клипинг

3.5) Верну hiden=258 и сделаю batch_size=64


----4) Что-то все сломалось, попробую откатиться к версии 2, 
только исправив высод лоса + увеличить кол-во эпох
    Получил молель с точностью 0.89

----5) Попробую поищраться с гиперпараметрами + все же добавлю Attention в модель
        MAX_TIME = 48
        SAMPLE_RATE = 8000
        N_MELS = 64
        N_FFT = 512
        HOP_LENGTH = 160
        TOP_DB = 80
        FREQ_MASK = 15
        TIME_MASK = 20

        # Гиперпараметы обучения
        SEED = 42
        BATCH_SIZE = 64
        EPOCHS = 60
        LEARNING_RATE = 0.0002
        WEIGHT_DECAY = 0.00001
        + добавлю аугментаций аудио через audiomentations. 
        + Сперва попробую в Attention
        частом виде, если не поможет попробую multihead attention
        - Вывод: не помогло.

----6) Прочитал что BatchNorm2d плохо работает при маленьком батче

----7) Наткнулся на статью про SE блоки как аналог Attention но для CNN
Squeeze-and-Excitation (SE) блок в CNN
class SEBlock(nn.Module):
    def __init__(self, channels, reduction=16):
        super().__init__()
        self.squeeze = nn.AdaptiveAvgPool2d(1)  # [B, C, 1, 1]
        self.excitation = nn.Sequential(
            nn.Linear(channels, channels // reduction),  # [B, C/reduction]
            nn.ReLU(),
            nn.Linear(channels // reduction, channels),  # [B, C]
            nn.Sigmoid()
        )
    
    def forward(self, x):
        B, C, _, _ = x.shape
        squeezed = self.squeeze(x).view(B, C)  # [B, C]
        weights = self.excitation(squeezed).view(B, C, 1, 1)  # [B, C, 1, 1]
        return x * weights  # Масштабируем каналы
    
    -> Слано немного лучше но этого мало

    7.1) Добавил mha с ключами(как понял, без ключеной это просто self attention а он как
    как уже выяснил не подходит) - вроде стало лучше но на 22+ эпуже видно переобчение

    7.2) Пробую жестко порезать модель + добавить еще dropout
    FIRST_FE_COUNT = 8  # было 16
    SECOND_FE_COUNT = 16  # было 32
    THIRD_FE_COUNT = 16  # было 32
    QAD_FE_COUNT = 16  # было 32
    GRU_HIDEN = 256  # было 512

    WEIGHT_DECAY = 0.001 #было 0.0001

    FREQ_MASK = 30
    TIME_MASK = 40
  

    class SEBlock(nn.Module):
    def __init__(self, channels, reduction=4):

    7.3) Поднимаем гиперпараметры
        FIRST_FE_COUNT = 64
        SECOND_FE_COUNT = 128
        THIRD_FE_COUNT = 128
        QAD_FE_COUNT = 128
        PADDING = 'same'
        MAXPOOL_KERNEL = 2
        KERTNEL_SIZE = 3
        NERON_COUNT = 128
        GRU_HIDEN = 512

        Такие изменения нагрузили видеокарту, хотя до этого она вообще не напрягалась.
        Попробовал num_workers=4, pin_memory=True в даталоадере, запуск первого блока просто помер. 
        Причины не известны

        Попробую записать спектрограммы заранее. Было лень писать отдельные трансформации для 
        трейн тест валидацияЮ так что написал все скопом, а аугментация уже налету делаю
        Это тоже не помогло 

        FIRST_FE_COUNT = 32
        SECOND_FE_COUNT = 64
        THIRD_FE_COUNT = 64
        QAD_FE_COUNT = 64 
        уже запустилось

        Получил молель 0.8 качества на kagle. но этого явно мало. Мб если бы смог запустить прошлый вариан было бы лучше.
        А так буду пилить странсформер

-----8) Сделать transformer ?
    по идее то что я писал до жтого было енкодером траснформера. 
    Навреное я могу переписать его в полноценный енкодер и бавить декодер, на вход которого придут 
    выходные значения тренировочных данных

    seq2seq модель 


------9) Оказалось был косяк в декодировании сообщений)))



    preds = []
    logits_cpu = logits.cpu()
    max_inds = torch.argmax(logits_cpu.detach(), dim=2).t().numpy() 

    for ind in max_inds:
        merged_inds = []
        prev_idx = None
        for idx in ind:
            if idx != blank_label_idx and idx != prev_idx:
                merged_inds.append(idx)
            prev_idx = idx
        text = "".join([int_char_map.get(i, '') for i in merged_inds])
        preds.append(text)

    return preds


    preds = []
    logits_cpu = logits.cpu() 
    max_inds = torch.argmax(logits_cpu.detach(), dim=2).t().numpy() # арзмакс по лагитам и преобразование к словарю
    
    for ind in max_inds:
        merged_inds = []
        for idx in ind:
            if idx != blank_label_idx: 
                merged_inds.append(idx)
        text = "".join([int_char_map.get(i, '?') for i in merged_inds])
        preds.append(text)

    return preds

-----10) Попробую проверить первую модель что дала 0.89 какое качество бедет если исправить декодировшик
    Mean accurasu by The Levenshtein in train is : 0.9535349280226777
    Mean accurasu by The Levenshtein in validate is : 0.9708523424428829
    По итогу одна из первых моделей без selfatention блоков и MultiheadAttention далла качесто 0.43. Прикольно

    Думаю selfatention для Cnn это хорошая идея а Attention как я и изначально думал тут вообще не нужен так как нет сильной зависимости символов как например в больший предложениях

    но сперва просто попробую поднять гиперпараметры
        FIRST_FE_COUNT = 32
        SECOND_FE_COUNT = 64
        THIRD_FE_COUNT = 64
        QAD_FE_COUNT = 64
        PADDING = 'same'
        MAXPOOL_KERNEL = 2
        KERTNEL_SIZE = 3
        NERON_COUNT = 128
        GRU_HIDEN = 512

        И подниму nn.Dropout(0.5). Как понял для Rnn сетей лучше ставить побольше


        === Полуичлось качество хуже

    10.1) попробю те же гипер параметры, но добавлю selfatention для Cnn и 
    прокину 3 lstm слоя
    === Ну качаство стало немного хуже. Пока все еще самая базовая модель лучше всего

    10.2) Возможно я сильно сжимаю признаки на сверточном слое. nn.MaxPool2d. НАчальная логика
    была в том, что временная ось сильно длиннее чем частотная, пожтому сжимал ее сильнее.
    Но возможно теряю полезные признаки, так как усложнение модели не особо мопагает. В первой верстт
    был nn.MaxPool2d(2)

    Сейчас я сжимаю в 6 раз. Мб стоит только в 2 раза или вообще не трогать?
    - Попробую без maxpill и selfatention . stride=(1, 2) -> не трограю временнубю ось вообще

    == На 15 эпохе началось переобчение

    10.3) чуйка подсказывает что проблема в CNN блоке
    Попробую следующие параметры, что бы скократить признаки, но мешьше чем рашьне      
    self.net_conv = nn.Sequential(
            nn.Conv2d(in_channels=1, 
                      out_channels=FIRST_FE_COUNT, 
                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),
            nn.BatchNorm2d(FIRST_FE_COUNT),
            nn.GELU(),
            SEBlock(FIRST_FE_COUNT),
            nn.MaxPool2d(kernel_size=2, stride=2), # [batch, FIRST_FE_COUNT = 16, 64, 178]

            nn.Conv2d(in_channels=FIRST_FE_COUNT, 
                      out_channels=SECOND_FE_COUNT, 
                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),
            nn.BatchNorm2d(SECOND_FE_COUNT),
            nn.GELU(),
            SEBlock(SECOND_FE_COUNT),
            # nn.MaxPool2d(2, 2), #

            nn.Conv2d(in_channels=SECOND_FE_COUNT, 
                      out_channels=THIRD_FE_COUNT, 
                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),
            nn.BatchNorm2d(THIRD_FE_COUNT),
            nn.GELU(),
            SEBlock(THIRD_FE_COUNT),
            # nn.MaxPool2d((2, 2), (2, 2)), 

            nn.Conv2d(in_channels=THIRD_FE_COUNT, 
                      out_channels=QAD_FE_COUNT, 
                      kernel_size=KERTNEL_SIZE , stride=1, padding=PADDING),
            nn.BatchNorm2d(QAD_FE_COUNT),
            # SEBlock(QAD_FE_COUNT),
            nn.GELU(),
            nn.MaxPool2d(kernel_size=2, stride=2) # [batch=32, QAD_FE_COUNT = 32, 32, 89]
        )

    11) Думаю последнй эксперемент по улучщению качества. Попробую обьеденить в ансамбль 2 модели что дали близкие результаты 0.42 и 0.44
    ?? У меня в оптимизаторе небыло WEIGHT_DECAY))??

    результат не утешительный 

    12) Уже не могу вспомнить, но почему то уменьшил
    FIRST_FE_COUNT = 32
    SECOND_FE_COUNT = 64
    THIRD_FE_COUNT = 128
    QAD_FE_COUNT = 256
    Попробую вернуть на исходную

    - Вспомнил) сходимость модели упала в ноль. Слишком много разрозненой информации получается после Cnn